{
  "docs/authentication.html": {
    "href": "docs/authentication.html",
    "title": "Autenticação | AIVAX",
    "keywords": "Autenticação Quando tiver sua conta em mãos, use sua chave de autenticação única para se autenticar na nossa API através do cabeçalho Authorization: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' Você também pode enviar o seu token de autorização pelo parâmetro da query ?api-key, exemplo: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... Não há necessidade de enviar o esquema de autenticação Bearer em ambos cabeçalhos, mas é possível por questões de compatibilidade. Autenticando um nonce Nonces são gerados por webhooks e funções de protocolo para autenticar transações feitas pela AIVAX em conteúdos externos. Em requisições que incluam o cabeçalho X-Aivax-Nonce, valide esse token chamando este endpoint: POST /api/v1/auth/nonce { \"nonce\": \"kgnjcyjy7ksz5eeocc4iw14bre.7cahcn8phfmrdkidhqaecp4e4w\" } Observação: essa rota requer autenticação para funcionar. Uma resposta para um nonce validado é retornada neste formato: { \"message\": null, \"data\": { \"validated\": true, \"reason\": \"The nonce informed is valid and has been consumed.\" } } Ao validar um nonce, ele é consumido, e não é possível validar o mesmo nonce novamente. Nonces devem ser validados em até 10 minutos de sua emissão."
  },
  "docs/en/authentication.html": {
    "href": "docs/en/authentication.html",
    "title": "Authentication | AIVAX",
    "keywords": "Authentication When you have your account in hand, use your unique authentication key to authenticate on our API through the Authorization header: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' You can also send your authorization token through the query parameter ?api-key, example: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... There is no need to send the Bearer authentication scheme in both headers, but it is possible for compatibility reasons. Authenticating a nonce Nonces are generated by webhooks and protocol functions to authenticate transactions made by AIVAX in external content. In requests that include the X-Aivax-Nonce header, validate this token by calling this endpoint: POST /api/v1/auth/nonce { \"nonce\": \"kgnjcyjy7ksz5eeocc4iw14bre.7cahcn8phfmrdkidhqaecp4e4w\" } Note: this route requires authentication to work. A response for a validated nonce is returned in this format: { \"message\": null, \"data\": { \"validated\": true, \"reason\": \"The nonce informed is valid and has been consumed.\" } } When validating a nonce, it is consumed, and it is not possible to validate the same nonce again. Nonces must be validated within 10 minutes of their issuance."
  },
  "docs/en/entities/ai-gateway.html": {
    "href": "docs/en/entities/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway The AI gateways are a service provided by AIVAX to create an inference tunnel between an LLM model and a knowledge base. It is possible to: Create a model with customized instructions Use a model provided by you through an OpenAI compatible endpoint, or use a model made available by AIVAX Customize inference parameters, such as temperature, top_p, prefill Use a knowledge collection as the foundation for AI responses Among other features. With the AI Gateway, you create a model ready for use, parameterized and based on the instructions you define. Models You can bring an AI model compatible with the OpenAI interface to the AI gateway. If you bring your AI model, we will only charge for the document search attached to the AI. You can also use one of the models below that are already ready to start with AIVAX. When using a model, you will notice that some are more intelligent than others for specific tasks. Some models are better with certain data acquisition strategies than others. Perform tests to find the best model. You can view the available models on the models page. Choosing a search strategy If you are using a knowledge collection with an AI model, you can choose a strategy that the AI will use to perform an information search. Each strategy is more refined than the other. Some create better results than others, but it is essential to perform practical tests with several strategies to understand which one fits best in the model, conversation, and user tone. It may be necessary to make adjustments to the system prompt to better inform the AI how to consider the documents attached to the conversation. The documents are attached as a user message, limited to the parameters you define in the acquisition strategy. Rewrite strategies usually generate the best results at a low latency and cost. The rewrite model used is always the one with the lowest cost, usually chosen by an internal pool that decides which model has the lowest latency at the moment. Strategies without rewrite cost: Plain: the default strategy. It is the least optimized and has no rewrite cost: the last user message is used as a search term to search the attached collection of the gateway. Concatenate: concatenates the last N user messages in lines, and then the result of the concatenation is used as a search term. Strategies with rewrite cost (inference tokens are charged): UserRewrite: rewrites the last N user messages using a smaller model, creating a contextualized question about what the user wants to say. FullRewrite: rewrites the last N*2 chat messages using a smaller model. Similar to UserRewrite, but also considers the assistant's messages in formulating the new question. It usually creates the best questions, with a slightly higher cost. It is the most stable and consistent strategy. It works with any model. Function strategies: QueryFunction: provides a search function in the integrated collection for the AI model. You should adjust the system instructions to define the ideal scenarios for the model to call this function when necessary. It may not work as well with smaller models. Using an AI gateway AIVAX provides an endpoint compatible with the OpenAI interface through an AI-gateway, which facilitates the integration of the model created by AIVAX with existing applications and SDKs. It is worth noting that only some properties are supported. In an AI gateway, you already configure the model parameters, such as System Prompt, temperature, and model name. When using this endpoint, some gateway values can be overridden by the request. Request POST /api/v1/chat-completions { \"model\": \"0197cb0f-893a-7b7d-be0a-71ada1208aaf\", \"messages\": [ { \"role\": \"user\", \"content\": \"Who are you?\" } ], \"stream\": false } Response for non-streaming { \"id\": \"0197dbdc-6456-7d54-b7ec-04cb9c80f460\", \"object\": \"chat.completion\", \"created\": 1751740343, \"model\": \"@aivax/sentinel-mini\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"completion_text\": \"Hi! I'm Zia, the intelligent assistant from Zé do Ingresso. I'm here to help you with everything about tickets, events, and everything that happens in our beloved São José do Rio Preto. If you need anything, just call! Let's enjoy everything together! What do you need? \", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 1118, \"completion_tokens\": 88, \"total_tokens\": 1206 }, \"sentinel_usage\": null, \"service_tier\": \"default\" } Streaming response data: {\"id\":\"0197dbde-c40b-720d-a13e-f689c303c571\",\"object\":\"chat.completion.chunk\",\"created\":1751740498,\"model\":\"gpt-4o-mini\",\"system_fingerprint\":\"fp_y8jidd\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} ... data: {\"id\":\"0197dbde-c88c-7764-8017-c1fee0d79096\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"gpt-4o-mini\",\"system_fingerprint\":\"fp_2he7ot\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} data: {\"id\":\"0197dbde-c890-7329-9e65-faecbe158efa\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@aivax\\/sentinel-mini\",\"system_fingerprint\":\"fp_q6qh7x\",\"choices\":[{\"index\":0,\"finish_reason\":\"STOP\",\"delta\":{}}],\"usage\":{\"prompt_tokens\":1097,\"completion_tokens\":92,\"total_tokens\":1189},\"sentinel_usage\":null}"
  },
  "docs/en/entities/chat-clients.html": {
    "href": "docs/en/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients A chat client provides a user interface through an AI Gateway that allows the user to converse with their assistant. A chat client is integrated with the AI gateway's inference and supports deep thinking, research, and text conversation. Multi-modal features, such as sending images and audio, are under development. Note AIVAX never stores the content of a chat between a client and the user. You can use JavaScript for this task, but under your responsibility for usage and storage. You can customize the interface of your chat client with custom CSS and JavaScript, as well as choose the language of the chat features. Creating a Chat Session A chat session is where you create a conversation between your chat client and the user. You can call this endpoint by providing additional context for the conversation, such as the user's name, location, etc. A chat session expires after some time for security of the generated access token. When you call this endpoint by providing a tag, you can call the same endpoint multiple times and get the active chat session for the informed tag, or create a new chat if there is no ongoing session. A chat session also restores all conversation messages from the same session after disconnection. The user can clear the conversation by clicking the clear conversation button in the top right corner of the chat client. This session uses the limits defined by the chat client, such as the maximum number of messages and tokens in the conversation. A session is automatically renewed for another 3 days when receiving a message from the user. Important It is only possible to determine the number of tokens used in a message when using a model provided by AIVAX. If you use an external model, the limitingParameters.userInputMaxTokens property will be ignored. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Optional. Additional context for the AI about the chat. \"extraContext\": \"# Additional context\\r\\n\\r\\nYou are talking to Eduardo.\", // Time in seconds for the chat to expire. The minimum is 10 minutes. The maximum is 30 days. \"expires\": 3600, // Optional (recommended). An external ID to identify the session later and reuse it whenever calling the same endpoint. It can be the ID of the user in your database or a string that facilitates the identification of this chat later. \"tag\": \"my-user-tag\" } Response { \"message\": null, \"data\": { // ID of the created chat session. \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Public access key of the chat. \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // Public URL to converse with the chat. \"talkUrl\": \"https://preview-s01.proj.pw/www/web-chat-clients/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } }"
  },
  "docs/en/entities/collections.html": {
    "href": "docs/en/entities/collections.html",
    "title": "Collections | AIVAX",
    "keywords": "Collections A collection is a knowledge library: it houses several knowledge documents. Use collections to group documents by purpose, such as documenting a product, a company, a service or flow. Collections do not incur costs. There is no limit to the number of collections per account. Create a collection To create an empty collection, simply provide its name: Request POST /api/v1/collections { // The collection name cannot be empty. \"collectionName\": \"My first collection\" } Response { \"message\": null, \"data\": { // Unique ID of the created collection. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } List collections Lists the collections available in your account. Request GET /api/v1/collections Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"My collection\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Another collection\" } ] } } View a collection Obtains details of a collection, such as its indexing progress and information like creation date. Request GET /api/v1/collections/{collection-id}/ Response { \"message\": null, \"data\": { \"name\": \"My collection\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // Returns the number of documents waiting for indexing \"queuedDocuments\": 0, // Number of documents ready for query \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Delete a collection Deletes a collection and all documents within it. This action is irreversible. Request DELETE /api/v1/collections/{collection-id}/ Response { \"message\": \"Collection deleted successfully.\", \"data\": null } Clear a collection Unlike collection deletion, this operation removes all documents from the collection, including indexed and queued ones. Request DELETE /api/v1/collections/{collection-id}/reset-only Response { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/en/entities/documents.html": {
    "href": "docs/en/entities/documents.html",
    "title": "Documents | AIVAX",
    "keywords": "Documents A document represents a piece of knowledge. It is a limited, self-sufficient, and meaningful piece of text on its own. A document is the component that is indexed by the internal model to be retrieved later through a semantic search term. Consider a car manual: it is not a document, but rather several documents. Each of these documents talks, in isolation, about a specific topic related to that car, in such a way that the document does not depend on external context or information to make sense. Each document in this manual will talk about a topic: one will talk about how to turn on the car, another about how to turn it off, another about how its paint is made, and another about how to change the oil periodically. It is not a good idea to reserve a document to talk about several things at the same time, as this will reduce the objectivity and scope of the inference and reduce the quality of acquisition. Examples of document creation: Do not do Do not create very short documents (with 10 or fewer words). Do not create very large documents (with 700 or more words). Do not talk about more than one thing in a document. Do not mix different languages in documents. Do not be implicit in documents. Do not write documents using technical language, such as codes or structures like JSON. Do Be explicit about the purpose of your document. Focus documents on individual topics, summarizing what should be done or explained. Always repeat terms that are keywords for the document search. Example: prefer to use \"The color of the Honda Civic 2015 is yellow\" instead of \"the color of the car is yellow\". Restrict the document content to talk about only one topic or subject. Use simple and easy-to-understand human language. API Usage As all documents are entities that belong to a collection, always have the collection where the document is/will be located at hand. Sending documents in bulk To send a large list of documents to a collection, structure them following the JSONL format. The indexing file structure is: {\"docid\":\"Cars/HondaCivic2015.rmd:1\",\"text\":\"The Honda Civic 2015 is available in [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:2\",\"text\":\"The engine of the Honda Civic 2015 is [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:3\",\"text\":\"The color of the Honda Civic 2015 is Yellow [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} ... The structure consists of the following properties: Property Type Description docid string Specifies the document name. Useful for debugging and identification. text string The \"raw\" content of the document that will be indexed. __ref string Optional. Specifies a reference ID of the document. __tags string[] Optional. Specifies an array of document tags. Useful for document management. A document reference is an ID that can be specified in several documents that need to be linked in a search when one of them is matched in a similarity search. For example, if a search finds a document that has a reference ID, all other documents in the same collection that share the same reference ID as the matched document will also be included in the search response. The use of references can be useful when a document depends on another or more documents to make sense. There is no format requirement for the reference ID: any format is accepted. You can send up to 1,000 lines of documents per request. If you need to send more documents, separate the sending into more requests. If you send a document with more than 1,000 lines, the following lines will be ignored. Note that very long documents, which exceed the allowed number of tokens in the internal embedding model, will have their content truncated and the indexing quality may be severely affected. To avoid this problem, send documents that contain between 20 and 700 words. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of each document. The content of each document is tokenized according to the model used in the indexing of the documents. Request The sending must be done using multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Response { \"message\": null, \"data\": [ { \"name\": \"Institutional/Company.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institutional/Company.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institutional/Company.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Products/Schedules.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Create or modify document This endpoint creates or modifies a document from its name. When a document is modified, its indexing vectors are reset, that is, the document will enter the queue again to be indexed by the indexing engine. This indexing is not exempt from cost. The cost is relative to the number of tokens of the sent content. The cost is only generated when the document is actually changed. Calling this route with the same content as the document does not generate modification, therefore, it does not generate cost. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of the file. The content of the file is tokenized according to the model used in the indexing of the documents. Request PUT /api/v1/collections/{collection-id}/documents { // the name of the document that will be modified \"name\": \"document-name\", // the content of the document that will be created or overwritten if the name already exists \"contents\": \"Content of my document\", // parameters explained earlier \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Response { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // the state of the operation indicates whether the document was modified \"Modified\" or created \"Created\". \"state\": \"Modified\" } } List documents This endpoint lists all available documents in a collection. You can pass an additional query parameter filter to filter documents by name, tag, or content. This filter supports expressions that help filter what you are looking for: -t \"tag\" - filters documents that have this tag. -r \"reference\" - filters documents that have this reference ID. -c \"content\" - filters documents that have this snippet in their content. -n \"name\" - filters documents that have this snippet in their name. Request GET /api/v1/collections/{collection-id}/documents Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Help/Customers.rmd:1\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"A customer is a registration on your platform...\", \"indexState\": \"Indexed\" }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Help/Customers.rmd:2\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"In the customer registration, it is possible to modify...\", \"indexState\": \"Indexed\" }, ... ] } } View document View details about a specific document. Request GET /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institutional/Company.rmd:1\", // represents the indexing situation of the document. // valid values: Queued, Indexed, Cancelled \"state\": \"Indexed\", // content of the indexed document \"contents\": \"...\", // document reference ID \"reference\": \"institutional-company\" } } Delete document Permanently deletes a document through its ID. Request DELETE /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/en/entities/functions.html": {
    "href": "docs/en/entities/functions.html",
    "title": "Functions | AIVAX",
    "keywords": "Functions Functions are a way to force your model to process information using JSON as an intermediate communication medium. With functions, you can make any model respond in the JSON format you want. It can be useful for categorizing comments, applying moderation to reviews, or processing information with the help of AI. Currently, it is only possible to use functions with models provided by AIVAX. Calling a Function To call an AI function, you will need to specify what the AI should respond with and provide a JSON schema that it should follow. Less intelligent models tend to fail to generate JSON, resulting in an invalid or problematic document. To fix this, adjust your model, instruction, and attempt parameter if necessary. You are charged for each attempt the AI makes to generate a response. Slightly more intelligent models tend to generate correct results on the first attempt. It is guaranteed that a valid JSON will be generated, and this JSON will follow the same schema provided in the request. Additionally, you can opt to activate internet search for function calls. This option can be useful for bringing relevant data in real-time when structuring your response. When using this function, a model with internet access will be used to obtain data from the internet to structure your response. This model will also attempt to structure your response based on the provided data, and if it can formulate a valid JSON, the step of calling the structuring model is ignored, and the response is immediately returned. If the online search model is unable to structure a valid JSON, the model chosen in the request will be responsible for this task and will start the attempt chain to generate a response. More intelligent models tend to get the generation right on the first attempts. Through the fetch property, you can provide a list of URLs to be attached to the generation context. AIVAX makes a GET request to access the provided content and renders it in the request content. Only 2xx or 3xx responses are accepted, and the response content must be textual. HTML responses are sanitized to include only the page text, without scripts and CSS. The maximum size that can be read from a fetch URL is 10 Mb. The maximum number of items for fetch is 10 URLs. Requests that search the internet bring good results and dispense with crawlers, scrapers, or the need to pay for a specific API, but they can be expensive and relatively slow to obtain. Consider using a cache on the side of your application for data that does not need to be constantly updated, such as weather data, daily statistics, etc. AIVAX does not perform any caching on our side. Request POST /api/v1/functions/json { // Required. Specify the name of the integrated model to be used to perform the action. \"modelName\": \"@metaai/llama-3.1-8b\", // Required. Explain what your model should do with the input and how it should bring the response. \"instructions\": \"Classify the user's comment, indicating whether it is positive or negative, and if it contains any relevant information (number between 0 (not relevant) and 10 (very relevant))\", // Required. The JSON object that the model should generate. You can provide generation examples in the instructions field. This object must be a valid JSON in the API. // This object must be an object, an array, or a string. \"responseSchema\": { \"feedbackType\": \"{neutral|positive|negative}\", \"informationScore\": 5 }, // Optional. Specifies a list of JSON paths that the AI must always generate content for and that this field cannot be null. For arrays, specify with [*]. \"requiredFields\": [ \"$.feedbackType\", \"$.informationScore\" ], // Optional. Defines a JSON input for the model. It can be any type of JSON value. \"inputData\": { \"userComment\": \"Pessimo mercado. Tem guarda dentro te vigiando pra vc nao roubar e os acougueiros te ignoram e atendem mocinhas bonitinhas na tua frente. Mas graças a Deus tem outros mercados chegando e o fim dessa palhaçada vai chegar\" }, // Optional. Defines how many attempts the model should try before the API returns an error. It must be a number between 1 and 30. \"maxAttempts\": 10, // Optional. Defines the time limit in seconds to obtain a valid JSON before the API returns an error. It must be a number between 1 and 3600 (one hour). \"timeout\": 300, // Optional. Defines the temperature of JSON generation. Higher values tend to be more creative, while lower values are more deterministic. Number from 0 to 2. \"temperature\": 0.4, // Optional. Adds external resources to complement the response generation. \"fetch\": { // Required. Provides the list of URLs that AIVAX will access to complement the response generation. The maximum is 10 URLs. \"urls\": [ \"https://url1...\", \"https://url2...\", ], // Optional. Defines the fetch behavior for errors when trying to access the site. Errors include responses that are not 2xx or 3xx, timeouts, certificate errors, etc. // fail -> returns an error in the function response (default) // warn -> adds a warning to the function response and does not include the error in the AI generation // ignore -> ignores the error and includes the error in the AI generation \"fetchFailAction\": \"fail\" | \"warn\" | \"ignore\", // Optional. Defines the timeout in seconds for the maximum response time and reading the contents. The maximum is 120 seconds (two minutes). \"timeout\": 10, // Optional. Defines the maximum content size in characters that can be included in the AI generation before being truncated. \"pageMaxLength\": 2048 } } Response { \"message\": null, \"data\": { // the result contains the object defined in \"responseSchema\", with the fields filled in by the AI \"result\": { \"feedbackType\": \"negative\", \"informationScore\": 8 }, // in which attempt the AI was able to generate a valid JSON \"attempt\": 1, // the time in milliseconds to obtain a valid JSON \"elapsedMilliseconds\": 527, // warnings produced by the generation \"warnings\": [] } } Considerations about the JSON Schema Specify enumerated values with \"{value1|value2|value3}\". In this way, the model should choose one of the values presented in the generation of the JSON. All values are placeholders for the model generation. Indicate what a field is or what value it should receive with a hint in its own placeholder or indicate directly in the function instructions. All values can be null, unless you specify directly to the model that they cannot. The output structure of the model is the same as informed in responseSchema. The input structure is irrelevant. Real-time Functions with Sentinel Models You can use the @aivax/sentinel-lambda agent to execute intelligent functions that involve internet search, code execution, mathematical problem solving, and all other functionalities that Sentinel agents can provide. Sentinel agents are connected to the internet by default, so it is natural that they search for something on the internet to complement their response. When calling a function with a model that searches the internet, such as a Sentinel agent, the consumption limit is Live Function. Examples Check out examples of AI functions for various everyday tasks: Summarize Order and Classify if it Requires Attention or Not POST /api/v1/functions/json { \"modelName\": \"@metaai/llama-4-scout-17b\", \"instructions\": \"Summarize the user's comment, creating a short description, with a maximum of 10 words indicating what they want to do. Also, indicate if this comment requires attention or not.\", \"responseSchema\": { \"shortSummary\": \"...\", \"requiresAttention\": false }, \"inputData\": \"O cliente fernando de castro está tentando entrar em contato com o suporte desde sexta-feira e diz q vai cancelar se nao falar com alguém hoje. ele tbm disse que é amigo da rebeca do comercial e está ameaçando falar mal da empresa no tiktok. por favor alguém atende esse cara??\" } { \"message\": null, \"data\": { \"result\": { \"shortSummary\": \"Cliente quer contato com suporte para evitar cancelamento e ameaça\", \"requiresAttention\": true }, \"attempt\": 1, \"elapsedMilliseconds\": 639 } } Evaluate a Mathematical Expression POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Evaluate the given mathematical expression and provide the result.\", \"responseSchema\": { \"result\": \"...\" }, \"inputData\": { \"mathProblem\": \"12 + (42 / pi) ^ 20\" } } { \"message\": null, \"data\": { \"result\": { \"result\": \"3.3265063290400284e+22\" }, \"attempt\": 0, \"elapsedMilliseconds\": 3869, \"warnings\": [] } } Bring Latest News and Weather for a Given City POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Search for the 5 latest news and weather data for the given city.\", \"responseSchema\": { \"latestNews\": [ { \"title\": \"...\", \"details\": \"...\", \"link\": \"https://...\" } ], \"weather\": { \"currentTemperature\": 0, \"currentWeather\": \"{sunny|cloudy|rain|thunderstorm}\", \"forecast\": \"{sunny|cloudy|rain|thunderstorm}\" } }, \"inputData\": { \"city\": \"São José do Rio Preto\" } } { \"message\": null, \"data\": { \"result\": { \"latestNews\": [ { \"title\": \"Festival Paralímpico no Instituto Lucy Montoro\", \"details\": \"Familiares e pacientes do Instituto de Reabilitação Lucy Montoro de Rio Preto (SP) participaram neste sábado (14) do Festival Paralímpico.\", \"link\": \"https://g1.globo.com/sp/sao-jose-do-rio-preto-aracatuba/cidade/sao-jose-do-rio-preto/\" }, { \"title\": \"Investimento imobiliário na zona Leste de Rio Preto\", \"details\": \"Empreendimentos residenciais marcam o maior investimento imobiliário na zona Leste de Rio Preto.\", \"link\": \"https://www.gazetaderiopreto.com.br/\" }, { \"title\": \"Acidente na Washington Luís\", \"details\": \"Motociclista é hospitalizado após se envolver em acidente na Washington Luís, em Rio Preto.\", \"link\": \"https://www.diariodaregiao.com.br/cidades\" }, { \"title\": \"Curso gratuito para artistas\", \"details\": \"Curso gratuito ensina artistas de Rio Preto e região a escrever projetos para editais culturais.\", \"link\": \"https://www.diariodaregiao.com.br/\" }, { \"title\": \"PIX automático\", \"details\": \"Modalidade permite pagar contas recorrentes, como de energia, telefone, escolas, academias, entre outras, de forma automática pelo PIX.\", \"link\": \"https://g1.globo.com/sp/sao-jose-do-rio-preto-aracatuba/\" } ], \"weather\": { \"currentTemperature\": 23, \"currentWeather\": \"sunny\", \"forecast\": \"sunny\" } }, \"attempt\": 0, \"elapsedMilliseconds\": 10990, \"warnings\": [] } } Bring COVID-19 Statistics in Real-time POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Bring the count of cases and deaths from COVID-19.\", \"responseSchema\": { \"deathsWorld\": 0, \"deathsBrazil\": 0, \"casesWorld\": 0, \"casesBrazil\": 0 } } { \"message\": null, \"data\": { \"result\": { \"deathsWorld\": 7010681, \"deathsBrazil\": 711380, \"casesWorld\": 704753890, \"casesBrazil\": 38743918 }, \"attempt\": 1, \"elapsedMilliseconds\": 1620 } } Bring Artists in High Demand by Music Genre POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Search and format a list of 10 artists in the TOP 10 of music streaming by genre.\", \"responseSchema\": { \"edm\": [ \"artist name\", \"artist name\", \"...\" ], \"rap\": [ \"artist name\", \"artist name\", \"...\" ], \"pop\": [ \"artist name\", \"artist name\", \"...\" ] }, \"inputData\": null } { \"message\": null, \"data\": { \"result\": { \"edm\": [ \"Marshmello\", \"Swedish House Mafia\", \"Skrillex\", \"Illenium\", \"John Summit\", \"Kaskade\", \"Daft Punk\", \"Deadmau5\", \"Avicii\", \"Calvin Harris\" ], \"rap\": [ \"Drake\", \"Kendrick Lamar\", \"Eminem\", \"Travis Scott\", \"J. Cole\", \"Nicki Minaj\", \"21 Savage\", \"Metro Boomin\", \"Lil Wayne\", \"Bad Bunny\" ], \"pop\": [ \"Taylor Swift\", \"Bruno Mars\", \"The Weeknd\", \"Lady Gaga\", \"Billie Eilish\", \"Sabrina Carpenter\", \"Ariana Grande\", \"Ed Sheeran\", \"Post Malone\", \"Doja Cat\" ] }, \"attempt\": 0, \"elapsedMilliseconds\": 14961, \"warnings\": [] } }"
  },
  "docs/en/entities/search.html": {
    "href": "docs/en/entities/search.html",
    "title": "Search | AIVAX",
    "keywords": "Search The search API, through the query key obtained from the collections, performs a semantic search on it, performing an intelligent comparison for each indexed document in a collection. After creating a collection, you will get its ID. Use the ID of your collection to perform the search on the indexed documents of the same. Use the endpoints of this API to embed the semantic search of documents in your AI model or chatbot. Searching documents This endpoint expects a GET request with the following parameters: term: required. Specifies the search term that will be searched in the documents. top: Specifies the maximum number of documents that should be returned in the search. min: Specifies the minimum score for obtaining the documents. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the search term. The search term is tokenized according to the model used in the indexing of the documents. Request GET /api/v1/collections/{collection-id}/query term=What is the color of the Honda CIVIC? Response { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Products/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Products/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Products/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } For the search result, the higher the score, the more similar the document is to the search term. AIVAX uses embedding models that allow task orientation. For the search, the term is vectorized with a DOCUMENT_QUERY orientation. For document indexing, the orientation is DOCUMENT_RETRIEVAL, which provides a more optimized search and not to verify the similarity between documents."
  },
  "docs/en/getting-started.html": {
    "href": "docs/en/getting-started.html",
    "title": "Welcome | AIVAX",
    "keywords": "Welcome Welcome to AIVAX. Our service makes it easier to develop intelligent AI models that use a knowledge base provided by you to converse with the user, answer questions, provide real-time information, and more. To get started, all endpoints must be made to the AIVAX production URL: https://inference.aivax.net/ Concepts and definitions Understand the concepts used by the API below: Account: represents a user account, which has an authentication token. Collection: represents a collection of knowledge documents. A user can have multiple document collections. Document: represents a fact, a single piece of knowledge, and an item in a collection. A collection can have multiple documents. AI Gateway: represents an AI gateway that benefits from or does not use a knowledge collection, such as a plug-and-play knowledge middleware for a model. Embedded model: represents an AI model that AIVAX provides to the user. Chat client: represents a user interface that makes the AI gateway available through an interactive online chat. Chat session: hosts a conversation and context of a chat client. Handling errors All API errors return an HTTP response with a non-OK status (never 2xx or 3xx), and always follow the JSON format: { \"error\": \"An explanatory error message\", \"details\": {} // an object containing relevant error information. Most of the time it is null }"
  },
  "docs/en/limits.html": {
    "href": "docs/en/limits.html",
    "title": "API Limits | AIVAX",
    "keywords": "API Limits Rate limits regulate the number of requests you can send in a time window. These limits help AIVAX prevent abuse and provide a stable API to everyone. The API limits below are the same for all AIVAX embedded models. These limits are categorized by operations performed by the API. Each account has a tier that defines which limits are applied to the account. Tiers change according to the total invested in AIVAX and the time the account exists. Tier zero (free account): new account that has never added credits. Tier 1: account created at least 48 hours ago and has added any credit value. Tier 2: account created at least 1 month ago and has added at least $100 in credits. Tier 3: account created at least 3 months ago and has added at least $1,000 in credits. The measurement is by credit addition and not by consumption. For example, you don't need to consume $100 in credits to advance to Tier 2. Limit legends: RPM: requests per minute. RPD: requests per day (24 hours). Free Tier 1 Tier 2 Tier 3 Operation RPM RPD Document search 50 - Document insertion - 100 Inference 5 300 Function 5 300 Function (Live) 2 30 Operation RPM RPD Document search 150 - Document insertion - 3,000 Inference 75 10,000 Function 60 10,000 Function (Live) 20 500 Operation RPM RPD Document search 300 - Document insertion - 10,000 Inference 150 - Function 60 - Function (Live) 60 - Operation RPM RPD Document search 1,000 - Document insertion - 30,000 Inference 1,000 - Function 500 - Function (Live) 200 - Document search: includes semantic search of documents in a collection through the search endpoint ../collections/{id}/query. Document insertion: includes creation and modification of documents in a collection. Inference: every inference call, either through the Open-AI compatible API, the /ai-gateways/{id}/inference route, or each message sent by a client chat session. Function: every function call /functions. Function (Live): every function call connected to the internet through internet search (does not include fetch). There is no limit for inference on models defined by you, only on those provided by AIVAX."
  },
  "docs/en/models.html": {
    "href": "docs/en/models.html",
    "title": "Models | AIVAX",
    "keywords": "Models AIVAX provides models from different providers to make development even faster, eliminating the need to configure an account for each provider to access their latest models. See the list below of available models and their pricing. All prices consider the total input and output of tokens, with or without cache. All prices are in United States dollars. aivax Model Name Prices Description @aivax/fn-1 Input: $ 0.10 /1m tokens Output: $ 0.40 /1m tokens Internet Search: $ 5.00 /1,000 searches Optimized agent for function usage. Function Calls Deep Thinking Internet Search Code Execution JSON Functions deepseekai Model Name Prices Description @deepseekai/r1-distill-llama-70b Input: $ 0.75 /1m tokens Output: $ 0.99 /1m tokens Model with deep reasoning and thought, best for more demanding tasks. Deep Thinking google Model Name Prices Description @google/gemini-2.5-pro Input: $ 1.25 /1m tokens Output: $ 10.00 /1m tokens One of the most powerful models today. Input: accepts images, videos, audio Function Calls Deep Thinking @google/gemini-2.5-flash Input: $ 0.30 /1m tokens Output: $ 2.50 /1m tokens Google's best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Input: accepts images, videos, audio Function Calls JSON Functions @google/gemini-2.5-flash-lv Input: $ 0.30 /1m tokens Output: $ 2.50 /1m tokens Internet Search: $ 35.00 /1,000 searches Gemini 2.5 Flash with Google internet search. Input: accepts images, videos, audio Deep Thinking Internet Search @google/gemini-2.5-flash-lite Input: $ 0.10 /1m tokens Output: $ 0.40 /1m tokens A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Input: accepts images, videos, audio Function Calls Deep Thinking JSON Functions @google/gemini-2.0-flash Input: $ 0.10 /1m tokens Output: $ 0.40 /1m tokens Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Input: accepts images, videos, audio Function Calls JSON Functions @google/gemini-2.0-flash-lv Input: $ 0.10 /1m tokens Output: $ 0.40 /1m tokens Internet Search: $ 35.00 /1,000 searches Gemini 2.0 Flash with internet search via Google. Input: accepts images, videos, audio Internet Search @google/gemini-2.0-flash-lite Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Input: accepts images, videos, audio Function Calls JSON Functions @google/gemini-1.5-flash-8b Input: $ 0.04 /1m tokens Output: $ 0.08 /1m tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Input: accepts images, videos, audio Function Calls JSON Functions inception Model Name Prices Description @inception/mercury Input: $ 0.25 /1m tokens Output: $ 1.00 /1m tokens Extremely fast model by generative diffusion. JSON Functions metaai Model Name Prices Description @metaai/llama-3.3-70b Input: $ 0.59 /1m tokens Output: $ 0.79 /1m tokens Previous generation model with many parameters and surprisingly fast speed. Function Calls JSON Functions @metaai/llama-4-maverick-17b-128e Input: $ 0.20 /1m tokens Output: $ 0.60 /1m tokens Fast model, with 17 billion activated parameters and 128 experts. Input: accepts images Function Calls JSON Functions @metaai/llama-4-scout-17b-16e Input: $ 0.11 /1m tokens Output: $ 0.34 /1m tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Input: accepts images Function Calls JSON Functions @metaai/llama-3.1-8b Input: $ 0.05 /1m tokens Output: $ 0.08 /1m tokens Cheap and fast model for less demanding tasks. Function Calls JSON Functions model-router Model Name Prices Description @model-router/gemini Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash Lite, Gemini 2.0 Flash and Gemini 2.5 Flash. @model-router/gemini-high Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash, Gemini 2.5 Flash (low) and Gemini 2.5 Flash (high). @model-router/openai Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 4.1 Nano, GPT 4.1 Mini and o4-mini. @model-router/openai-high Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 4.1 Mini, o4-mini (low) and o4-mini (high). @model-router/llama Input: $ 0.08 /1m tokens Output: $ 0.30 /1m tokens Model router for Meta Llama. The routing is made between Llama 4 Scout, Llama 4 Maverick and Llama 3.3 70b. openai Model Name Prices Description @openai/gpt-4o Input: $ 2.50 /1m tokens Output: $ 10.00 /1m tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Input: accepts images Function Calls @openai/gpt-4.1 Input: $ 2.00 /1m tokens Output: $ 8.00 /1m tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Input: accepts images Function Calls @openai/o4-mini Input: $ 1.10 /1m tokens Output: $ 4.40 /1m tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Input: accepts images Function Calls Deep Thinking @openai/gpt-4.1-mini Input: $ 0.40 /1m tokens Output: $ 1.60 /1m tokens Fast and cheap for focused tasks. Input: accepts images Function Calls @openai/gpt-4o-mini Input: $ 0.15 /1m tokens Output: $ 0.60 /1m tokens Smaller version of 4o, optimized for everyday tasks. Input: accepts images Function Calls @openai/gpt-4.1-nano Input: $ 0.10 /1m tokens Output: $ 0.40 /1m tokens The fastest and cheapest GPT 4.1 model. Input: accepts images Function Calls qwen Model Name Prices Description @qwen/qwen3-32b Input: $ 0.29 /1m tokens Output: $ 0.59 /1m tokens Latest generation of LLMs in the Qwen series, offering advancements in reasoning, instruction-following, agent capabilities, and multilingual support. Deep Thinking @qwen/qwq-32b Input: $ 0.29 /1m tokens Output: $ 0.39 /1m tokens Conversational model with thinking and reasoning for solving complex tasks. Deep Thinking"
  },
  "docs/en/protocol-functions.html": {
    "href": "docs/en/protocol-functions.html",
    "title": "Server-side Functions | AIVAX",
    "keywords": "Server-side Functions The AIVAX protocol functions, or server-side functions, are a customized implementation of function calls created by AIVAX that allows the model to strictly follow a function context that is not based on JSON documents. Similar to MCP, but a bit simpler. The protocol functions enable actions to be taken on the AIVAX server-side, removing the need to implement the function on the client-side and integrating with existing applications and services. These functions expect a callback through a URL, and when the model decides to call the function, the callback is accessed with the parameters informed by the assistant itself. The assistant does not know which URL it is calling, as it remains invisible to both the assistant and the user. Choosing the Function Name The function name should be simple and deterministic to what the function does. Avoid names that are difficult to guess or do not refer to the function's role, as the assistant may become confused and not call the function when appropriate. For example, let's consider a function to query a user in an external database. The following names are good examples to consider for the call: search-user query-user search_user Bad names include: search (too broad) query-user-in-database-data (name too large) pesquisa-usuario (name not in English) search user (name with improper characters) Having the function name, we can think about the function description. Choosing the Function Description The function description should conceptually explain two situations: what it does and when it should be called by the assistant. This description should include scenarios that the assistant should consider calling it and when it should not be called, providing a few examples of calls (one-shot) and/or making the function's rules explicit. Defining Protocol Functions These functions are defined in the AI-gateway, which allows the creation of intelligent agents that perform actions without human intervention. They follow a simple syntax, expecting the function name, a description of what it does, and the invocation parameters. Protocol functions are defined in the AI gateway following the JSON: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctions\": [ { \"name\": \"list-clients\", \"description\": \"Use this tool to list and search for the user's clients.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use this tool to get details and orders from a client through their ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"user_id\": \"guid\" } } ] } } In the snippet above, you are providing two functions for your AI model: list-clients and view-client, which will decide which one to execute during its reasoning. You can also provide a JSON content format for which the model will call your API providing the informed content. You can also define the list of supported functions through an endpoint. Every time the model receives a message, it will consult the provided endpoint to get a list of functions it can execute. Define the function listing endpoints in your AI gateway: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctionSources\": [ \"https://my-external-api.com/api/scp/listings\" ] } } The function provision endpoint must respond following the format: GET https://my-external-api.com/api/scp/listings { \"functions\": [ { \"name\": \"list-clients\", \"description\": \"Use this tool to list and search for the user's clients.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use this tool to get details and orders from a client through their ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"user_id\": \"guid\" } } ] } These functions are stored in cache for 10 minutes before a new request is made to the provided endpoint. Handling Function Calls The functions are invoked at the endpoint provided in callbackUrl through an HTTP POST request, with the body: { \"function\": { \"name\": \"view-client\", \"content\": { \"user_id\": \"3e5a2823-98fa-49a1-831a-0c4c5d33450e\" } }, \"context\": { \"externalUserId\": \"...\", \"moment\": \"2025-05-18T03:36:27\" } } The response to this action must always respond with an HTTP OK status (2xx or 3xx), even for errors that the assistant may have made. A non-OK response will indicate to the assistant that it was not possible to call the function and it will not continue with what it was planning to do. Response Format Successful responses must be textual and will be attached as a response to the function in the way it is responded by the endpoint. There is no JSON format or structure for this response, but it is advisable to provide a simple, human-readable response, so that the assistant can read the result of the action. Errors can be common, such as not finding a client by ID or some field not being in the desired format. In these cases, respond with an OK status and in the response body include a human description of the error and how the assistant can work around it. It is guaranteed that the request will strictly follow the content format provided by the function definition. Functions that do not expect arguments should not specify a content format for that function. You can also indicate to the model how it should fill in the function content fields in the function instructions. More complex, nested, or deeply structured content may increase the time it takes to generate this content, as it increases the chance of the assistant making mistakes and failing to validate the generated content. Important The more functions you define, the more tokens you will consume in the reasoning process. The function definition, as well as its format, consumes tokens from the reasoning process. Authentication The authentication of requests is done through the X-Aivax-Nonce header sent in all protocol function requests, including listing requests. See the authentication manual to understand how to authenticate reverse AIVAX requests. User Authentication Function calls send a $.context.externalUserId field containing the user tag created in a chat session. This tag can be used to authenticate the user who called this function. Security Considerations For the AI model, only the function name, description, and format are visible. It is not capable of seeing the endpoint to which that function points. Additionally, it does not have access to the user tag that is authenticated in a chat client."
  },
  "docs/entities/ai-gateway.html": {
    "href": "docs/entities/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway Os gateways de AI é um serviço que a AIVAX fornece para criar um túnel de inferência entre um modelo de LLM e uma base de conhecimento. Nele é possível: Criar um modelo com instruções personalizadas Usar um modelo provido por você através de um endpoint OpenAI compatível, ou usar um modelo disponibilizado pela AIVAX Personalizar parâmetros de inferência, como temperatura, top_p, prefill Usar uma coleção de conhecimento como fundação de respostas para IA Dentre outros recursos. Com o AI Gateway, você cria um modelo pronto para uso, parametrizado e fundamentado nas instruções que você definir. Modelos Você pode trazer um modelo de IA compatível com a interface OpenAI para o gateway de IA. Se você trazer seu modelo de IA, iremos cobrar apenas pela pesquisa de documentos anexada na IA. Você também pode usar um dos modelos abaixo que já estão prontos para começar com o AIVAX. Ao usar um modelo, você perceberá que alguns são mais inteligentes que outros para determinadas tarefas. Alguns modelos são melhores com certas estratégias de obtenção de dados do que outros. Realize testes para encontrar o melhor modelo. Você pode ver os modelos disponíveis na página de modelos. Escolhendo uma estratégia de busca Se você for usar uma coleção de conhecimento com um modelo de IA, você poderá escolher uma estratégia que a IA usará para realizar uma busca por informação. Cada estratégia é mais refinada que a outra. Algumas criam resultados melhores que as demais, mas é importante realizar testes práticos com várias estratégias para entender qual se ajusta melhor no modelo, conversa e tom do usuário. Talvez seja necessário realizar ajustes no prompt do sistema para informar melhor como a IA deverá considerar os documentos anexados na conversa. Os documentos são anexados como uma mensagem do usuário, limitados aos parâmetros que você define na estratégia de obtenção. Estratégias com reescrita normalmente geram os melhores resultados à um baixo custo de latência e custo. O modelo de reescrita usado sempre o com menor custo, escolhido normalmente por um pool interno que decide o modelo que está com menor latência no momento. Estratégias sem custo de reescrita: Plain: a estratégia padrão. É a menos otimizada e não possui custo de reescrita: a última mensagem do usuário é usada como termo de busca para pesquisar na coleção anexada do gateway. Concatenate: Concatena em linhas as últimas N mensagens do usuário, e então o resultado da concatenação é usada como termo de busca. Estratégias com custo de reescrita (os tokens de inferência são cobrados): UserRewrite: reescreve as últimas N mensagens do usuário usando um modelo menor, criando uma pergunta contextualizada no que o usuário quer dizer. FullRewrite: reescreve as últimas N*2 mensagens do chat usando um modelo menor. Similar ao UserRewrite, mas considera também as mensagens da assistente na formulação da nova pergunta. Geralmente cria as melhores perguntas, com um custo um pouco maior. É a estratégia mais estável e consistente. Funciona com qualquer modelo. Estratégias de função: QueryFunction: fornece uma função de pesquisa na coleção integrada para o modelo de IA. Você deverá ajustar nas instruções do sistema os cenários ideais para o modelo chamar essa função quando necessário. Pode não funcionar tão bem em modelos menores. Usar um gateway de IA A AIVAX provê um endpoint compatível com a interface OpenAI através de um AI-gateway, o que facilita a integração do modelo criado pela AIVAX com aplicações e SDKs existentes. Vale ressaltar que somente algumas propriedades são suportadas. Em um gateway de IA, você já configura os parâmetros do modelo, como System Prompt, temperatura e nome do modelo. Ao usar esse endpoint, alguns valores do gateway podem ser sobrescritos pela requisição. Requisição POST /api/v1/chat-completions { \"model\": \"0197cb0f-893a-7b7d-be0a-71ada1208aaf\", \"messages\": [ { \"role\": \"user\", \"content\": \"Quem é você?\" } ], \"stream\": false } Resposta para não-streaming { \"id\": \"0197dbdc-6456-7d54-b7ec-04cb9c80f460\", \"object\": \"chat.completion\", \"created\": 1751740343, \"model\": \"@aivax/sentinel-mini\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"completion_text\": \"Oi! 😊 Eu sou a Zia, a assistente inteligente do Zé do Ingresso. Tô aqui pra te ajudar com tudo sobre ingressos, eventos, e tudo que rola na nossa querida São José do Rio Preto. Se precisar de alguma coisa, tipo saber sobre nomeação de ja o bagulho, só chamar! Vamos juntos aproveitar tudo o que tiver rolando! O que você precisa? 🥳 \", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 1118, \"completion_tokens\": 88, \"total_tokens\": 1206 }, \"sentinel_usage\": null, \"service_tier\": \"default\" } Resposta streaming data: {\"id\":\"0197dbde-c40b-720d-a13e-f689c303c571\",\"object\":\"chat.completion.chunk\",\"created\":1751740498,\"model\":\"gpt-4o-mini\",\"system_fingerprint\":\"fp_y8jidd\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} ... data: {\"id\":\"0197dbde-c88c-7764-8017-c1fee0d79096\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"gpt-4o-mini\",\"system_fingerprint\":\"fp_2he7ot\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} data: {\"id\":\"0197dbde-c890-7329-9e65-faecbe158efa\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@aivax\\/sentinel-mini\",\"system_fingerprint\":\"fp_q6qh7x\",\"choices\":[{\"index\":0,\"finish_reason\":\"STOP\",\"delta\":{}}],\"usage\":{\"prompt_tokens\":1097,\"completion_tokens\":92,\"total_tokens\":1189},\"sentinel_usage\":null}"
  },
  "docs/entities/chat-clients.html": {
    "href": "docs/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients Um cliente de chat provê uma interface de usuário através de um AI Gateway que permite o usuário conversar com sua assistente. Um chat client é integrado à inferência do AI gateway e dá suporte para pensamento profundo, pesquisa e conversa por texto. Recursos multi-modais, como envio de imagens e áudio estão em desenvolvimento. Note A AIVAX nunca armazena o conteúdo de um chat entre um cliente e o usuário. Você pode usar JavaScript para essa tarefa, mas sob sua responsabilidade de uso e armazenamento. Você pode personalizar a interface do seu chat client com CSS e JavaScript personalizado, além de poder escolher a linguagem dos recursos do chat. Criar uma sessão de chat Uma sessão de chat é onde você cria uma conversa entre seu chat client e o usuário. Você pode chamar esse endpoint informando contexto adicional para conversa, como o nome do usuário, onde ele está, etc. Uma sessão de chat expira após algum tempo por segurança do token de acesso gerado. Quando você chama esse endpoint informando uma tag você pode chamar o mesmo endpoint várias vezes e obter a sessão de chat que está ativa para a tag informada, ou criar um chat novo se não existir uma sessão em andamento. Uma sessão de chat também restaura todas as mensagens da conversa da mesma sessão após desconexão. O usuário pode limpar a conversa ao clicar no botão de limpar conversa no canto superior direito do cliente de chat. Essa sessão usa os limites definidos pelo cliente de chat, como máximo de mensagens e tokens na conversa. Uma sessão é automaticamente renovada por mais 3 dias ao receber uma mensagem do usuário. Important Só é possível determinar a quantidade de tokens usados em uma mensagem ao usar um modelo provido pela AIVAX. Se você usar um modelo externo, a propriedade limitingParameters.userInputMaxTokens será ignorada. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Opcional. Contexto adicional para a IA sobre o chat. \"extraContext\": \"# Contexto adicional\\r\\n\\r\\nVocê está falando com Eduardo.\", // Tempo em segundos para o chat expirar. O mínimo é 10 minutos. O máximo é 30 dias. \"expires\": 3600, // Opcional (recomendado). Um id externo para identificar a sessão posteriormente e reaproveitá-la sempre que chamar o mesmo endpoint. Pode ser o ID do usuário do seu banco de dados ou uma string que facilite a identificação desse chat posteriormente. \"tag\": \"my-user-tag\" } Resposta { \"message\": null, \"data\": { // ID da sessão de chat criada. \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Chave pública de acesso do chat. \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // A URL pública para conversar com o chat. \"talkUrl\": \"https://preview-s01.proj.pw/www/web-chat-clients/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } }"
  },
  "docs/entities/collections.html": {
    "href": "docs/entities/collections.html",
    "title": "Coleções | AIVAX",
    "keywords": "Coleções Uma coleção é uma biblioteca de conhecimento: ela abriga vários documentos de conhecimento. Use coleções para agrupar documentos por finalidade, como documentar um produto, uma empresa, um serviço ou fluxo. Coleções não produzem custo. Não há limite de coleções por conta. Criar uma coleção Para criar uma coleção vazia, informe apenas o nome dela: Requisição POST /api/v1/collections { // O nome da coleção não pode ser vazio. \"collectionName\": \"Minha primeira coleção\" } Resposta { \"message\": null, \"data\": { // ID único da coleção criada. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } Listar coleções Lista as coleções disponíveis na sua conta. Requisição GET /api/v1/collections Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"Minha coleção\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Outra coleção\" } ] } } Ver uma coleção Obtém detalhes de uma coleção, como seu progresso de indexação e informações como data de criação. Requisição GET /api/v1/collections/{collection-id}/ Resposta { \"message\": null, \"data\": { \"name\": \"Minha coleção\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // traz a quantidade de documentos aguardando indexação \"queuedDocuments\": 0, // quantidade de documentos pronto para consulta \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Excluir uma coleção Exclui uma coleção e todos os documentos nela. Essa ação é irreversível. Requisição DELETE /api/v1/collections/{collection-id}/ Resposta { \"message\": \"Collection deleted successfully.\", \"data\": null } Limpar uma coleção Diferente da exclusão de coleção, essa operação remove todos os documentos da coleção, incluindo os indexados e os em fila. Requisição DELETE /api/v1/collections/{collection-id}/reset-only Resposta { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/entities/documents.html": {
    "href": "docs/entities/documents.html",
    "title": "Documentos | AIVAX",
    "keywords": "Documentos Um documento representa um pedaço de um conhecimento. É um trecho limitado, autosuficiente e que faça sentido de forma isolada. Um documento é o componente que é indexado pelo modelo interno para ser recuperado posteriormente através de um termo de busca semântico. Considere um manual sobre um carro: ele não é um documento mas sim vários documentos. Cada um destes documentos fala, de forma isolada, sobre um determinado assunto sobre esse carro, de forma que esse documento não dependa de um contexto ou informação externa para fazer sentido. Cada documento deste manual irá falar de um assunto: um irá falar sobre como ligar o carro, outro de como desligá-lo, outro de como sua pintura é feita e outro de como trocar o óleo periodicamente. Não é uma boa ideia reservar um documento para falar de várias coisas ao mesmo tempo, pois isso irá reduzir a objetividade e escopo da inferência e reduzir a qualidade de obtenção. Exemplos de criação de documentos: ❌ Não faça Não crie documentos muito curtos (com 10 ou menos palavras). Não crie documentos muito grandes (com 700) ou mais palavras. Não fale sobre mais de uma coisa em um documento. Não misture linguas diferentes em documentos. Não seja implícito em documentos. Nâo escreva documentos usando linguagem técnica, como códigos ou estruturas como JSON. ✅ Faça Seja explícito sobre o objetivo do seu documento. Foque documentos em assuntos individuais, que resumam o que deve ser feito ou explicado. Sempre repita termos que são palavras-chave para a busca do documento. Exemplo: prefira usar \"A cor do Honda Civic 2015 é amarela\" ao invés de \"a cor do carro é amarelo\". Restrinja o conteúdo do documento para falar de apenas um tópico ou assunto. Use uma linguagem humana, simples e fácil de entender. Uso da API Como todos os documentos são entidades que pertencem à uma coleção, sempre tenha em mãos a coleção de onde o documento está/será localizado. Enviar documentos em lote Para enviar uma lista em massa de documentos para uma coleção, estruture-os seguindo o formato JSONL. A estrutura do arquivo de indexação é: {\"docid\":\"Carros/HondaCivic2015.rmd:1\",\"text\":\"O Honda Civic 2015 está disponível em [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:2\",\"text\":\"O motor do Honda Civic 2015 é [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:3\",\"text\":\"A cor do Honda Civic 2015 é Amarela [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} ... A estrutura é compsta pelas propriedades: Propriedade Tipo Descrição docid string Especifica o nome do documento. Útil para depuração e identificação. text string O conteúdo \"cru\" do documento que será indexado. __ref string Opcional. Especifica um ID de referência do documento. __tags string[] Opcional. Especifica um array de tags do documento. Útil para gestão de documentos. A referência de um documento é um ID que pode ser especificado em vários documentos que precisam estar vinculados em uma busca quando um dos mesmos for correspondido em uma busca de similaridade. Por exemplo, se uma busca encontrar um documento que possui um ID de referência, todos os outros documentos da mesma coleção que compartilham o mesmo ID de referência do documento correspondido também serão incluídos na resposta da busca. O uso de referências pode ser útil para quando um documento depende de outro ou mais documentos para fazer sentido. Não há exigência de formato para o ID de referência: qualquer formato é aceito. Você pode enviar até 1.000 linhas de documentos por requisição. Se precisar enviar mais documentos, separe o envio em mais requisições. Se você enviar um documento com mais de 1.000 linhas, as linhas seguintes serão ignoradas. Vale notar que documentos muito longos, que excede a quantidade de tokens permitida no modelo de embedding interno, terão seu conteúdo truncado e a qualidade de indexação poderá ser gravemente afetada. Para evitar esse problema, envie documentos que contenham entre 20 e 700 palavras. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do conteúdo de cada documento. O conteúdo de cada documento é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição O envio deve ser feito usando multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Resposta { \"message\": null, \"data\": [ { \"name\": \"Institucional/Empresa.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institucional/Empresa.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institucional/Empresa.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Produtos/Agendamentos.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Criar ou modificar documento Esse endpoint cria ou modifica um documento a partir do seu nome. Quando um documento é modificado, seus vetores de indexação são resetados, isto é, o documento entrará em fila novamente para ser indexado pelo motor de indexação. Essa indexação não é isenta de custo. O custo é relativo à quantidade de tokens do conteúdo enviado. O custo somente é gerado quando o documento é de fato alterado. Chamar essa rota com o mesmo conteúdo do documento não gera modificação, portanto, não gera custo. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do conteúdo do arquivo. O conteúdo do arquivo é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição PUT /api/v1/collections/{collection-id}/documents { // o nome do documento que será modificado \"name\": \"document-name\", // o conteúdo do documento que será criado ou sobreposto caso o nome já exista \"contents\": \"Conteúdo do meu documento\", // parâmetros explicados anteriormente \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Resposta { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // o estado da operação indica se o documento foi modificado \"Modified\" ou criado \"Created\". Sempre virá apenas um valor no array. \"state\": [\"Modified\"] } } Listar documentos Esse endpoint lista todos os documentos disponíveis em uma coleção. Você pode passar um parâmetro da query adicional filter para filtrar documentos por nome, tag ou conteúdo. Esse filtro suporta expressões que auxiliam a filtrar o que você está procurando: -t \"tag\" - filtra documentos que possuem essa tag. -r \"reference\" - filtra documentos que possuem esse ID de referência. -c \"content\" - filtra documentos que possuem esse trecho em seu conteúdo. -n \"name\" - filtra documentos que possuem esse trecho em seu nome. Requisição GET /api/v1/collections/{collection-id}/documents Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Ajuda/Clientes.rmd:1\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"Um cliente é um cadastro na sua platafor...\", \"indexState\": [\"Indexed\"] }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Ajuda/Clientes.rmd:2\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"No cadastro do cliente, é possível modif...\", \"indexState\": [\"Indexed\"] }, ... ] } } Ver documento Vê detalhes sobre um documento específico. Requisição GET /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institucional/Empresa.rmd:1\", // representa a situação de indexação do documento. // valores válidos: Queud, Indexed, Cancelled \"state\": [\"Indexed\"], // conteúdo do documento indexado \"contents\": \"...\", // id da referência do documento \"reference\": \"institucional-empresa\" } } Excluir documento Permanentemente exclui um documento através do seu ID. Requisição DELETE /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/entities/functions.html": {
    "href": "docs/entities/functions.html",
    "title": "Funções | AIVAX",
    "keywords": "Funções Funções é uma forma de forçar seu modelo à processamento de informações usando JSON como intermédio de comunicação. Com as funções, você consegue fazer qualquer modelo responder no formato JSON que você quiser. Pode ser útil para categorizar comentários, aplicar moderação em avaliações ou processar informações com auxílio da IA. No momento, só é possível usar funções com modelos providos pela AIVAX. Chamar uma função Para chamar uma função de IA, você precisará informar o que a IA deverá responder e fornecer um esquema JSON que ela deverá seguir. Modelos menos inteligentes tendem a falhar a geração de JSON, gerando um documento inválido ou problemático. Para isso, ajuste seu modelo, a instrução e o parâmetro de tentativas se for necessário. Você é cobrado por cada tentativa que a IA tentar gerar. Modelos um pouco mais inteligentes tendem a gerar resultados corretos na primeira tentativa. É garantido que um JSON válido será gerado e que esse JSON seguirá o mesmo esquema fornecido na requisição. Adicionalmente, você pode optar em ativar pesquisa na internet para chamada de função. Essa opção pode ser útil para trazer dados relevantes em tempo real ao estruturar sua resposta. Ao usar essa função, um modelo com acesso na internet será usado para obter dados da internet para estruturar sua resposta. Esse modelo também tentará estruturar sua resposta a partir dos dados fornecidos, e se conseguir formular um JSON válido a etapa de chamar o modelo de estruturação é ignorada e a resposta é imediatamente retornada. Se for o caso do modelo de busca online não conseguir estruturar um JSON válido, o modelo escolhido na requisição ficará responsável por essa tarefa, e irá começar o encadeamento de tentativas de geração. Modelos mais inteligentes acertam a geração nas primeiras tentativas. Através da propriedade fetch, você pode fornecer uma lista de URLs para serem anexadas no contexto da geração. O AIVAX faz uma requisição GET para acessar os conteúdos fornecidos e renderiza-os no conteúdo da requisição. Somente respostas 2xx ou 3xx são aceitas e o conteúdo da resposta deve ser textual. Respostas em HTML são sanitizadas para incluirem somente o texto da página, sem script e CSS. O tamanho máximo que pode ser lido de uma URL do fetch é 10 Mb. O máximo de itens para o fetch são 10 URLs. Requisições que pesquisam na internet trazem bons resultados e dispensam crawlers, scrappers ou a necessidade de pagar por uma API específica, mas podem ser custosas e relativamente lentas para serem obtidas. Considere usar um cache do lado da sua aplicação para dados que não precisam ser constantementes atualizados, como dados meteorológicos, estatísticas diárias, etc. A AIVAX não realiza nenhum cache pelo nosso lado. Requisição POST /api/v1/functions/json { // Obrigatório. Especifique o nome do modelo integrado que será usado para realizar a ação. \"modelName\": \"@metaai/llama-3.1-8b\", // Obrigatório. Explique o que seu modelo deverá fazer com a entrada e como ele deve trazer a resposta. \"instructions\": \"Classifique o comentário do usuário, indicando se é positivo ou negativo, e se possui alguma informação relevante (número entre 0 (pouco relevante) e 10 (muito relevante))\", // Obrigatório. O objeto JSON que o modelo deverá gerar. Você pode fornecer exemplos de geração no campo de instruções. Esse objeto deve ser um JSON válido na API. // Esse objeto deve ser um objeto, um array ou uma string. \"responseSchema\": { \"feedbackType\": \"{neutral|positive|negative}\", \"informationScore\": 5 }, // Opcional. Especifica uma lista de caminhos JSON que a IA deve gerar conteúdo sempre e que esse campo não pode ser nulo. Para arrays, especifique com [*]. \"requiredFields\": [ \"$.feedbackType\", \"$.informationScore\" ], // Opcional. Define uma entrada JSON para o modelo. Pode ser qualquer tipo de valor JSON. \"inputData\": { \"userComment\": \"Pessimo mercado. Tem guarda dentro te vigiando pra vc nao roubar e os acougueiros te ignoram e atendem mocinhas bonitinhas na tua frente. Mas graças a Deus tem outros mercados chegando e o fim dessa palhaçada vai chegar\" }, // Opcional. Define quantas tentativas o modelo deve tentar antes da API retornar um erro. Deve ser um número entre 1 e 30. \"maxAttempts\": 10, // Opcional. Define o tempo limite em segundos para obter um JSON válido antes da API retornar um erro. Deve ser um número entre 1 e 3600 (uma hora). \"timeout\": 300, // Opcional. Define a temperatura de geração do JSON. Valores maiores tendem a serem mais criativos, enquanto menores mais determinísticos. Número de 0 à 2. \"temperature\": 0.4, // Opcional. Adiciona recursos externos para complementar a geração da resposta. \"fetch\": { // Obrigatório. Fornece a lista de URLS que o AIVAX irá acessar para complementar a geração de resposta. O máximo são 10 URLs. \"urls\": [ \"https://url1...\", \"https://url2...\", ], // Opcional. Define o comportamento do fetch para erros ao tentar acessar o site. Erros incluem respostas que não são 2xx ou 3xx, timeouts, erros de certificados, etc. // fail -> retorna um erro na resposta da função (padrão) // warn -> adiciona um aviso na resposta da função e não inclui o erro na geração da IA // ignore -> ignora o erro e adiciona o erro na geração da IA \"fetchFailAction\": \"fail\" | \"warn\" | \"ignore\", // Opcional. Define o timeout em segundos para o tempo maximo da resposta responder e ler os conteúdos. O máximo é 120 segundos (dois minutos). \"timeout\": 10, // Opcional. Define o tamanho máximo do conteúdo em quantidade de caracteres que podem ser incluídos na geração da IA antes de serem truncados. \"pageMaxLength\": 2048 } } Resposta { \"message\": null, \"data\": { // o resultado contém o objeto definido em \"responseSchema\", com os campos preenchidos pela IA \"result\": { \"feedbackType\": \"negative\", \"informationScore\": 8 }, // em qual tentativa a IA conseguiu um JSON válido \"attempt\": 1, // o tempo em milissegundos para obter um JSON válido \"elapsedMilliseconds\": 527, // avisos produzidos pela geração \"warnings\": [] } } Considerações sobre o esquema JSON Especifique valores enumerados com \"{valor1|valor2|valor3}\". Dessa forma, o modelo deverá escolher um dos valores apresentados na geração do JSON. Todos os valores são placeholders para a geração do modelo. Indique o que um campo é ou o que deve receber de valor com um hint em seu próprio placeholder ou indique diretamente nas instruções da função. Todos os valores podem ser nulos, à menos que você especifique diretamente para o modelo que não podem. A estrutura de saída do modelo é a mesma que informada em responseSchema. A estrutura de entrada é indiferente. Funções em tempo real com modelos Sentinel Você pode usar o agente @aivax/sentinel-lambda para executar funções inteligentes que envolvam pesquisa na internet, execução de código, resolução de contas matemáticas e todas as outras funcionalidades que agentes Sentinel consigam fornecer. Agentes Sentinel são conectados à internet por padrão, portanto, é natural que ele pesquise algo na internet para complementar sua resposta. Ao usar chamar uma função com um modelo que pesquisa na internet, como um agente Sentinel, o limite de consumo contabilizado é de Live Function. Exemplos Confira exemplos de funções de IA para várias tarefas cotidianas: Resumir pedido e classificar se requer atenção ou não POST /api/v1/functions/json { \"modelName\": \"@metaai/llama-4-scout-17b\", \"instructions\": \"Resuma o comentário do usuário, criando uma descrição curta, com no máximo de 10 palavras indicando o que ele quer fazer. Indique também se esse comentário requer atenção ou não.\", \"responseSchema\": { \"shortSummary\": \"...\", \"requiresAttention\": false }, \"inputData\": \"O cliente fernando de castro está tentando entrar em contato com o suporte desde sexta-feira e diz q vai cancelar se nao falar com alguém hoje. ele tbm disse que é amigo da rebeca do comercial e está ameaçando falar mal da empresa no tiktok. por favor alguém atende esse cara??\" } { \"message\": null, \"data\": { \"result\": { \"shortSummary\": \"Cliente quer contato com suporte para evitar cancelamento e ameaça\", \"requiresAttention\": true }, \"attempt\": 1, \"elapsedMilliseconds\": 639 } } Avaliar uma expressão matemática POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Avalie a conta matemática informada e forneça o resultado.\", \"responseSchema\": { \"result\": \"...\" }, \"inputData\": { \"mathProblem\": \"12 + (42 / pi) ^ 20\" } } { \"message\": null, \"data\": { \"result\": { \"result\": \"3.3265063290400284e+22\" }, \"attempt\": 0, \"elapsedMilliseconds\": 3869, \"warnings\": [] } } Trazer últimas notícias e clima de uma determinada cidade POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Pesquise as 5 últimas notícias e dados meteorológicos para a cidade informada.\", \"responseSchema\": { \"latestNews\": [ { \"title\": \"...\", \"details\": \"...\", \"link\": \"https://...\" } ], \"weather\": { \"currentTemperature\": 0, \"currentWeather\": \"{sunny|cloudy|rain|thunderstorm}\", \"forecast\": \"{sunny|cloudy|rain|thunderstorm}\" } }, \"inputData\": { \"city\": \"São José do Rio Preto\" } } { \"message\": null, \"data\": { \"result\": { \"latestNews\": [ { \"title\": \"Festival Paralímpico no Instituto Lucy Montoro\", \"details\": \"Familiares e pacientes do Instituto de Reabilitação Lucy Montoro de Rio Preto (SP) participaram neste sábado (14) do Festival Paralímpico.\", \"link\": \"https://g1.globo.com/sp/sao-jose-do-rio-preto-aracatuba/cidade/sao-jose-do-rio-preto/\" }, { \"title\": \"Investimento imobiliário na zona Leste de Rio Preto\", \"details\": \"Empreendimentos residenciais marcam o maior investimento imobiliário na zona Leste de Rio Preto.\", \"link\": \"https://www.gazetaderiopreto.com.br/\" }, { \"title\": \"Acidente na Washington Luís\", \"details\": \"Motociclista é hospitalizado após se envolver em acidente na Washington Luís, em Rio Preto.\", \"link\": \"https://www.diariodaregiao.com.br/cidades\" }, { \"title\": \"Curso gratuito para artistas\", \"details\": \"Curso gratuito ensina artistas de Rio Preto e região a escrever projetos para editais culturais.\", \"link\": \"https://www.diariodaregiao.com.br/\" }, { \"title\": \"PIX automático\", \"details\": \"Modalidade permite pagar contas recorrentes, como de energia, telefone, escolas, academias, entre outras, de forma automática pelo PIX.\", \"link\": \"https://g1.globo.com/sp/sao-jose-do-rio-preto-aracatuba/\" } ], \"weather\": { \"currentTemperature\": 23, \"currentWeather\": \"sunny\", \"forecast\": \"sunny\" } }, \"attempt\": 0, \"elapsedMilliseconds\": 10990, \"warnings\": [] } } Trazer estatísticas da COVID-19 em tempo real POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Traga a contagem de casos e mortes por COVID-19.\", \"responseSchema\": { \"deathsWorld\": 0, \"deathsBrazil\": 0, \"casesWorld\": 0, \"casesBrazil\": 0 } } { \"message\": null, \"data\": { \"result\": { \"deathsWorld\": 7010681, \"deathsBrazil\": 711380, \"casesWorld\": 704753890, \"casesBrazil\": 38743918 }, \"attempt\": 1, \"elapsedMilliseconds\": 1620 } } Trazer artistas em alta por gênero musical POST /api/v1/functions/json { \"modelName\": \"@aivax/sentinel-lambda\", \"instructions\": \"Pesquise e formate uma lista de 10 artistas no TOP 10 do streaming musical por gênero.\", \"responseSchema\": { \"edm\": [ \"artist name\", \"artist name\", \"...\" ], \"rap\": [ \"artist name\", \"artist name\", \"...\" ], \"pop\": [ \"artist name\", \"artist name\", \"...\" ] }, \"inputData\": null } { \"message\": null, \"data\": { \"result\": { \"edm\": [ \"Marshmello\", \"Swedish House Mafia\", \"Skrillex\", \"Illenium\", \"John Summit\", \"Kaskade\", \"Daft Punk\", \"Deadmau5\", \"Avicii\", \"Calvin Harris\" ], \"rap\": [ \"Drake\", \"Kendrick Lamar\", \"Eminem\", \"Travis Scott\", \"J. Cole\", \"Nicki Minaj\", \"21 Savage\", \"Metro Boomin\", \"Lil Wayne\", \"Bad Bunny\" ], \"pop\": [ \"Taylor Swift\", \"Bruno Mars\", \"The Weeknd\", \"Lady Gaga\", \"Billie Eilish\", \"Sabrina Carpenter\", \"Ariana Grande\", \"Ed Sheeran\", \"Post Malone\", \"Doja Cat\" ] }, \"attempt\": 0, \"elapsedMilliseconds\": 14961, \"warnings\": [] } }"
  },
  "docs/entities/search.html": {
    "href": "docs/entities/search.html",
    "title": "Pesquisa | AIVAX",
    "keywords": "Pesquisa A API de pesquisa, através da query key obtida das coleções, realiza uma busca semântica na mesma, realizando uma comparação inteligente para cada documento indexado em uma coleção. Após criar uma coleção, você obterá seu ID. Utilize o ID da sua coleção para realizar a busca nos documentos indexados da mesma. Use os endpoints dessa API para embutir a pesquisa semântica de documentos no seu modelo de IA ou chatbot. Pesquisando documentos Esse endpoint espera uma requisição GET com os parâmetros: term: obrigatório. Especifica o termo de pesquisa que será pesquisado nos documentos. top: Especifica o máximo de documentos que deverão ser retornados na busca. min: Especifica o score mínimo para obtenção dos documentos. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do termo de busca. O termo de busca é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição GET /api/v1/collections/{collection-id}/query term=Qual a cor do honda CIVIC? Resposta { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } Para o resultado da busca, quanto maior o score, mais semelhante é o documento para o termo da busca. O AIVAX utiliza modelos de embedding que permitem a orientação da tarefa. Para a busca, o termo é vetorizado com uma orientação DOCUMENT_QUERY. Para indexação dos documentos, a orientação é DOCUMENT_RETRIEVAL, o que fornece uma busca mais otimizada e não para averiguar a similaridade entre documentos."
  },
  "docs/getting-started.html": {
    "href": "docs/getting-started.html",
    "title": "Bem-vindo | AIVAX",
    "keywords": "Bem-vindo Boas vindas ao AIVAX. Nosso serviço torna mais fácil o desenvolvimento de modelos de IA inteligentes que usam uma base de conhecimento providenciada por você para conversar com o usuário, responder perguntas, fornecer informações em tempo real e mais. Para começar, todos os endpoints devem ser feitos na URL de produção da AIVAX: https://inference.aivax.net/ Conceitos e definições Entenda os conceitos usados pela API abaixo: Conta: representa uma conta do usuário, que possui um token de autenticação. Coleção: representa uma coleção de documentos de conhecimento. Um usuário pode ter várias coleções de documentos. Documento: representa um fato, um único conhecimento e um item de uma coleção. Uma coleção pode ter vários documentos. AI Gateway: representa um gateway de IA que se beneficia ou não de uma coleção de conhecimento, como um middleware de conhecimento plug-and-play para um modelo. Modelo embutido: representa um modelo de IA que o AIVAX provê para o usuário. Chat client: representa uma interface de usuário que disponibiliza o AI gateway através de um chat interagível online. Sessão de chat: abriga uma conversa e contexto de um cliente de chat. Lidando com erros Todos os erros da API retornam uma resposta HTTP com um status não OK (nunca 2xx ou 3xx), e sempre seguindo o formato JSON: { \"error\": \"Uma mensagem explicando o erro\", \"details\": {} // um objeto contendo informações relevantes sobre o erro. Na maioria das vezes é nulo }"
  },
  "docs/limits.html": {
    "href": "docs/limits.html",
    "title": "Limites da API | AIVAX",
    "keywords": "Limites da API Limites de taxa (\"rate limiters\") regulam o número de requisições que você pode enviar em uma janela de tempo. Esses limites ajudam a AIVAX a prevenir abuso e fornecer uma API estável à todos. Os limites da API abaixo são os mesmos para todos os modelos embutidos da AIVAX. Esses limites são categorizados por operações feitas pela API. Cada conta possui um tier que define quais limites são aplicados à conta. Tiers mudam de acordo com o total investido na AIVAX e o tempo que a conta existe. Tier zero (conta grátis): conta nova que nunca adicionou créditos. Tier 1: conta criada há pelo menos 48 horas e que já adicionou qualquer valor em créditos. Tier 2: conta criada há pelo menos 1 mês e que já adicionou pelo menos $ 100 em créditos. Tier 3: conta criada há pelo menos 3 meses e que já adicionou pelo menos $ 1.000 em créditos. A medição é pela adição de créditos e não pelo seu consumo. Por exemplo, você não consumir $ 100 em créditos para avançar ao Tier 2. Legendas dos limites: RPM: requisições por minuto. RPD: requisições por dia (24 horas). Grátis Tier 1 Tier 2 Tier 3 Operação RPM RPD Pesquisa de documentos 50 - Inserção de documentos - 100 Inferência 5 300 Função 5 300 Função (Live) 2 30 Operação RPM RPD Pesquisa de documentos 150 - Inserção de documentos - 3.000 Inferência 75 10.000 Função 60 10.000 Função (Live) 20 500 Operação RPM RPD Pesquisa de documentos 300 - Inserção de documentos - 10.000 Inferência 150 - Função 60 - Função (Live) 60 - Operação RPM RPD Pesquisa de documentos 1.000 - Inserção de documentos - 30.000 Inferência 1.000 - Função 500 - Função (Live) 200 - Pesquisa de documentos: inclui pesquisa semântica de documentos em uma coleção pelo endpoint de pesquisa ../collections/{id}/query. Inserção de documentos: inclui criação e modificação de documentos em uma coleção. Inferência: toda chamada de inferência, seja pela API Open-AI compatível, pela rota /ai-gateways/{id}/inference ou por cada mensagem enviada por uma sessão de cliente de chat. Função: toda chamada de função /functions. Função (Live): toda chamada de função conectada à internet através de pesquisa na internet (não inclui fetch). Não há nenhum limite para inferência em modelos definidos por você, apenas pelos providos pela AIVAX."
  },
  "docs/models.html": {
    "href": "docs/models.html",
    "title": "Modelos | AIVAX",
    "keywords": "Modelos A AIVAX provê modelos de diferentes provedores para tornar o desenvolvimento ainda mais rápido, dispensando a necessidade de ter que configurar uma conta para cada provedor para ter acessos aos seus modelos mais recentes. Veja a lista abaixo dos modelos disponíveis e suas precificações. Todos os preços consideram o total de entrada e saída de tokens, com ou sem cache. Todos os preços estão em dólares dos Estados Unidos. aivax Nome do modelo Preços Descrição @aivax/fn-1 Entrada: $ 0.10 /1m tokens Saída: $ 0.40 /1m tokens Pesquisa na internet: $ 5.00 /1.000 pesquisas Optimized agent for function usage. Chamadas de função Pensamento profundo Pesquisa na internet Execução de código Funções JSON deepseekai Nome do modelo Preços Descrição @deepseekai/r1-distill-llama-70b Entrada: $ 0.75 /1m tokens Saída: $ 0.99 /1m tokens Model with deep reasoning and thought, best for more demanding tasks. Pensamento profundo google Nome do modelo Preços Descrição @google/gemini-2.5-pro Entrada: $ 1.25 /1m tokens Saída: $ 10.00 /1m tokens One of the most powerful models today. Entrada: aceita imagens, vídeos, áudios Chamadas de função Pensamento profundo @google/gemini-2.5-flash Entrada: $ 0.30 /1m tokens Saída: $ 2.50 /1m tokens Google's best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-2.5-flash-lv Entrada: $ 0.30 /1m tokens Saída: $ 2.50 /1m tokens Pesquisa na internet: $ 35.00 /1.000 pesquisas Gemini 2.5 Flash with Google internet search. Entrada: aceita imagens, vídeos, áudios Pensamento profundo Pesquisa na internet @google/gemini-2.5-flash-lite Entrada: $ 0.10 /1m tokens Saída: $ 0.40 /1m tokens A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Entrada: aceita imagens, vídeos, áudios Chamadas de função Pensamento profundo Funções JSON @google/gemini-2.0-flash Entrada: $ 0.10 /1m tokens Saída: $ 0.40 /1m tokens Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-2.0-flash-lv Entrada: $ 0.10 /1m tokens Saída: $ 0.40 /1m tokens Pesquisa na internet: $ 35.00 /1.000 pesquisas Gemini 2.0 Flash with internet search via Google. Entrada: aceita imagens, vídeos, áudios Pesquisa na internet @google/gemini-2.0-flash-lite Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-1.5-flash-8b Entrada: $ 0.04 /1m tokens Saída: $ 0.08 /1m tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON inception Nome do modelo Preços Descrição @inception/mercury Entrada: $ 0.25 /1m tokens Saída: $ 1.00 /1m tokens Extremely fast model by generative diffusion. Funções JSON metaai Nome do modelo Preços Descrição @metaai/llama-3.3-70b Entrada: $ 0.59 /1m tokens Saída: $ 0.79 /1m tokens Previous generation model with many parameters and surprisingly fast speed. Chamadas de função Funções JSON @metaai/llama-4-maverick-17b-128e Entrada: $ 0.20 /1m tokens Saída: $ 0.60 /1m tokens Fast model, with 17 billion activated parameters and 128 experts. Entrada: aceita imagens Chamadas de função Funções JSON @metaai/llama-4-scout-17b-16e Entrada: $ 0.11 /1m tokens Saída: $ 0.34 /1m tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Entrada: aceita imagens Chamadas de função Funções JSON @metaai/llama-3.1-8b Entrada: $ 0.05 /1m tokens Saída: $ 0.08 /1m tokens Cheap and fast model for less demanding tasks. Chamadas de função Funções JSON model-router Nome do modelo Preços Descrição @model-router/gemini Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash Lite, Gemini 2.0 Flash and Gemini 2.5 Flash. @model-router/gemini-high Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash, Gemini 2.5 Flash (low) and Gemini 2.5 Flash (high). @model-router/openai Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 4.1 Nano, GPT 4.1 Mini and o4-mini. @model-router/openai-high Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 4.1 Mini, o4-mini (low) and o4-mini (high). @model-router/llama Entrada: $ 0.08 /1m tokens Saída: $ 0.30 /1m tokens Model router for Meta Llama. The routing is made between Llama 4 Scout, Llama 4 Maverick and Llama 3.3 70b. openai Nome do modelo Preços Descrição @openai/gpt-4o Entrada: $ 2.50 /1m tokens Saída: $ 10.00 /1m tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Entrada: aceita imagens Chamadas de função @openai/gpt-4.1 Entrada: $ 2.00 /1m tokens Saída: $ 8.00 /1m tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Entrada: aceita imagens Chamadas de função @openai/o4-mini Entrada: $ 1.10 /1m tokens Saída: $ 4.40 /1m tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Entrada: aceita imagens Chamadas de função Pensamento profundo @openai/gpt-4.1-mini Entrada: $ 0.40 /1m tokens Saída: $ 1.60 /1m tokens Fast and cheap for focused tasks. Entrada: aceita imagens Chamadas de função @openai/gpt-4o-mini Entrada: $ 0.15 /1m tokens Saída: $ 0.60 /1m tokens Smaller version of 4o, optimized for everyday tasks. Entrada: aceita imagens Chamadas de função @openai/gpt-4.1-nano Entrada: $ 0.10 /1m tokens Saída: $ 0.40 /1m tokens The fastest and cheapest GPT 4.1 model. Entrada: aceita imagens Chamadas de função qwen Nome do modelo Preços Descrição @qwen/qwen3-32b Entrada: $ 0.29 /1m tokens Saída: $ 0.59 /1m tokens Latest generation of LLMs in the Qwen series, offering advancements in reasoning, instruction-following, agent capabilities, and multilingual support. Pensamento profundo @qwen/qwq-32b Entrada: $ 0.29 /1m tokens Saída: $ 0.39 /1m tokens Conversational model with thinking and reasoning for solving complex tasks. Pensamento profundo"
  },
  "docs/protocol-functions.html": {
    "href": "docs/protocol-functions.html",
    "title": "Funções do lado do servidor | AIVAX",
    "keywords": "Funções do lado do servidor As funções de protocolo da AIVAX, ou server-side functions, é uma implementação customizada de chamadas de função criada pela AIVAX que permite que o modelo siga estritamente um contexto de função que não é baseada em documentos JSON. Similar ao MCP, mas um pouco mais simples. As funções de protocolo permitem a tomada de ações no lado do servidor da AIVAX, removendo a necessidade de implementação da função no lado do cliente e integrando com aplicações e serviços existentes. Essas funções esperam um callback através de uma URL, e quando o modelo decide chamar a função, o callback é acessado com os parâmetros informados pela própria assistente. A assistente não sabe qual URL ela está chamando, pois a mesma permanece invisível tanto para a assistente quanto para o usuário. Escolhendo o nome da função O nome da função deve ser simples e determinístico ao que essa função faz. Evite nomes difíceis de advinhar ou que não remetam ao papel da função, pois a assistente pode se confundir e não chamar a função quando apropriado. Como um exemplo, vamos pensar em uma função de consultar um usuário em um banco de dados externo. Os nomes a seguir são bons exemplos para considerar para a chamada: search-user query-user search_user Nomes ruins incluem: search (pouco abrangente) query-user-in-database-data (nome muito grande) pesquisa-usuario (nome não em inglês) search user (nome com caracteres impróprios) Tendo o nome da função, podemos pensar na descrição da função. Escolhendo a descrição da função. A descrição da função deve explicar conceitualmente duas situações: o que ela faz e quando deve ser chamada pela assistente. Essa descrição deve incluir os cenários que a assistente deve considerar chamar ela e quando não deve ser chamada, fornecendo poucos exemplos de chamadas (one-shot) e/ou tornando explícitas as regras da função. Definindo funções de protocolo Essas funções são definidas no AI-gateway, o que permite a criação de agentes inteligentes que realizam ações sem intervenção humana. Elas seguem uma sintaxe simples, esperam o nome da função, uma descrição do que ela faz e os parâmetros de invocação. Funções de protocolo são definidas no AI gateway seguindo o JSON: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctions\": [ { \"name\": \"list-clients\", \"description\": \"Use essa ferramenta para listar e procurar pelos clientes do usuário.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use essa ferramenta para obter detalhes e pedidos de um cliente através do seu ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"user_id\": \"guid\" } } ] } } No snippet acima, você está fornecendo duas funções para seu modelo de IA: list-clients e view-client, o qual irá decidir qual será executada durante o seu raciocínio. Você pode fornecer também um formato de conteúdo JSON para qual o modelo irá chamar sua API fornecendo o contéudo informado. Você também pode definir as lista de funções suportadas através de um endpoint. Toda vez que o modelo receber uma mensagem, ele irá consultar o endpoint fornecido para obter uma lista de funções que ele possa executar. Defina os endpoints de listagem de funções no seu AI gateway: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctionSources\": [ \"https://my-external-api.com/api/scp/listings\" ] } } Os endpoint de fornecimento de funções deve responder seguindo o formato: GET https://my-external-api.com/api/scp/listings { \"functions\": [ { \"name\": \"list-clients\", \"description\": \"Use essa ferramenta para listar e procurar pelos clientes do usuário.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use essa ferramenta para obter detalhes e pedidos de um cliente através do seu ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"user_id\": \"guid\" } } ] } Essas funções são armazenadas em cache por 10 minutos antes de uma nova requisição ser feita no endpoint fornecido. Lidando com chamada de funções As funções são invocadas no endpoint fornecido em callbackUrl através de uma requisição HTTP POST, com o corpo: { \"function\": { \"name\": \"view-client\", \"content\": { \"user_id\": \"3e5a2823-98fa-49a1-831a-0c4c5d33450e\" } }, \"context\": { \"externalUserId\": \"...\", \"moment\": \"2025-05-18T03:36:27\" } } A resposta dessa ação deve responder sempre com um status HTTP OK (2xx ou 3xx), até mesmo para erros que a assistente possa ter cometido. Uma resposta não OK irá indicar para a assistente que não foi possível chamar a função e ela não irá continuar com o que estava planejando fazer. Formato das respostas As respostas bem sucedidas devem ser textuais e serão anexadas como resposta da função do jeito que for respondida pelo endpoint. Não há formato JSON ou estrutura para essa resposta, mas é aconselhável que dê uma resposta simples, humanamente legível, para que a assistente consiga ler o resultado da ação. Erros podem ser comuns, como não encontrar um cliente pelo ID ou algum campo não estiver no formato desejado. Nestes casos, responda com um status OK e no corpo da resposta inclua uma descrição humana do erro e como a assistente pode contornar ele. É garantido que a requisição irá seguir estritamente o formato de conteúdo fornecido pela definição da função. Funções que não esperam argumentos não devem especificar um formato de conteúdo para essa função. Você também pode indicar para o modelo de como ele deve preencher os campos do conteúdo da função nas instruções da função. Conteúdos mais complexos, aninhados ou com alta profundidade de estrutura pode aumentar o tempo de geração deste conteúdo, pois aumenta a chance da assistente cometer erros e falhar na validação do conteúdo gerado. Important Quanto mais funções você definir, mais tokens você irá consumir no processo de raciocínio. A definição da função, bem como o formato dela, consome tokens do processo de raciocínio. Autenticação A autenticação das requisições são feitas pelo cabeçalho X-Aivax-Nonce enviado em todas as requisições de protocolo das funções, até mesmo as de listagem. Veja o manual de autenticação para entender como autenticar requisições reversas do AIVAX. Autenticação de usuário As chamadas de função enviam um campo $.context.externalUserId contendo a tag de usuário criada em uma sessão de chat. Essa tag pode ser usada para autenticar o usuário que chamou essa função. Considerações de segurança Para o modelo de IA, somente é visível o nome, descrição e formato da função. Ela não é capaz de ver o endpoint para onde essa função aponta. Além disso, ela não possui acesso à tag do usuário que está autenticado em um cliente de chat."
  },
  "index.html": {
    "href": "index.html",
    "title": "Bem-vindo! | AIVAX",
    "keywords": "Bem-vindo! Bem-vindo à documentação da AIVAX."
  },
  "readme.html": {
    "href": "readme.html",
    "title": "Sisk Documentation | AIVAX",
    "keywords": "Sisk Documentation This repository contains the source code of the Sisk Documentation website. Building Firstly, make sure you have docfx installed in your machine. You'll need .NET SDK to install it. Clone this repository. Build the Sisk Framework project and put the .DLL binaries and XML documentation file at the ref/ directory, on the repository root. Run docfx, then docfx serve. Warning Please, do not use the docfx version 2.78.0 or later. This version has a bug that changes the documentation navigation layout. See the tracking issue. Prefer the version 2.76.0: dotnet tool install -g docfx --version 2.76.0 Then you're ready to go and you'll have the static website files at /_site. Contributing Contributions are always welcome. Contribute with spelling corrections, fixing broken links and more. Please, only edit english documentation files. Documentation files for another languages are AI-generated from english files through. Note Please do not edit API specification files (XML). These files are generated. If you want to edit any API documentation, edit it in the repository where the code is hosted."
  }
}