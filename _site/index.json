{
  "docs/authentication.html": {
    "href": "docs/authentication.html",
    "title": "AutenticaÃ§Ã£o | AIVAX",
    "keywords": "AutenticaÃ§Ã£o Quando tiver sua conta em mÃ£os, use sua chave de autenticaÃ§Ã£o Ãºnica para se autenticar na nossa API atravÃ©s do cabeÃ§alho Authorization: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' VocÃª tambÃ©m pode enviar o seu token de autorizaÃ§Ã£o pelo parÃ¢metro da query ?api-key, exemplo: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... NÃ£o hÃ¡ necessidade de enviar o esquema de autenticaÃ§Ã£o Bearer em ambos cabeÃ§alhos, mas Ã© possÃ­vel por questÃµes de compatibilidade. Autenticando hooks RequisiÃ§Ãµes da AIVAX para seus serviÃ§os, seja workers de gateways de IA ou chamadas de funÃ§Ã£o server-side, um cabeÃ§alho X-Request-Nonce Ã© encaminhado em todas as requisiÃ§Ãµes contÃ©ndo um hash BCrypt sendo um salto da chave de hook definida em sua conta. A validaÃ§Ã£o Ã© simples: verifique se o hash em X-Request-Nonce Ã© um produto da chave de salto definida em sua conta. Dessa forma, vocÃª poderÃ¡ autenticar se as requisiÃ§Ãµes da AIVAX para seus serviÃ§os sÃ£o genuÃ­nas atravÃ©s desse token. Se sua conta nÃ£o tiver definido uma chave de hook, esse cabeÃ§alho nÃ£o serÃ¡ enviado. Veja os exemplos abaixo para validaÃ§Ã£o da chave de hook: C# (com Sisk) Python (com Flask) JavaScript (com Express.js) using BCrypt.Net; [RoutePrefix(\"/api/aivax-protocol-functions\")] internal class MyController : Controller { public MyController() { this.HasRequestHandler(RequestHandler.Create( execute: (req, ctx) => { // ObtÃ©m o nonce enviado da requisiÃ§Ã£o var hash = this.Request.Headers [\"X-Request-Nonce\"]; if (hash == null) { return new HttpResponse(HttpStatusInformation.Unauthorized); } // Valida o hook usando a biblioteca BCrypt.Net var secretWord = Environment.GetEnvironmentVariable (\"AIVAX_HOOK_SECRET\"); if (!BCrypt.Net.BCrypt.Verify(secretWord, hash, enhancedEntropy: false)) { return new HttpResponse(HttpStatusInformation.Forbidden); } // Continua a requisiÃ§Ã£o apÃ³s hook validado return null; })); } ... } from flask import Flask, request, abort import os import bcrypt app = Flask(__name__) @app.before_request def autenticar_token(): # 1. LÃª o cabeÃ§alho que contÃ©m o hash do token token_hash = request.headers.get(\"X-Request-Nonce\") if not token_hash: abort(401) # 2. Carrega o segredo em texto puro das variÃ¡veis de ambiente secret = os.getenv(\"AIVAX_HOOK_SECRET\") if secret is None: abort(500) # 3. Verifica se o hash recebido corresponde ao segredo if not bcrypt.checkpw(secret.encode(\"utf-8\"), token_hash.encode(\"utf-8\")): abort(403) @app.route(\"/api/aivax-protocol-functions/some-action\", methods=[\"POST\"]) def some_action(): return \"\", 204 if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", port=5000) require('dotenv').config() const express = require('express') const bcrypt = require('bcrypt') const app = express() app.use(async (req, res, next) => { const tokenHash = req.header('X-Request-Nonce') if (!tokenHash) { return res.sendStatus(401) } // 2. Carrega o segredo em texto puro das variÃ¡veis de ambiente const secret = process.env.AIVAX_HOOK_SECRET if (!secret) { return res.sendStatus(500) } try { // 3. Verifica se o hash recebido corresponde ao segredo const match = await bcrypt.compare(secret, tokenHash) if (!match) { return res.sendStatus(403) } next() } catch (err) { return res.sendStatus(500) } }) app.post( '/api/aivax-protocol-functions/some-action', (req, res) => { res.sendStatus(204) } ) const PORT = process.env.PORT || 3000 app.listen(PORT, () => { console.log(`Server running on port ${PORT}`) })"
  },
  "docs/builtin-tools.html": {
    "href": "docs/builtin-tools.html",
    "title": "Ferramentas embutidas | AIVAX",
    "keywords": "Ferramentas embutidas A AIVAX fornece uma lista de ferramentas embutidas para vocÃª habilitar em seu modelo. Essas ferramentas podem ser usadas em conjunto com as funÃ§Ãµes do lado do servidor. Algumas funÃ§Ãµes possuem custo. Esse custo Ã© aplicado em modelos usados pela AIVAX e os que vocÃª fornece atravÃ©s do BYOK (bring-your-own-key), portanto, Ã© importante adicionar saldo se vocÃª pretende usar essas ferramentas. Note que cada modelo decide qual funÃ§Ã£o chamar e seus parÃ¢metros. Nem todos os modelos podem obedecer as regras de chamadas. Pesquisa na internet Essa funÃ§Ã£o habilita a pesquisa na internet no seu modelo. Com isso, o modelo pode consultar por informaÃ§Ãµes especÃ­ficas ou em tempo real, como dados meteorolÃ³gicos, notÃ­cias, resultados de jogos, etc. A pesquisa na internet Ã© feita por vÃ¡rios provedores, escolhido conforme disponibilidade de rede e latÃªncia. Os provedores atuais usados pela AIVAX Ã© linkup e Exa. A AIVAX fornece dois tipos de pesquisa configurÃ¡veis pelo seu dashboard: Full: a pesquisa realizada Ã© completa, inserindo no contexto da conversa o conteÃºdo inteiro de cada resultado encontrado. Summarized: a pesquisa realizada Ã© resumida, inserindo no contexto da conversa um resumo feito por IA pelo prÃ³prio provedor de pesquisa. O custo dos dois modos Ã© de $5 Ã  cada 1.000 pesquisas realizadas. O modo Full pode consumir mais tokens de entrada da conversa, mas pode proporcionar resultados mais precisos. ExecuÃ§Ã£o de cÃ³digo Essa funÃ§Ã£o permite que o modelo execute cÃ³digo JavaScript e inspecione o resultado da execuÃ§Ã£o. Com isso, o modelo consegue avaliar atravÃ©s de algoritmos resultados de expressÃµes matemÃ¡ticas e outras situaÃ§Ãµes que sÃ£o melhores representadas atravÃ©s de cÃ³digo. O cÃ³digo Ã© executado em um ambiente protegido com pouquÃ­ssimas funÃ§Ãµes disponÃ­veis. O modelo nÃ£o conseguirÃ¡ acessar I/O, acesso Ã  internet ou importar scripts por essa ferramenta. Essa funÃ§Ã£o nÃ£o tem custo. Contexto de URL Essa funÃ§Ã£o permite que o modelo acesse conteÃºdo externo em URLs e links providos pelo usuÃ¡rio. Com essa funÃ§Ã£o, o modelo consegue acessar links e avaliar seu conteÃºdo. Note que, alguns destinos podem identificar o acesso como bot e barrar o acesso, desde que essa funÃ§Ã£o nÃ£o Ã© um crawling e sim um simples GET feito no destino. O modelo consegue acessar atÃ© 5 links de uma vez. Somente os primeiros 10MB dos links sÃ£o lidos. Ao obter o conteÃºdo do link, o sistema verifica o conteÃºdo de retorno e lida com eles de acordo com cada tipo: ConteÃºdos de HTML sÃ£o renderizados: as tags HTML, scripts, CSS e \"ruÃ­dos\" sÃ£o removidos do resultado do acesso, mantendo somente o texto puro do link. Outros conteÃºdos textuais: o conteÃºdo Ã© lido diretamente e nenhuma transformaÃ§Ã£o Ã© realizada. ConteÃºdos nÃ£o textuais: quando o link responde com um conteÃºdo nÃ£o textual e a resposta indica um nome de arquivo (seja pelo caminho ou pelo cabeÃ§alho Content-Disposition), o sistema tenta converter o arquivo baixado para uma versÃ£o textual. Essa funÃ§Ã£o nÃ£o tem custo. MemÃ³ria Essa funÃ§Ã£o permite que o modelo armazene conteÃºdo relevante para ser usado por vÃ¡rias conversas. No momento, essa funÃ§Ã£o sÃ³ estÃ¡ disponÃ­vel quando usada em chat clients e quando a sessÃ£o estÃ¡ identificada por uma tag. AtravÃ©s da tag da sessÃ£o, o modelo armazena um dado relevante da conversa, como preferÃªncia de nomes, lembretes ou aÃ§Ãµes que a assistente deve realizar. A instruÃ§Ã£o dessa memÃ³ria instrui o modelo Ã  nÃ£o salvar dados sensÃ­veis ou pessoais, no entanto, nÃ£o Ã© garantido que o modelo sempre irÃ¡ seguir essa regra. Os dados sÃ£o armazenados por um ano nos bancos de dados da AIVAX e podem ser excluÃ­dos Ã  qualquer momento pela plataforma. Para toda conversa por um chat client, esses dados sÃ£o obtidos e anexados na conversa. Essa funÃ§Ã£o nÃ£o tem custo. GeraÃ§Ã£o de imagens Essa funÃ§Ã£o permite que o modelo crie imagens de IA. As imagens geradas por IA sÃ£o anexadas no contexto da conversa, mas nÃ£o sÃ£o visÃ­veis diretamente para a assistente. Atualmente, existem quatro categorias de imagens geradas: Qualidade baixa: gera imagens rapidamente com um custo baixo, no entanto, pode gerar bastante artefato, como dedos Ã  mais, olhos distorcidos, braÃ§os fora do lugar. Qualidade mÃ©dia: equilÃ­brio entre boa qualidade e velocidade, mas ainda pode gerar artefatos. Qualidade alta: maior qualidade nas imagens e menor chance de existir artefatos na imagem. Qualidade altÃ­ssima: a maior qualidade na geraÃ§Ã£o de imagens. ApÃ³s a geraÃ§Ã£o, um modelo de upscaling realiza o reajuste na imagem criada. Essa funÃ§Ã£o possui custo. O custo varia do tempo de processamento de cada imagem, o que Ã© interferido com o consumo do processamento, tamanho da fila de processamento, modelos usados, etc. A geraÃ§Ã£o de imagens ocorre em um provedor externo, o que o custo pode mudar conforme vÃ¡rios fatores. A imagem de exemplo acima mostra uma previsÃ£o do preÃ§o de cada qualidade de imagem. VocÃª tambÃ©m pode ativar a geraÃ§Ã£o de imagens explÃ­citas e adultas na geraÃ§Ã£o de imagem. Ao ativar esse recurso, o modelo serÃ¡ permitido gerar material adulto. Para isso ocorrer, o modelo tambÃ©m deve \"concordar\" em gerar esse conteÃºdo. Certos modelos possuem um filtro de seguranÃ§a menor que outros. Por exemplo, os modelos Gemini sÃ£o os com o menor filtro de seguranÃ§a, sendo uma opÃ§Ã£o viÃ¡vel para role-play e geraÃ§Ã£o desse tipo de material. VocÃª Ã© sempre responsÃ¡vel pelo material que gera e o material gerado deve ser compatÃ­vel com nossos termos de serviÃ§o. Pesquisa de posts no X Essa funÃ§Ã£o permite o modelo pesquisar por posts no X (antigo Twitter). Ã‰ uma alternativa direta ao web_search, pois pode ser usada para procurar por informaÃ§Ãµes atualizadas em tempo real, como notÃ­cias, informaÃ§Ãµes, resultados de jogos, etc. Essa ferramenta traz resultados muito mais recentes que a ferramenta de pesquisa na internet convencional. NÃ£o Ã© recomendado usar as duas funÃ§Ãµes em conjunto pois elas possuem o mesmo objetivo. No momento, os Ãºltimos 20 posts de um determinado assunto Ã© inserido no contexto da conversa, contendo link e autor. No momento, nÃ£o Ã© possÃ­vel acessar posts de perfis especÃ­ficos. O custo dessa funÃ§Ã£o Ã© de $5 Ã  cada 1.000 pesquisas realizadas."
  },
  "docs/en/authentication.html": {
    "href": "docs/en/authentication.html",
    "title": "Authentication | AIVAX",
    "keywords": "Authentication When you have your account ready, use your unique authentication key to authenticate to our API via the Authorization header: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' You can also send your authorization token via the query parameter ?api-key, for example: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... There is no need to send the Bearer authentication scheme in both headers, but it is possible for compatibility reasons. Authenticating hooks AIVAX requests to your services, whether AI gateway workers or serverâ€‘side function calls, include a X-Request-Nonce header in all requests containing a BCrypt hash that is a derived value of the hook key defined in your account. The validation is simple: check that the hash in X-Request-Nonce is a product of the hook key defined in your account. In this way, you can authenticate whether the AIVAX requests to your services are genuine using this token. If your account has not defined a hook key, this header will not be sent. See the examples below for validating the hook key: C# (with Sisk) Python (with Flask) JavaScript (with Express.js) using BCrypt.Net; [RoutePrefix(\"/api/aivax-protocol-functions\")] internal class MyController : Controller { public MyController() { this.HasRequestHandler(RequestHandler.Create( execute: (req, ctx) => { // Retrieves the nonce sent from the request var hash = this.Request.Headers[\"X-Request-Nonce\"]; if (hash == null) { return new HttpResponse(HttpStatusInformation.Unauthorized); } // Validates the hook using the BCrypt.Net library var secretWord = Environment.GetEnvironmentVariable(\"AIVAX_HOOK_SECRET\"); if (!BCrypt.Net.BCrypt.Verify(secretWord, hash, enhancedEntropy: false)) { return new HttpResponse(HttpStatusInformation.Forbidden); } // Continue the request after hook validated return null; })); } ... } from flask import Flask, request, abort import os import bcrypt app = Flask(__name__) @app.before_request def authenticate_token(): # 1. Read the header containing the token hash token_hash = request.headers.get(\"X-Request-Nonce\") if not token_hash: abort(401) # 2. Load the plainâ€‘text secret from environment variables secret = os.getenv(\"AIVAX_HOOK_SECRET\") if secret is None: abort(500) # 3. Verify that the received hash matches the secret if not bcrypt.checkpw(secret.encode(\"utf-8\"), token_hash.encode(\"utf-8\")): abort(403) @app.route(\"/api/aivax-protocol-functions/some-action\", methods=[\"POST\"]) def some_action(): return \"\", 204 if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", port=5000) require('dotenv').config() const express = require('express') const bcrypt = require('bcrypt') const app = express() app.use(async (req, res, next) => { const tokenHash = req.header('X-Request-Nonce') if (!tokenHash) { return res.sendStatus(401) } // 2. Load the plainâ€‘text secret from environment variables const secret = process.env.AIVAX_HOOK_SECRET if (!secret) { return res.sendStatus(500) } try { // 3. Verify that the received hash matches the secret const match = await bcrypt.compare(secret, tokenHash) if (!match) { return res.sendStatus(403) } next() } catch (err) { return res.sendStatus(500) } }) app.post( '/api/aivax-protocol-functions/some-action', (req, res) => { res.sendStatus(204) } ) const PORT = process.env.PORT || 3000 app.listen(PORT, () => { console.log(`Server running on port ${PORT}`) })"
  },
  "docs/en/builtin-tools.html": {
    "href": "docs/en/builtin-tools.html",
    "title": "Built-in Tools | AIVAX",
    "keywords": "Built-in Tools AIVAX provides a list of built-in tools for you to enable in your model. These tools can be used in conjunction with server-side functions. Some functions have a cost. This cost is applied to models used by AIVAX and those you provide through BYOK (bring-your-own-key), so it's essential to add balance if you intend to use these tools. Note that each model decides which function to call and its parameters. Not all models can follow the calling rules. Internet Search This function enables internet search in your model. With this, the model can query specific information or real-time data, such as weather, news, game results, etc. The internet search is performed by multiple providers, chosen based on network availability and latency. The current providers used by AIVAX are linkup and Exa. AIVAX provides two types of configurable search through your dashboard: Full: the search performed is complete, inserting the entire content of each result found into the conversation context. Summarized: the search performed is summarized, inserting a summary made by AI by the search provider into the conversation context. The cost of both modes is $5 per 1,000 searches performed. The Full mode may consume more conversation input tokens, but it can provide more accurate results. Code Execution This function allows the model to execute JavaScript code and inspect the execution result. With this, the model can evaluate mathematical expression results and other situations that are better represented through code. The code is executed in a protected environment with very few available functions. The model will not be able to access I/O, internet access, or import scripts through this tool. This function has no cost. URL Context This function allows the model to access external content in URLs and links provided by the user. With this function, the model can access links and evaluate their content. Note that some destinations may identify access as a bot and block access, since this function is not a crawl but a simple GET request to the destination. The model can access up to 5 links at a time. Only the first 10MB of the links are read. When obtaining the link content, the system checks the return content and handles it according to each type: HTML content is rendered: HTML tags, scripts, CSS, and \"noise\" are removed from the access result, keeping only the pure text of the link. Other textual content: the content is read directly, and no transformation is performed. Non-textual content: when the link responds with non-textual content and the response indicates a file name (either by path or by the Content-Disposition header), the system attempts to convert the downloaded file to a textual version. This function has no cost. Memory This function allows the model to store relevant content for use in multiple conversations. Currently, this function is only available when used in chat clients and when the session is identified by a tag. Through the session tag, the model stores relevant conversation data, such as name preferences, reminders, or actions the assistant should perform. The instruction of this memory instructs the model not to save sensitive or personal data; however, it is not guaranteed that the model will always follow this rule. The data is stored for one year in AIVAX's databases and can be deleted at any time by the platform. For every conversation by a chat client, this data is obtained and attached to the conversation. This function has no cost. Image Generation This function allows the model to create AI-generated images. The AI-generated images are attached to the conversation context but are not directly visible to the assistant. Currently, there are four categories of generated images: Low quality: generates images quickly with a low cost, but may generate many artifacts, such as extra fingers, distorted eyes, or misplaced arms. Medium quality: balances good quality and speed, but may still generate artifacts. High quality: higher quality in images and lower chance of artifacts in the image. Very high quality: the highest quality in image generation. After generation, an upscaling model performs the adjustment on the created image. This function has a cost. The cost varies depending on the processing time of each image, which is affected by processing consumption, queue size, models used, etc. Image generation occurs on an external provider, so the cost may change according to various factors. The example image above shows a forecast of the price of each image quality. You can also activate the generation of explicit and adult images in image generation. When activating this feature, the model will be allowed to generate adult material. For this to happen, the model must also \"agree\" to generate this content. Certain models have a lower security filter than others. For example, Gemini models have the lowest security filter, making them a viable option for role-playing and generating this type of material. You are always responsible for the generated material, and the generated material must be compatible with our terms of service. Post Search on X This function allows the model to search for posts on X (formerly Twitter). It is a direct alternative to web_search, as it can be used to search for up-to-date information in real-time, such as news, information, game results, etc. This tool brings much more recent results than the conventional internet search tool. It is not recommended to use both functions together, as they have the same objective. Currently, the last 20 posts on a specific topic are inserted into the conversation context, containing a link and author. Currently, it is not possible to access posts from specific profiles. The cost of this function is $5 per 1,000 searches performed."
  },
  "docs/en/entities/ai-gateways/ai-gateway.html": {
    "href": "docs/en/entities/ai-gateways/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway The AI gateways are a service that AIVAX provides to create an inference tunnel between an LLM model and a knowledge base. With it you can: Create a model with custom instructions Use a model provided by you through a compatible OpenAI endpoint, or use a model provided by AIVAX Customize inference parameters such as temperature, top_p, prefill Use a knowledge collection as the foundation for AI responses Among other features. With the AI Gateway, you create a readyâ€‘toâ€‘use model, parameterized and based on the instructions you define. Models You can bring an AI model compatible with the OpenAI interface to the AI gateway. If you bring your own AI model, we will only charge for the document search attached to the AI. You can also use one of the models below that are already ready to start with AIVAX. When using a model, you will notice that some are smarter than others for certain tasks. Some models are better with certain data retrieval strategies than others. Run tests to find the best model. You can see the available models on the models page. Using an AI gateway AIVAX provides an endpoint compatible with the OpenAI interface through an AI gateway, which makes it easier to integrate the model created by AIVAX with existing applications and SDKs. Note that only some properties are supported. In an AI gateway, you already configure model parameters such as System Prompt, temperature, and model name. When using this endpoint, some gateway values can be overridden by the request. Request POST /api/v1/chat-completions { \"model\": \"0197cb0f-893a-7b7d-be0a-71ada1208aaf\", \"messages\": [ { \"role\": \"user\", \"content\": \"Who are you?\" } ], \"stream\": false } Response for nonâ€‘streaming { \"id\": \"0197dbdc-6456-7d54-b7ec-04cb9c80f460\", \"object\": \"chat.completion\", \"created\": 1751740343, \"model\": \"@google/gemini-2.5-flash\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"completion_text\": \"Hi! ðŸ˜Š I am Zia, the smart assistant of ZÃ© do Ingresso. I'm here to help you with everything about tickets, events, and everything happening in our beloved SÃ£o JosÃ© do Rio Preto. If you need anything, like learning about the naming of the thing, just ask! Let's enjoy everything that's happening! What do you need? ðŸ¥³ \", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 1118, \"completion_tokens\": 88, \"total_tokens\": 1206 }, \"service_tier\": \"default\" } Streaming response data: {\"id\":\"0197dbde-c40b-720d-a13e-f689c303c571\",\"object\":\"chat.completion.chunk\",\"created\":1751740498,\"model\":\"@google/gemini-2.5-flash\",\"system_fingerprint\":\"fp_y8jidd\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} ... data: {\"id\":\"0197dbde-c88c-7764-8017-c1fee0d79096\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@google/gemini-2.5-flash\",\"system_fingerprint\":\"fp_2he7ot\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} data: {\"id\":\"0197dbde-c890-7329-9e65-faecbe158efa\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@aivax\\/sentinel-mini\",\"system_fingerprint\":\"fp_q6qh7x\",\"choices\":[{\"index\":0,\"finish_reason\":\"STOP\",\"delta\":{}}],\"usage\":{\"prompt_tokens\":1097,\"completion_tokens\":92,\"total_tokens\":1189},\"sentinel_usage\":null} Usage with SDKs By providing endpoints compatible with the OpenAI interface, AIVAX is fully compatible with existing SDKs, making plugâ€‘andâ€‘play integration easy. See the example below: from openai import OpenAI client = OpenAI( base_url=\"https://inference.aivax.net/api/v1\", api_key=\"oky_gr5u...oqbfd3d9y\" ) response = client.chat.completions.create( model=\"my-gateway:50c3\", # you can also provide your ai-gateway full ID here messages=[ {\"role\": \"user\", \"content\": \"Explain why AI-gateways are useful.\"} ] ) print(response.choices[0].message.content) Currently, AIVAX only supports the chat/completions format. In the future, we plan to add support for the API Responses."
  },
  "docs/en/entities/ai-gateways/pipelines.html": {
    "href": "docs/en/entities/ai-gateways/pipelines.html",
    "title": "AI Pipelines | AIVAX",
    "keywords": "AI Pipelines AIVAX provides several pipelines to use in your AI gateway. You can use multiple pipelines to run in the context of your gateway. RAG Through collections, you can link a collection of documents to your AI gateway. You can set the embedding parameters, such as number of documents, minimum score, and embedding strategy. Each embedding strategy is more refined than the other. Some produce better results than others, but it is important to conduct practical tests with various strategies to understand which fits best with the model, conversation, and user tone. It may be necessary to adjust the system prompt to better inform how the AI should consider the attached documents in the conversation. The documents are attached as a user message, limited by the parameters you define in the retrieval strategy. Rewrite strategies usually generate the best results at low latency and cost. The rewrite model used is always the one with the lowest cost, usually chosen by an internal pool that decides which model has the lowest latency at the moment. Strategies without rewrite cost: Plain: the default strategy. It is the least optimized and has no rewrite cost: the last user message is used as the search term to query the gateway's attached collection. Concatenate: concatenates the last N user messages in lines, and then the result of the concatenation is used as the search term. Strategies with rewrite cost (inference tokens are charged): UserRewrite: rewrites the last N user messages using a smaller model, creating a contextual question about what the user wants to say. FullRewrite: rewrites the last N*2 chat messages using a smaller model. Similar to UserRewrite, but also considers the assistant's messages when formulating the new question. Usually creates the best questions, with a slightly higher cost. It is the most stable and consistent strategy. Works with any model. Function strategies: QueryFunction: provides a search function in the integrated collection for the AI model. You should adjust the system instructions with ideal scenarios for the model to call this function when necessary. May not work as well on smaller models. When defining a RAG collection in your gateway's pipeline, the first message in the conversation context will be the result of the RAG embedding as a user message (except when used as tools where the embedding result is attached as a tool response). Defining many RAG response documents increases input token consumption and may increase the final inference cost. Fixing instructions The instruction pipeline allows prefixing instructions in various places of the model, guiding and restricting the model's response format. The current ways to define instructions are: System instructions: inserts a fixed text into the system prompt of the context. User prompt template: reformats the user's question to follow a specific question format. Assistant initialization (prefill): initializes the assistant's message with initial generation tokens. These parameters can be very useful for prompt engineering, however, they may not be compatible with all models. Attention: prefixing instructions, templates, and initializations can remove the model's reasoning, multiâ€‘modal interpretation, and toolâ€‘calling capabilities. Parameterization The parameterization pipeline configures the initial inference hyperparameters, such as temperature, nucleus sampling, presence penalty, and other inference hyperparameters. Truncating The truncating pipeline allows setting the size of a conversation in tokens before it is trimmed. When this pipeline is enabled, before each inference, it calculates whether num_of_chars / 4 is greater than the maximum input tokens for the conversation. If the context is larger, the pipeline starts removing messages from the beginning of the conversation until the messages fit within the specified context. At least one user message (commonly the last message) is kept in the conversation. All other messages are removed, except system instructions. Alternatively, you can define that when the limit is reached, an error is thrown in the API instead of truncating the context. Tool message truncating The tool message counting pipeline is similar to truncating: it removes older tool responses and preserves only the newest. This can be useful when previous tool responses are no longer useful in newer messages and occupy space in the context, but can be detrimental when using agentic models that call tools in a chain. This pipeline is configured in the number of tool messages to be preserved instead of tokens. When a tool message is considered old, it is not removed, but its content is removed. Serverâ€‘side tools This pipeline allows execution of AIVAX serverâ€‘side tools, similar to the MCP protocol. Read more about this pipeline here. Builtâ€‘in tools You can add tools provided by AIVAX to your gateway, such as internet search, image generation, and link access. See all available tools here. Workers Workers define the behavior of your gateway remotely, used to control when certain events should be aborted or continued. Read more about this pipeline here."
  },
  "docs/en/entities/ai-gateways/workers.html": {
    "href": "docs/en/entities/ai-gateways/workers.html",
    "title": "AI Workers | AIVAX",
    "keywords": "AI Workers The AI gateway workers are resources of the AI Gateways that allow you to control the behavior of your resources remotely through events. With an external controller configured, events are sent to it, and the response from your controller determines whether that action should continue, be aborted, or be configured. When an event is triggered on the AIVAX side, a POST request is sent to the configured worker with information about the triggered event. Based on its response, the action can be cancelled or configured. There is no caching â€“ the request is made for every event that occurs in your AI gateway. The processing time of the response adds latency to each gateway action, however, it adds a layer of control and moderation that you can manage at any time. Creating an AI Worker When an event is triggered, a POST request is sent to your worker following the format below: { \"gatewayId\": \"0197dda5-985f-7d76-96e5-0d0451c539f6\", \"moment\": \"2025-08-09T00:21:40\", \"event\": { \"name\": \"message.received\", \"data\": { \"message\": { \"role\": \"user\", \"content\": \"Bom dia!\" }, \"origin\": [ \"SessionsApi\" ], \"externalUserId\": \"mini-app-session@lot1xc9k03g2my3j4w2y1\" } } } The example above illustrates a message.received event message with its event arguments. After sending the request, AIVAX waits for the response from your worker, and with it: OK response (2xx): continues and proceeds with the normal execution of the event. Other responses: aborts and stops the execution of the event. List of events Currently, the events that can be sent to your worker are: message.received â€“ sent when a message is received by the gateway. This event is triggered with the last message received in the context, which may be from the user or not. { \"name\": \"message.received\", \"data\": { \"message\": {}, // chat/completions message entity \"origin\": [ \"SessionsApi\" // message origin ], \"externalUserId\": \"mini-app-session@lot1xc9k03g2my3j4w2y1\" } } Example The example below illustrates a Cloudflare Worker that authenticates a Telegram conversation based on the username of the conversation: export default { async fetch(request, env, ctx) { // The ID of the gateway we are expecting to handle const CHECKING_GATEWAY_ID = \"0197dda5-985f-7c76-96e5-0d0451c596e5\"; const ALLOWED_USERNAMES = [ \"myusername\" ]; if (request.method == \"POST\") { const requestData = await request.json(); const { event, gatewayId } = requestData; // Checks if it is a message received event, if it is the gateway we are // managing in the worker, and if this message comes from a Telegram chat if (gatewayId === CHECKING_GATEWAY_ID && event.name == \"message.received\" && event.data.externalUserId?.startsWith(\"zp_telegram:\")) { // obtains the Telegram username, which is between the ':' and '@' of the externalUserId const telegramUsername = event.data.externalUserId.split(':')[0].split('@')[0]; // checks if the user is allowed in the integration if (!ALLOWED_USERNAMES.includes(telegramUsername)) { // the user does not exist in the list of allowed usernames, therefore returns a non-ok response // indicating that the message should not be sent return new Response(\"User is not authed\", { status: 400 }); } } } // continues execution return new Response(); } };"
  },
  "docs/en/entities/chat-clients.html": {
    "href": "docs/en/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients A chat client provides a user interface through an AI Gateway that allows the user to converse with their assistant. A chat client is integrated with the AI gateway's inference and supports deep thinking, research, and text conversation. Multi-modal features, such as sending images and audio, are under development. You can customize the interface of your chat client with custom CSS and JavaScript, as well as choose the language of the chat features. Creating a Chat Session A chat session is where you create a conversation between your chat client and the user. You can call this endpoint by providing additional context for the conversation, such as the user's name, location, etc. A chat session expires after a certain period for security reasons related to the generated access token. When you call this endpoint by providing a tag, you can call the same endpoint multiple times and get the active chat session for the provided tag, or create a new chat session if none exists. When a session is found on the chat client through the provided tag, the session is renewed for the specified period and the context is updated. A chat session also restores all conversation messages from the same session after disconnection. The user can clear the conversation by clicking the clear conversation button in the top right corner of the chat client. This session uses the limits defined by the chat client, such as the maximum number of messages and tokens in the conversation. If a session is about to expire, it is renewed for an additional 20 minutes on the user's next message. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Time in seconds for the chat to expire. The minimum is 10 minutes. The maximum is 30 days. \"expires\": 3600, // Optional. Additional context for the AI about the chat. \"extraContext\": \"# Additional context\\r\\n\\r\\nYou are talking to Eduardo.\", // Optional. Provides an endpoint for the session to get additional context. This endpoint is called on every message sent by the user, updated in real-time without any cache. \"contextLocation\": \"https://example.com/context.txt\", // Optional (recommended). An external ID to identify the session later and reuse it whenever calling the same endpoint. It can be the user's ID from your database or a string that facilitates identifying this chat later. \"tag\": \"my-user-tag\" } Response { \"message\": null, \"data\": { // ID of the created chat session \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Public access key for the chat \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // Public URL to converse with the chat \"talkUrl\": \"https://console.aivax.net/chat/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } } Integration Sessions AIVAX provides two integrations for chat clients: Telegram and WhatsApp (through Z-Api). Each conversation in an application is an individual session, identified by the conversation ID or the user's phone number. These sessions follow the rules of the original chat client. Additionally, chat sessions in these integrations have two special commands: /reset: clears the current context of the session. /usage: when debug is active on the chat client, displays the current usage of the chat in tokens."
  },
  "docs/en/entities/collections.html": {
    "href": "docs/en/entities/collections.html",
    "title": "Collections | AIVAX",
    "keywords": "Collections A collection is a knowledge library: it houses several knowledge documents. Use collections to group documents by purpose, such as documenting a product, a company, a service or flow. Collections do not incur costs. There is no limit to the number of collections per account. Create a collection To create an empty collection, simply provide its name: Request POST /api/v1/collections { // The collection name cannot be empty. \"collectionName\": \"My first collection\" } Response { \"message\": null, \"data\": { // Unique ID of the created collection. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } List collections Lists the collections available in your account. Request GET /api/v1/collections Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"My collection\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Another collection\" } ] } } View a collection Obtains details of a collection, such as its indexing progress and information like creation date. Request GET /api/v1/collections/{collection-id}/ Response { \"message\": null, \"data\": { \"name\": \"My collection\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // Returns the number of documents waiting for indexing \"queuedDocuments\": 0, // Number of documents ready for query \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Delete a collection Deletes a collection and all documents within it. This action is irreversible. Request DELETE /api/v1/collections/{collection-id}/ Response { \"message\": \"Collection deleted successfully.\", \"data\": null } Clear a collection Unlike collection deletion, this operation removes all documents from the collection, including indexed and queued ones. Request DELETE /api/v1/collections/{collection-id}/reset-only Response { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/en/entities/documents.html": {
    "href": "docs/en/entities/documents.html",
    "title": "Documents | AIVAX",
    "keywords": "Documents A document represents a piece of knowledge. It is a limited, self-sufficient, and meaningful piece of text on its own. A document is the component that is indexed by the internal model to be retrieved later through a semantic search term. Consider a car manual: it is not a document, but rather several documents. Each of these documents talks, in isolation, about a specific topic related to that car, in such a way that the document does not depend on external context or information to make sense. Each document in this manual will talk about a topic: one will talk about how to turn on the car, another about how to turn it off, another about how its paint is made, and another about how to change the oil periodically. It is not a good idea to reserve a document to talk about several things at the same time, as this will reduce the objectivity and scope of the inference and reduce the quality of acquisition. Examples of document creation: Do not do Do not create very short documents (with 10 or fewer words). Do not create very large documents (with 700 or more words). Do not talk about more than one thing in a document. Do not mix different languages in documents. Do not be implicit in documents. Do not write documents using technical language, such as codes or structures like JSON. Do Be explicit about the purpose of your document. Focus documents on individual topics, summarizing what should be done or explained. Always repeat terms that are keywords for the document search. Example: prefer to use \"The color of the Honda Civic 2015 is yellow\" instead of \"the color of the car is yellow\". Restrict the document content to talk about only one topic or subject. Use simple and easy-to-understand human language. API Usage As all documents are entities that belong to a collection, always have the collection where the document is/will be located at hand. Sending documents in bulk To send a large list of documents to a collection, structure them following the JSONL format. The indexing file structure is: {\"docid\":\"Cars/HondaCivic2015.rmd:1\",\"text\":\"The Honda Civic 2015 is available in [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:2\",\"text\":\"The engine of the Honda Civic 2015 is [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:3\",\"text\":\"The color of the Honda Civic 2015 is Yellow [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} ... The structure consists of the following properties: Property Type Description docid string Specifies the document name. Useful for debugging and identification. text string The \"raw\" content of the document that will be indexed. __ref string Optional. Specifies a reference ID of the document. __tags string[] Optional. Specifies an array of document tags. Useful for document management. A document reference is an ID that can be specified in several documents that need to be linked in a search when one of them is matched in a similarity search. For example, if a search finds a document that has a reference ID, all other documents in the same collection that share the same reference ID as the matched document will also be included in the search response. The use of references can be useful when a document depends on another or more documents to make sense. There is no format requirement for the reference ID: any format is accepted. You can send up to 1,000 lines of documents per request. If you need to send more documents, separate the sending into more requests. If you send a document with more than 1,000 lines, the following lines will be ignored. Note that very long documents, which exceed the allowed number of tokens in the internal embedding model, will have their content truncated and the indexing quality may be severely affected. To avoid this problem, send documents that contain between 20 and 700 words. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of each document. The content of each document is tokenized according to the model used in the indexing of the documents. Request The sending must be done using multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Response { \"message\": null, \"data\": [ { \"name\": \"Institutional/Company.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institutional/Company.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institutional/Company.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Products/Schedules.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Create or modify document This endpoint creates or modifies a document from its name. When a document is modified, its indexing vectors are reset, that is, the document will enter the queue again to be indexed by the indexing engine. This indexing is not exempt from cost. The cost is relative to the number of tokens of the sent content. The cost is only generated when the document is actually changed. Calling this route with the same content as the document does not generate modification, therefore, it does not generate cost. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of the file. The content of the file is tokenized according to the model used in the indexing of the documents. Request PUT /api/v1/collections/{collection-id}/documents { // the name of the document that will be modified \"name\": \"document-name\", // the content of the document that will be created or overwritten if the name already exists \"contents\": \"Content of my document\", // parameters explained earlier \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Response { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // the state of the operation indicates whether the document was modified \"Modified\" or created \"Created\". \"state\": \"Modified\" } } List documents This endpoint lists all available documents in a collection. You can pass an additional query parameter filter to filter documents by name, tag, or content. This filter supports expressions that help filter what you are looking for: -t \"tag\" - filters documents that have this tag. -r \"reference\" - filters documents that have this reference ID. -c \"content\" - filters documents that have this snippet in their content. -n \"name\" - filters documents that have this snippet in their name. Request GET /api/v1/collections/{collection-id}/documents Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Help/Customers.rmd:1\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"A customer is a registration on your platform...\", \"indexState\": \"Indexed\" }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Help/Customers.rmd:2\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"In the customer registration, it is possible to modify...\", \"indexState\": \"Indexed\" }, ... ] } } View document View details about a specific document. Request GET /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institutional/Company.rmd:1\", // represents the indexing situation of the document. // valid values: Queued, Indexed, Cancelled \"state\": \"Indexed\", // content of the indexed document \"contents\": \"...\", // document reference ID \"reference\": \"institutional-company\" } } Delete document Permanently deletes a document through its ID. Request DELETE /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/en/entities/functions.html": {
    "href": "docs/en/entities/functions.html",
    "title": "Functions | AIVAX",
    "keywords": "Functions Functions are a way to force your model to process information using JSON as an intermediate communication medium. With functions, you can make any model respond in the JSON format you want. It can be useful for categorizing comments, applying moderation to reviews, or processing information with the help of AI. Currently, it is only possible to use functions with models provided by AIVAX. Calling a Function To call an AI function, you will need to inform what the AI should respond to and provide a JSON Schema that it should follow. Less intelligent models tend to fail to generate JSON, generating an invalid or problematic document. To fix this, adjust your model, the instruction, and the attempt parameter if necessary. You are charged for each attempt the AI tries to generate. Slightly more intelligent models tend to generate correct results on the first attempt. It is guaranteed that a valid JSON will be generated and that this JSON will follow the same schema provided in the request. Consider using a cache on the side of your application for data that does not need to be constantly updated, such as weather data, daily statistics, etc. AIVAX does not perform any caching on our side. Request POST /api/v1/functions/json { // Required. Specify the name of the integrated model to be used to perform the action. \"modelName\": \"@metaai/llama-3.1-8b\", // Required. Explain what your model should do with the input and how it should bring the response. \"instructions\": \"Classify the user's comment, indicating whether it is positive or negative, and if it has any relevant information (number between 0 (not relevant) and 10 (very relevant))\", // Required. The JSON Schema that the model should follow to generate the response. You can provide examples of generation in the instructions field. \"responseSchema\": { \"type\": \"object\", \"properties\": { \"feedbackType\": { \"type\": \"string\", \"enum\": [\"neutral\", \"positive\", \"negative\"] }, \"informationScore\": { \"type\": \"integer\", \"minimum\": 0, \"maximum\": 10 } }, \"required\": [\"feedbackType\", \"informationScore\"] }, // Optional. Defines a JSON input for the model. Can be any type of JSON value. \"inputData\": { \"userComment\": \"Terrible market. There's a guard inside watching you so you don't steal and the butchers ignore you and serve pretty girls in front of you. But thank God there are other markets coming and the end of this nonsense will come\" }, // Optional. Defines how many attempts the model should try before the API returns an error. Should be a number between 1 and 30. \"maxAttempts\": 10, // Optional. Defines the time limit in seconds to obtain a valid JSON before the API returns an error. Should be a number between 1 and 3600 (one hour). \"timeout\": 300, // Optional. Defines the temperature of JSON generation. Higher values tend to be more creative, while lower values are more deterministic. Number from 0 to 2. \"temperature\": 0.4, // Optional. Provides additional context for generation through chat/completions messages. You can also provide multi-modal content for compatible models. \"context\": [ { \"role\": \"user\", \"content\": \"Additional context\" } ], // Optional. Provides built-in AIVAX tools for JSON generation. \"tools\": [ \"WebSearch\", \"Code\", \"OpenUrl\", \"ImageGeneration\", \"XPostsSearch\" ], // Optional. Defines tool generation parameters. \"toolsOptions\": { \"webSearchMode\": \"Full\" | \"Summarized\", \"webSearchMaxResults\": 10, \"imageGenerationMaxResults\": 2, \"imageGenerationQuality\": \"Low\" | \"Medium\" | \"High\" | \"Highest\", \"imageGenerationAllowMatureContent\": false } } Response { \"result\": { \"requiresAttention\": true, \"shortSummary\": \"Customer threatens cancellation and bad publicity if not contacted today.\" }, \"attempt\": 0, \"elapsedMilliseconds\": 1235, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0000123, \"unitPrice\": 1e-7, \"quantity\": 123, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0000116, \"unitPrice\": 4e-7, \"quantity\": 29, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" } ], \"runnedFunctions\": [] } } JSON Schema Guidelines The response format must be provided by a JSON Schema. Behind the scenes, AIVAX guides the model to generate a response with the provided JSON schema. When the model generates something invalid, we indicate to it to try again and correct the errors until the output conforms to the provided specification. The supported JSON Schema guidelines from AIVAX are: string: minLength maxLength pattern format Can be date-time, email, time, duration, uri, url, ipv4, ipv6, uuid or guid. enum number and integer: minimum maximum exclusiveMinimum exclusiveMaximum multipleOf array items uniqueItems minItems maxItems object properties required bool and boolean null Additionally, it is possible to inform one or more values in the type of the object, example: { \"type\": [\"string\", \"number\"] } Note: number and integer are synonyms and integer does not guarantee that the number will be an integer. Functions in Tools It is possible to use built-in tools with JSON functions. This will allow the model to call functions to obtain the necessary context to generate the final JSON. Examples Check out examples of AI functions for various everyday tasks: Classify good or bad comments POST /api/v1/functions/json { \"modelName\": \"@google/gemini-2.0-flash\", \"instructions\": \"Classify the user's comment, providing a note for their comment.\", \"inputData\": { \"inputText\": \"The food is good, but the environment is very noisy and a bit dirty too.\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"commentSummary\": { \"type\": \"string\", \"description\": \"Summary of what the user meant to say.\" }, \"score\": { \"type\": \"integer\", \"min\": 1, \"max\": 5, \"description\": \"The note extracted from the evaluation, being 1 very bad and 5 very good.\" } }, \"required\": [ \"commentSummary\", \"score\" ] } } { \"result\": { \"commentSummary\": \"The food is good, but the environment is noisy and a bit dirty.\", \"score\": 3 }, \"attempt\": 0, \"elapsedMilliseconds\": 788, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0000083, \"unitPrice\": 1e-7, \"quantity\": 83, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0000128, \"unitPrice\": 4e-7, \"quantity\": 32, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" } ], \"runnedFunctions\": [] } } Evaluate a math expression POST /api/v1/functions/json { \"modelName\": \"@qwen/qwen3-32b\", \"instructions\": \"Evaluate the given math problem and provide the result step-by-step.\", \"inputData\": { \"inputText\": \"what is two plus two minus pi ?\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"steps\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"stepDescription\": { \"type\": \"string\", \"description\": \"Current math step description.\" }, \"carry\": { \"type\": \"number\", \"description\": \"The current result.\" } }, \"required\": [ \"stepDescription\", \"carry\" ] }, \"minItems\": 1 }, \"finalResult\": { \"type\": \"number\", \"description\": \"The math operation final result.\" } }, \"required\": [ \"steps\", \"finalResult\" ] }, \"tools\": [ \"Code\" ] } { \"result\": { \"steps\": [ { \"stepDescription\": \"Add 2 and 2\", \"carry\": 4 }, { \"stepDescription\": \"Subtract pi (Ï€) from the result\", \"carry\": 0.858407346410207 } ], \"finalResult\": 0.858407346410207 }, \"attempt\": 0, \"elapsedMilliseconds\": 5775, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.00031001, \"unitPrice\": 2.9e-7, \"quantity\": 1069, \"description\": \"JSON function rendering (@qwen/qwen3-32b)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.00054162, \"unitPrice\": 5.9e-7, \"quantity\": 918, \"description\": \"JSON function rendering (@qwen/qwen3-32b)\" } ], \"runnedFunctions\": [ { \"functionName\": \"evaluate_code\", \"success\": true, \"context\": { \"arguments\": \"console.log(2 + 2 - Math.PI);\", \"result\": { \"evaluatedCode\": \"console.log(2 + 2 - Math.PI);\", \"result\": \"0.8584073464102069\\nScript evaluation result: undefined\\n\" } } } ] } } Bring the latest news and weather from a specific city POST /api/v1/functions/json { \"modelName\": \"@openai/gpt-4o-mini\", \"instructions\": \"Search for the 5 latest news and weather data for the given city.\", \"inputData\": { \"city\": \"Tokyo\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"latestNews\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"title\": { \"type\": \"string\" }, \"details\": { \"type\": \"string\" }, \"link\": { \"type\": \"string\", \"format\": \"uri\" } }, \"required\": [ \"title\", \"details\", \"link\" ], \"additionalProperties\": false } }, \"weather\": { \"type\": \"object\", \"properties\": { \"currentTemperature\": { \"type\": \"number\" }, \"currentWeather\": { \"type\": \"string\", \"enum\": [ \"sunny\", \"cloudy\", \"rain\", \"thunderstorm\" ] }, \"forecast\": { \"type\": \"string\", \"enum\": [ \"sunny\", \"cloudy\", \"rain\", \"thunderstorm\" ] } }, \"required\": [ \"currentTemperature\", \"currentWeather\", \"forecast\" ], \"additionalProperties\": false }, \"assistantSummary\": { \"type\": \"string\" } }, \"required\": [ \"latestNews\", \"weather\", \"assistantSummary\" ], \"additionalProperties\": false }, \"tools\": [ \"XPostsSearch\", \"WebSearch\" ] } { \"result\": { \"latestNews\": [ { \"title\": \"Indian Ambassador to Japan Expands Cooperation\", \"details\": \"Indian Ambassador to Japan Sibi George expressed eagerness to expand cooperation between India and Japan in business, technology, and security.\", \"link\": \"https://japannews.yomiuri.co.jp/\" }, { \"title\": \"Emperor Emeritus Akihito Discharged from Hospital\", \"details\": \"Japanâ€™s Emperor Emeritus Akihito was discharged from the University hospital.\", \"link\": \"https://www.japantimes.co.jp/\" }, { \"title\": \"Tokyo Stocks Climb Following Wall Street Gains\", \"details\": \"Tokyo stocks climbed in the morning following Wall Street gains.\", \"link\": \"https://www.independent.co.uk/topic/tokyo\" }, { \"title\": \"Tightening of Business Manager Visa Requirements\", \"details\": \"Tokyo is tightening requirements for popular business manager visas.\", \"link\": \"https://www.japantimes.co.jp/latest-news/\" }, { \"title\": \"ANA Plans Affordable Flying Taxi Service\", \"details\": \"ANA plans to launch an affordable flying taxi service in Japan by 2027.\", \"link\": \"https://www.japantimes.co.jp/\" } ], \"weather\": { \"currentTemperature\": 31, \"currentWeather\": \"cloudy\", \"forecast\": \"rain\" }, \"assistantSummary\": \"The latest news from Tokyo includes diplomatic and economic updates, while the current weather is partly cloudy with a temperature of 31Â°C.\" }, \"attempt\": 0, \"elapsedMilliseconds\": 19772, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"serviceusage.web_search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"Web Search request\" }, { \"sku\": \"serviceusage.web_search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"Web Search request\" }, { \"sku\": \"inference.functions.json.in\", \"amount\": 0.000453, \"unitPrice\": 1.5e-7, \"quantity\": 3020, \"description\": \"JSON function rendering (@openai/gpt-4o-mini)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0002406, \"unitPrice\": 6e-7, \"quantity\": 401, \"description\": \"JSON function rendering (@openai/gpt-4o-mini)\" } ], \"runnedFunctions\": [ { \"functionName\": \"web_search\", \"success\": true, \"context\": { \"arguments\": { \"search_term\": \"Tokyo weather\" }, \"result\": { \"results\": [ { \"title\": \"Search results\", \"url\": \"\" } ] } } }, { \"functionName\": \"web_search\", \"success\": true, \"context\": { \"arguments\": { \"search_term\": \"latest news in Tokyo\" }, \"result\": { \"results\": [ { \"title\": \"Search results\", \"url\": \"\" } ] } } } ] } } Bring top artists by music genre POST /api/v1/functions/json { \"modelName\": \"@openai/gpt-4.1-mini\", \"instructions\": \"The function should search, using the latest posts from X, the music streaming platforms (such as Spotify, Apple Music, etc.) and identify the 10 most played artists in the genre informed by the user. Then, it should format an object containing a list of 10 artists, including position (1-10), name, and estimated number of streams.\", \"inputData\": { \"genre\": \"dubstep\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"artists\": { \"type\": \"array\", \"description\": \"List of the 10 most played artists in the specified genre\", \"items\": { \"type\": \"object\", \"properties\": { \"rank\": { \"type\": \"integer\", \"description\": \"Position in the Top 10\", \"minimum\": 1, \"maximum\": 10 }, \"name\": { \"type\": \"string\", \"description\": \"Artist name\" }, \"monthlyStreams\": { \"type\": \"string\", \"description\": \"Approximate number of monthly streams, formatted, e.g., \\\"150M\\\"\" }, \"source\": { \"type\": \"string\", \"description\": \"Platform or data source\" } }, \"required\": [ \"rank\", \"name\", \"monthlyStreams\", \"source\" ], \"additionalProperties\": false } } }, \"required\": [ \"artists\" ], \"additionalProperties\": false }, \"maxAttempts\": 4, \"temperature\": 1, \"tools\": [ \"XPostsSearch\", \"WebSearch\" ], \"toolsOptions\": {} } { \"result\": { \"artists\": [ { \"rank\": 1, \"name\": \"Skrillex\", \"monthlyStreams\": \"31M\", \"source\": \"Spotify\" }, { \"rank\": 2, \"name\": \"Virtual Riot\", \"monthlyStreams\": \"12M\", \"source\": \"Spotify\" }, { \"rank\": 3, \"name\": \"Excision\", \"monthlyStreams\": \"11M\", \"source\": \"Spotify\" }, { \"rank\": 4, \"name\": \"Zeds Dead\", \"monthlyStreams\": \"10M\", \"source\": \"Spotify\" }, { \"rank\": 5, \"name\": \"Flux Pavilion\", \"monthlyStreams\": \"9M\", \"source\": \"Spotify\" }, { \"rank\": 6, \"name\": \"Illenium\", \"monthlyStreams\": \"8.5M\", \"source\": \"Spotify\" }, { \"rank\": 7, \"name\": \"Rusko\", \"monthlyStreams\": \"7M\", \"source\": \"Spotify\" }, { \"rank\": 8, \"name\": \"Bassnectar\", \"monthlyStreams\": \"6M\", \"source\": \"Spotify\" }, { \"rank\": 9, \"name\": \"Seven Lions\", \"monthlyStreams\": \"5.5M\", \"source\": \"Spotify\" }, { \"rank\": 10, \"name\": \"Getter\", \"monthlyStreams\": \"5M\", \"source\": \"Spotify\" } ] }, \"attempt\": 0, \"elapsedMilliseconds\": 11243, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"serviceusage.x_api.search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"X Posts search\" }, { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0016448, \"unitPrice\": 4e-7, \"quantity\": 4112, \"description\": \"JSON function rendering (@openai/gpt-4.1-mini)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0006272, \"unitPrice\": 0.0000016, \"quantity\": 392, \"description\": \"JSON function rendering (@openai/gpt-4.1-mini)\" } ], \"runnedFunctions\": [ { \"functionName\": \"x_posts_search\", \"success\": true, \"context\": { \"arguments\": { \"search_query\": \"dubstep artists Spotify OR Apple Music OR streaming\" }, \"result\": [ { \"url\": \"https://x.com/nathanielblow/status/1867148757466304607\", \"text\": \"NOW LIVE ON APPLE MUSIC. SPOTIFY. YOUTUBE MUSIC.\\n\\nEnjoy. https://t.co/apUxnXc7IE\", \"authorUserName\": \"nathanielblow\", \"createdAt\": \"2024-12-12T10:05:28\" }, { \"url\": \"https://x.com/yobrxxzy/status/1754800896041505222\", \"text\": \"MOST STREAMED ARTISTS ON THESE STREAMING PLATFORMS \\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nYouTube â€” BURNA BOY\\nPandora â€” WIZKID\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID https://t.co/Nm2jO5R5P6\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2024-02-06T09:35:10\" }, { \"url\": \"https://x.com/yobrxxzy/status/1922763266549301642\", \"text\": \"MOST STREAMED ARTISTS ON THESE DSP:\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nPandora â€” WIZKID\\nYouTube â€” BURNA BOY\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nDeezer â€” WIZKID\\nAnghami â€” REMA\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID\\n\\n https://t.co/3aUCGbYiEO\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2025-05-14T21:17:40\" }, { \"url\": \"https://x.com/yobrxxzy/status/1856816512524587343\", \"text\": \"MOST STREAMED ARTISTS ON THESE DSP:\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nYouTube â€” BURNA BOY\\nPandora â€” WIZKID\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nDeezer â€” WIZKID\\nAnghami â€” REMA\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID\\n\\n https://t.co/yL2tmJJpHM\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2024-11-13T21:48:49\" }, { \"url\": \"https://x.com/hourjinnie/status/1858756269902881208\", \"text\": \"PLAYLISTS! Letâ€™s get to streaming and utilize all our tools! More pl coming tomorrow!!!\\n\\nSPOTIFY\\n\\nhttps://t.co/L0HRMpYEQG\\n\\nhttps://t.co/HmhdB859e5\\n\\nhttps://t.co/FA4eDmvvkS\\n\\nhttps://t.co/ZOedbxiKmO\\n\\nhttps://t.co/DyPs0qpOMQ\\n\\nDeezer\\n\\nhttps://t.co/36PY8aXkV7\\n\\nApple Music \\n\\nhttps://t.co/wk5MXtzDYC\\n\\nhttps://t.co/OgovuVtoDq\\n\\nPandora \\n\\nhttps://t.co/idkimHTTjp\\n\\nhttps://t.co/WQ83YKeUOK\", \"authorUserName\": \"hourjinnie\", \"createdAt\": \"2024-11-19T06:16:43\" }, { \"url\": \"https://x.com/runthismusic/status/1853150149788209234\", \"text\": \"What if 'Sticky' by @tylerthecreator was Dubstep? \\nOUT NOW ON SC + FREE DL \\n@skybreakedm https://t.co/OjaU8um8GC\", \"authorUserName\": \"runthismusic\", \"createdAt\": \"2024-11-03T19:00:00\" }, { \"url\": \"https://x.com/BTStreamingID/status/1953425910612320348\", \"text\": \"Party start \\n\\nPlaylist \\n\\nSpotify : \\n1.Premium: https://t.co/qJwpckiSlS\\n2. Free: https://t.co/NqIV40psvp\\nApple music: https://t.co/dblVEdcd1q\\nYouTube: https://t.co/gmCR3oyfze \\nYouTube Music: https://t.co/xhxF2rt90o\\n\\n#BTS #ë°©íƒ„ì†Œë…„ë‹¨ #PTD_ON_STAGE_LIVE\\n#BangtanPlaylistID\", \"authorUserName\": \"BTStreamingID\", \"createdAt\": \"2025-08-07T12:00:04\" }, { \"url\": \"https://x.com/NewEDMToday/status/1728069779066392828\", \"text\": \"@FlatlandFunk_ @FrankieSinn2k @gabybaumusic @Gladez @grislymusik @HELOSPHEREmusic @HUMANSION_music @kausedubs @Kuhlosul_ @LazrusOfficial @SHEISLUTHIEN @LVCiDdubz @MagMag_dubstep @MVSLOTUNES @raddixofficial @RazrDub @ScarexxDub @SmilesOnlyMusic @soulvalient @SpeedShift6 @stvnkfvcemusic @itstoxicmusic @TremorrOfficial @TyphonOfficial @Verosdubz @voyagerdubz @wilco_beats @zovahofficial all been released on: @Emengy \\n\\ngenre of music: #edm #dubstep\\n\\nwith 30 tracks\\n\\n54 artists\\n\\nhttps://t.co/H9KmeiXJB4\", \"authorUserName\": \"NewEDMToday\", \"createdAt\": \"2023-11-24T15:15:15\" }, { \"url\": \"https://x.com/offsetemusic/status/1729861632308744501\", \"text\": \"i mostly use my local files to listen to music, but i'll share my spotify wrapped anyway\\n\\ni'm happy to see @canotodubz here as no. 1, he's one of the most unique dubstep artists i've discovered this year https://t.co/OUBhgtikn9\", \"authorUserName\": \"offsetemusic\", \"createdAt\": \"2023-11-29T13:55:27\" }, { \"url\": \"https://x.com/paulpoint_/status/1882728999962468414\", \"text\": \"Good Morning \\n\\nEspecially, Electronic Music Artists\\n\\nUse any of these?\\nâ€”Laptop \\nâ€”â€”MIDI Keyboard\\nâ€”â€”â€”Software\\n\\nHow about these?\\nâ€”Spotify\\nâ€”â€”Soundcloud\\nâ€”â€”â€”Apple Music\\n\\nMaking sounds like these?\\nâ€”House\\nâ€”â€”Techno\\nâ€”â€”â€”Trance\\nâ€”â€”â€”â€”DnB? ...Dubstep??\\n\\nIf you answered yes to anything above and you are staying consistent\\n\\nIf you are incessantly networking and improving your use of the above, particularly for live\\n\\nThen you may well be taking home your slice of a $25 Bn dollar industry\\n\\nExcited? Well are you? Keep making sounds on your computer \\n\\nCan't wait to hear your next track!\\n\\nP.\", \"authorUserName\": \"paulpoint_\", \"createdAt\": \"2025-01-24T09:55:47\" }, { \"url\": \"https://x.com/Gunfingers_eu/status/1701277617280663828\", \"text\": \"Join Kaps on his bass music journey! From listener to passionate player, he's mastered the dubstep genre, sharing stages with many confirmed artists.\\nExpect a wealth of experience and skills for your enjoyment!\\n\\nSC : https://t.co/s5sqmIzrK6\\n\\nSpotify : https://t.co/o8bmaRXTNU https://t.co/6DVahvcr5D\", \"authorUserName\": \"Gunfingers_eu\", \"createdAt\": \"2023-09-11T16:52:46\" }, { \"url\": \"https://x.com/ballsackious/status/1714854867645149399\", \"text\": \"i like that more dubstep artists are doing albums this year. like i know itâ€™s not the best release move in 2024 but that kinda lets you know that theyâ€™re doing it out of love for the music rather than like the Best Spotify Strategy \", \"authorUserName\": \"ballsackious\", \"createdAt\": \"2023-10-19T04:03:55\" }, { \"url\": \"https://x.com/mewaolix/status/1886435921895235989\", \"text\": \"Stays, we can easily reach #3 if we push a little \\n\\n(DSP counts Spotify, Apple Music, Amazon Music, Deezer, YT Music, Anghami, Gaana, Joox, Melon, JioSaavn, Boomplay, etc) https://t.co/P2hFoSmbtT\", \"authorUserName\": \"mewaolix\", \"createdAt\": \"2025-02-03T15:25:46\" }, { \"url\": \"https://x.com/bhadext/status/1951588819561546145\", \"text\": \"Listen to my songs on Apple music, \\nBhadext \\n\\nHere: https://t.co/3u79BQvHAZ https://t.co/lmMjN7m2oR\", \"authorUserName\": \"bhadext\", \"createdAt\": \"2025-08-02T10:20:08\" }, { \"url\": \"https://x.com/runthismusic/status/1853150149788209234\", \"text\": \"What if 'Sticky' by @tylerthecreator was Dubstep? \\nOUT NOW ON SC + FREE DL \\n@skybreakedm https://t.co/OjaU8um8GC\", \"authorUserName\": \"runthismusic\", \"createdAt\": \"2024-11-03T19:00:00\" }, { \"url\": \"https://x.com/OneRougeWave/status/1953444705204588608\", \"text\": \"Oh yeah.. almost forgot.. \\nStream my music â€¦\\n\\nhttps://t.co/mUNJL5AAGf\\n\\nhttps://t.co/aFqbVnuzZV\\n\\n#wavyboss https://t.co/c8mnh9mw3d\", \"authorUserName\": \"OneRougeWave\", \"createdAt\": \"2025-08-07T13:14:45\" } ] } } ] } }"
  },
  "docs/en/entities/search.html": {
    "href": "docs/en/entities/search.html",
    "title": "Search | AIVAX",
    "keywords": "Search The search API, through the query key obtained from the collections, performs a semantic search on it, performing an intelligent comparison for each indexed document in a collection. After creating a collection, you will get its ID. Use the ID of your collection to perform the search on the indexed documents of the same. Use the endpoints of this API to embed the semantic search of documents in your AI model or chatbot. Searching documents This endpoint expects a GET request with the following parameters: term: required. Specifies the search term that will be searched in the documents. top: Specifies the maximum number of documents that should be returned in the search. min: Specifies the minimum score for obtaining the documents. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the search term. The search term is tokenized according to the model used in the indexing of the documents. Request GET /api/v1/collections/{collection-id}/query term=What is the color of the Honda CIVIC? Response { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Products/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Products/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Products/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } For the search result, the higher the score, the more similar the document is to the search term. AIVAX uses embedding models that allow task orientation. For the search, the term is vectorized with a DOCUMENT_QUERY orientation. For document indexing, the orientation is DOCUMENT_RETRIEVAL, which provides a more optimized search and not to verify the similarity between documents."
  },
  "docs/en/getting-started.html": {
    "href": "docs/en/getting-started.html",
    "title": "Welcome | AIVAX",
    "keywords": "Welcome Welcome to AIVAX. Our service makes it easier to develop intelligent AI models that use a knowledge base provided by you to converse with the user, answer questions, provide real-time information, and more. To get started, all endpoints must be made to the AIVAX production URL: https://inference.aivax.net/ Concepts and definitions Understand the concepts used by the API below: Account: represents a user account, which has an authentication token. Collection: represents a collection of knowledge documents. A user can have multiple document collections. Document: represents a fact, a single piece of knowledge, and an item in a collection. A collection can have multiple documents. AI Gateway: represents an AI gateway that benefits from or does not use a knowledge collection, such as a plug-and-play knowledge middleware for a model. Embedded model: represents an AI model that AIVAX provides to the user. Chat client: represents a user interface that makes the AI gateway available through an interactive online chat. Chat session: hosts a conversation and context of a chat client. Handling errors All API errors return an HTTP response with a non-OK status (never 2xx or 3xx), and always follow the JSON format: { \"error\": \"An explanatory error message\", \"details\": {} // an object containing relevant error information. Most of the time it is null }"
  },
  "docs/en/legal/privacy-policy.html": {
    "href": "docs/en/legal/privacy-policy.html",
    "title": "Privacy Policy | AIVAX",
    "keywords": "Privacy Policy Last Update: July 30, 2025 Welcome to AIVAX. This Privacy Policy describes how AIVAX collects, uses, stores, shares, and protects user information from our AI inference services. Our commitment is to transparency and the protection of your data, in full compliance with the General Data Protection Law (LGPD) and the Brazilian Internet Civil Framework. By using AIVAX services, the AIVAX account manager agrees to the practices described in this policy. 1. Definitions AIVAX: The company that provides AI inference services and is the author of this policy. AIVAX Account Manager: The physical or legal person who creates and administers an account on the AIVAX platform, being the primary target of this policy. End User: Individuals who interact with applications or services created by the AIVAX Account Manager that use the AIVAX API. Inference Data: The data (prompts, inputs, etc.) sent to AI models through the AIVAX API and the responses (inferences) generated by them. Conversations: The record of Inference Data interactions, including inputs and outputs. 2. Data Collection and Use 2.1. Inference Data and Conversations AIVAX processes the Inference Data that the AIVAX Account Manager sends through our API. This data is used exclusively to provide the requested inference service. Commitment to Non-Use for Training: AIVAX never uses Inference Data or Conversations from its customers to train its own AI models or for any purpose other than the strict provision of the contracted service. Your business data and end-user interactions remain yours. 2.2. Conversation Storage To ensure the reliability and quality of our services, AIVAX temporarily stores Conversations in its databases. This storage has the following purposes: Monitoring and History: Allow the AIVAX Account Manager to view usage history, monitor costs, and analyze application traffic. Debugging: In case of failures, errors, or unexpected behavior of the models, access to these records is essential for our technical team to diagnose and resolve problems efficiently. Conversations are treated with maximum security and confidentiality. Access to this data is restricted to authorized personnel and limited to the mentioned purposes. 2.3. Data Exclusion The AIVAX Account Manager has total control over the stored data. Conversations are permanently removed from our production systems after a pre-established period, detailed in our technical documentation. Additionally, the AIVAX Account Manager can, at any time, request the deletion of all stored conversations through the control panel of their AIVAX account. Once requested, the deletion is definitive and irreversible. 3. Third-Party Model Providers AIVAX acts as an orchestrator, routing inference requests to various AI models provided by partner companies (\"AI Providers\"). It is crucial to understand that: Third-Party Policies: Each AI Provider has its own privacy policies and terms of use. By choosing to use a specific model through AIVAX, the AIVAX Account Manager is also subject to the terms of that provider. Possible Use for Training by Third Parties: Some of these AI Providers may use the inference data sent to their models for training and improvement of their own products and services. AIVAX strives to clearly inform the origin of each model, but strongly recommends that the AIVAX Account Manager review the privacy policies of the AI Provider corresponding to the model they intend to use, especially if sensitive data is processed. AIVAX is not responsible for the privacy practices of third parties. 4. Responsibilities 4.1. AIVAX Account Manager Responsibility The AIVAX Account Manager is the controller of the data processed through their application and is integrally responsible for: Model Use: The choice of AI models and the suitability of their use for the intended purpose. Conversation Content: All content, including texts, images, or other data that transits through the AIVAX API, is the responsibility of the AIVAX Account Manager. Personal Data: The AIVAX Account Manager is responsible for ensuring that the collection and processing of end-users' personal data, carried out through the AIVAX API, comply with the LGPD and other applicable laws. This includes obtaining consent, when necessary, and managing the rights of the data subjects. 4.2. AIVAX Disclaimer of Responsibility AIVAX acts as an operator in the processing of data, following the instructions of the AIVAX Account Manager (materialized in API calls). Therefore, AIVAX is not responsible for the nature of the processed content or the purpose of the interaction between the end-user, the application, and the AI model. The relationship and legal obligations arising from the use of the final application are exclusively between the AIVAX Account Manager and their end-users. 5. Legal Compliance 5.1. Brazilian Legislation AIVAX is a company based in Brazil. Our operations and this Privacy Policy are governed and interpreted in accordance with the laws of the Federative Republic of Brazil, especially: General Data Protection Law (LGPD - Law No. 13,709/2018): We are committed to following all its principles and obligations, ensuring the rights of data subjects, information security, and transparency in treatment. Internet Civil Framework (Law No. 12,965/2014): We respect the principles of network neutrality, privacy, and freedom of expression, as established by law. 6. Information Security AIVAX employs technical and organizational security measures to protect data against unauthorized access, alteration, disclosure, or destruction. This includes data encryption in transit and at rest, strict access control, and continuous monitoring of our systems. 7. Changes to this Policy AIVAX may periodically update this Privacy Policy to reflect changes in our practices or legislation. We will notify Account Managers of significant changes via email or through a prominent notice on our platform. 8. Contact If you have questions about this Privacy Policy or AIVAX practices, please contact us through the email: **legal@aivax.net**."
  },
  "docs/en/legal/terms-of-service.html": {
    "href": "docs/en/legal/terms-of-service.html",
    "title": "Terms of Service | AIVAX",
    "keywords": "Terms of Service Last Update: July 30, 2025 Welcome to AIVAX. These Terms of Service (\"Terms\") govern your access to and use of our AI inference services, APIs, website, and any associated software (collectively, the \"Services\"). By creating an account, accessing, or using our Services, you (\"AIVAX Account Manager\") agree to be bound by these Terms and our Privacy Policy. If you do not agree to these Terms, do not use our Services. 1. Definitions AIVAX: The company providing the services. AIVAX Account Manager: The individual or entity that creates and administers an account on the AIVAX platform and agrees to these Terms. Input Content: The data, texts, prompts, or any other information that the AIVAX Account Manager sends to the Services for processing. Generated Content: The responses, texts, images, or any other data generated by the AI models as a result of processing the Input Content. 2. Use of Services and Responsibilities 2.1. Responsible Use and Conduct You agree to use AIVAX's Services in an ethical and responsible manner. It is strictly prohibited to: Abuse the System: Do not engage in activities that abuse, interfere with, interrupt, or harm the Services, our servers, or networks. This includes, but is not limited to, sending an excessive volume of requests that may be characterized as a denial-of-service (DoS) attack, attempting to find and exploit vulnerabilities, or attempting to access unauthorized areas of the system. Inappropriate Behavior: Do not use the Services to harass, threaten, defame, deceive, or violate the legal rights and dignity of third parties. AIVAX values a healthy technological environment and will not tolerate conduct that can be reasonably interpreted as malicious or abusive. In summary: do not be a bad actor. 2.2. Legal Compliance You are solely responsible for ensuring that your use of the Services is in full compliance with all applicable laws and regulations. Brazilian and International Laws: You agree not to use the Services to create or disseminate Generated Content that violates any law in force in Brazil or in relevant international jurisdictions. This includes, but is not limited to, laws on copyrights, intellectual property, defamation, hate speech, terrorism, and child exploitation. Marco Civil da Internet and LGPD: Your use must respect the principles established by the Marco Civil da Internet (Law No. 12,965/2014) and the General Data Protection Law (LGPD - Law No. 13,709/2018). 2.3. Consent for Use of Personal Data The AIVAX Account Manager is the controller of the data entered into the Services. If the Input Content includes personal data of third parties, you declare and guarantee that: You Have the Appropriate Legal Basis: You have obtained the necessary legal basis (such as the explicit and informed consent of the data subject) to collect, process, and send this data to AIVAX for inference purposes. Full Responsibility: You are solely and exclusively responsible for complying with all obligations under the LGPD regarding this data, including responding to requests from data subjects. AIVAX acts only as an operator of this data under your instructions. 3. Generated Content and Intellectual Property 3.1. Ownership and Responsibility for Generated Content Subject to these Terms, AIVAX grants you all rights, titles, and interests in the Generated Content. In other words: what you create is yours. Consequently, you are the sole and exclusive responsible party for the Generated Content and its subsequent use. You assume all risks associated with it, including its legality, accuracy, suitability, and potential violations of third-party rights. AIVAX has no responsibility or obligation regarding your use of the Generated Content. 3.2. Adult, Explicit, and Sensitive Content AIVAX is a tool, and as such, it can be used to generate a wide range of content. The generation of explicit, adult, or pornographic content is allowed, provided that the following conditions are strictly observed: Total Responsibility: You assume total and complete responsibility for the creation, storage, and distribution of such material. Undisputed Legality: The material must not, under any circumstances, violate any law in force on the subject, with zero tolerance for content that represents or suggests child exploitation, non-consensual violence, or any other form of illegal abuse. Consent: If the material involves the representation of real individuals, you must have the explicit and verifiable consent of these individuals to create and use their images for this purpose. Access Control: You are responsible for implementing your own access control and age verification mechanisms if you decide to make this content available to third parties. AIVAX does not endorse this type of content and reserves the right to investigate and suspend accounts that violate the conditions above. 4. Third-Party Model Providers AIVAX's Services may route your Input Content to AI models operated by third parties. By using a specific model, you may also be subject to the terms of use of the provider of that model. It is your responsibility to review and comply with such terms. AIVAX is not responsible for the policies or practices of third-party providers. 5. Suspension and Termination AIVAX reserves the right to suspend or terminate your access to the Services, at our sole discretion and without prior notice, for any violation of these Terms. Activities that may lead to suspension include, but are not limited to, legal compliance violations, system abuse, or non-payment. 6. Limitation of Liability and Disclaimer of Warranties THE SERVICES ARE PROVIDED \"AS IS\" AND \"AS AVAILABLE\", WITHOUT WARRANTIES OF ANY KIND, WHETHER EXPRESS OR IMPLIED. AIVAX DOES NOT GUARANTEE THAT THE SERVICES WILL BE UNINTERRUPTED, SECURE, OR ERROR-FREE. IN NO EVENT WILL AIVAX BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM YOUR ACCESS TO OR USE OF THE SERVICES. 7. Changes to the Terms We may modify these Terms at any time. We will notify you of changes by posting the new Terms on the platform or sending a communication to your contact email. Continued use of the Services after the effective date of the changes will constitute your acceptance of the revised Terms. 8. General Provisions These Terms are governed by the laws of the Federative Republic of Brazil. The forum of the Comarca of [City], [State], Brazil, is elected to resolve any disputes arising from these Terms, with express waiver of any other, however privileged. For any questions about these Terms of Service, please contact us at **legal@aivax.net**."
  },
  "docs/en/limits.html": {
    "href": "docs/en/limits.html",
    "title": "API Limits | AIVAX",
    "keywords": "API Limits Rate limits regulate the number of requests you can send within a time window. These limits help AIVAX prevent abuse and provide a stable API to everyone. The API limits below are the same for all AIVAX embedded models. These limits are categorized by operations performed by the API. Each account has a tier that defines which limits are applied to the account. Tiers change according to the total invested in AIVAX and the time the account has existed. Tier zero: new account that has never added credits or has test credits. Tier 1: account created at least 48 hours ago and has added any credit value. Tier 2: account created at least 1 month ago and has added at least $100 in credits. Tier 3: account created at least 3 months ago and has added at least $1,000 in credits. The measurement is by credit addition and not by consumption. For example, you don't need to consume $100 in credits to advance to Tier 2. Limit legends: RPM: requests per minute. RPD: requests per day (24 hours). TPM: input tokens per minute. New account Tier 1 Tier 2 Tier 3 Operation RPM RPD TPM Document search 50 - - Document insertion - 100 - Inference 5 300 50,000 Inference (high-end models) - - - Tools (shared) - 100 - web_search tool - 20 - x_posts_search tool - 20 - generate_image tool - 5 - Operation RPM RPD TPM Document search 150 - - Document insertion - 3,000 - Inference 75 10,000 1,000,000 Inference (high-end models) 75 10,000 200,000 Tools (shared) - 1,000 - web_search tool - 300 - x_posts_search tool - 300 - generate_image tool - 30 - Operation RPM RPD TPM Document search 300 - - Document insertion - 10,000 - Inference 200 - 4,000,000 Inference (high-end models) 200 - 1,000,000 Tools (shared) - 10,000 - web_search tool - 1,000 - x_posts_search tool - 1,000 - generate_image tool - 300 - Operation RPM RPD TPM Document search 1,000 - - Document insertion - 30,000 - Inference 1,000 - 10,000,000 Inference (high-end models) 1,000 - 4,000,000 Tools (shared) - 50,000 - web_search tool - 10,000 - x_posts_search tool - 10,000 - generate_image tool - 1,000 - Document search: includes semantic search of documents in a collection by the search endpoint ../collections/{id}/query. Document insertion: includes creation and modification of documents in a collection. Inference: every inference or function call, either by chat client or API. high-end models refer to models that require Tier 1 to be used. Tools (shared): every built-in tool invoked by the assistant. This limit is shared for all tools provided by AIVAX and is not used for tools defined by you or your APIs. Tool (tool name): every use of the mentioned tool. Limits for BYOK (Bring-your-own-key) For models provided by you, the applied limit is 1,500 requests per minute. This limit is separate from the integrated inference limit."
  },
  "docs/en/models.html": {
    "href": "docs/en/models.html",
    "title": "Modelos | AIVAX",
    "keywords": "Modelos A AIVAX provÃª modelos de diferentes provedores para tornar o desenvolvimento ainda mais rÃ¡pido, dispensando a necessidade de ter que configurar uma conta para cada provedor para ter acessos aos seus modelos mais recentes. Veja a lista abaixo dos modelos disponÃ­veis e suas precificaÃ§Ãµes. Todos os preÃ§os consideram o total de entrada e saÃ­da de tokens, com ou sem cache. Todos os preÃ§os estÃ£o em dÃ³lares dos Estados Unidos. deepseekai Nome do modelo PreÃ§os DescriÃ§Ã£o @deepseekai/r1 Entrada: $ 0.80 /1m tokens SaÃ­da: $ 2.40 /1m tokens High-capacity chain-of-thought reasoning model with a massive 164K token context window. Ideal for complex math, step-by-step logic, and advanced code authoring. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @deepseekai/v3 Entrada: $ 0.50 /1m tokens SaÃ­da: $ 1.50 /1m tokens Balanced reasoning model with a 128K token context. Delivers efficient tool calling, reliable code assistance, and strong analytical output at lower cost. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @deepseekai/r1-distill-llama-70b Entrada: $ 0.75 /1m tokens SaÃ­da: $ 0.99 /1m tokens Distilled Llama 3 70B model emulating R1â€™s chain-of-thought prowess. Offers transparent reasoning blocks and fast throughput on Groq hardware. Pensamento profundo FunÃ§Ãµes JSON google Nome do modelo PreÃ§os DescriÃ§Ã£o @google/gemini-2.5-pro Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens One of the most powerful models today. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo @google/gemini-1.5-pro Entrada: $ 1.25 /1m tokens SaÃ­da: $ 5.00 /1m tokens Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-2.5-flash Entrada: $ 0.30 /1m tokens SaÃ­da: $ 2.50 /1m tokens Google's best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @google/gemini-2.5-flash-lite Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @google/gemini-2.0-flash Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-2.0-flash-lite Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-1.5-flash-8b Entrada: $ 0.04 /1m tokens SaÃ­da: $ 0.08 /1m tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON inception Nome do modelo PreÃ§os DescriÃ§Ã£o @inception/mercury Entrada: $ 0.25 /1m tokens SaÃ­da: $ 1.00 /1m tokens Extremely fast model by generative diffusion. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON metaai Nome do modelo PreÃ§os DescriÃ§Ã£o @metaai/llama-3.3-70b Entrada: $ 0.59 /1m tokens SaÃ­da: $ 0.79 /1m tokens Previous generation model with many parameters and surprisingly fast speed. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-4-maverick-17b-128e Entrada: $ 0.20 /1m tokens SaÃ­da: $ 0.60 /1m tokens Fast model, with 17 billion activated parameters and 128 experts. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-4-scout-17b-16e Entrada: $ 0.11 /1m tokens SaÃ­da: $ 0.34 /1m tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-3.1-8b Entrada: $ 0.05 /1m tokens SaÃ­da: $ 0.08 /1m tokens Cheap and fast model for less demanding tasks. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON model-router Nome do modelo PreÃ§os DescriÃ§Ã£o @model-router/gemini Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash, Gemini 2.5 Flash (no thinking) and Gemini 2.5 Flash (dynamic thinking). @model-router/openai Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 5 Nano, GPT 5 Mini and o4-mini. @model-router/openai-high Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 5 Mini, o4-mini (low) and o4-mini (high). @model-router/llama Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for Meta Llama. The routing is made between Llama 4 Scout, Llama 4 Maverick and Llama 3.3 70b. moonshotai Nome do modelo PreÃ§os DescriÃ§Ã£o @moonshotai/kimi-k2 Entrada: $ 1.00 /1m tokens SaÃ­da: $ 3.00 /1m tokens Model with 1tri total parameters, 32bi activated parameters, optimized for agentic intelligence. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON nvidia Nome do modelo PreÃ§os DescriÃ§Ã£o @nvidia/llama-3.1-70b-nemotron Entrada: $ 0.88 /1m tokens SaÃ­da: $ 0.88 /1m tokens Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON openai Nome do modelo PreÃ§os DescriÃ§Ã£o @openai/chatgpt-4o Entrada: $ 5.00 /1m tokens SaÃ­da: $ 15.00 /1m tokens The GPT-4o model snapshot used by ChatGPT. Entrada: aceita imagens Chamadas de funÃ§Ã£o @openai/gpt-4o Entrada: $ 2.50 /1m tokens SaÃ­da: $ 10.00 /1m tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-5 Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens OpenAI's newest flagship model for coding, reasoning, and agentic tasks across domains. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-5-chat Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens GPT-5 snapshot currently used by OpenAI's ChatGPT. Entrada: aceita imagens Chamadas de funÃ§Ã£o @openai/gpt-4.1 Entrada: $ 2.00 /1m tokens SaÃ­da: $ 8.00 /1m tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/o3 Entrada: $ 2.00 /1m tokens SaÃ­da: $ 8.00 /1m tokens A well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/o4-mini Entrada: $ 1.10 /1m tokens SaÃ­da: $ 4.40 /1m tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/o3-mini Entrada: $ 1.10 /1m tokens SaÃ­da: $ 4.40 /1m tokens o3-mini provides high intelligence at the same cost and latency targets of previous versions of o-mini series. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-5-mini Entrada: $ 0.25 /1m tokens SaÃ­da: $ 2.00 /1m tokens GPT-5 mini is a faster, more cost-efficient version of GPT-5. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-4.1-mini Entrada: $ 0.40 /1m tokens SaÃ­da: $ 1.60 /1m tokens Fast and cheap for focused tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-oss-120b Entrada: $ 0.15 /1m tokens SaÃ­da: $ 0.75 /1m tokens OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with 120 billion parameters and 128 experts. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-4o-mini Entrada: $ 0.15 /1m tokens SaÃ­da: $ 0.60 /1m tokens Smaller version of 4o, optimized for everyday tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-oss-20b Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.50 /1m tokens OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with 20 billion parameters and 128 experts. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-4.1-nano Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens The fastest and cheapest GPT 4.1 model. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-5-nano Entrada: $ 0.05 /1m tokens SaÃ­da: $ 0.40 /1m tokens OpenAI's fastest, cheapest version of GPT-5. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON qwen Nome do modelo PreÃ§os DescriÃ§Ã£o @qwen/qwen3-480b-coder Entrada: $ 2.00 /1m tokens SaÃ­da: $ 2.00 /1m tokens 480B-parameter coder-specialized LLM with a 64K-token context window, ultrafast inference and structured JSON outputs. Optimized for high-throughput code generation, debugging, review and documentation across multiple languages. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @qwen/qwen3-235b-a22b Entrada: $ 0.60 /1m tokens SaÃ­da: $ 1.20 /1m tokens Cost-efficient MoE model with 235B parameters (22 experts) and a 64K-token context window. Excels in instruction following, logical reasoning, mathematics, coding, multilingual tasks, tool integration, and structured JSON outputs. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @qwen/qwen3-235b-a22b-think Entrada: $ 0.60 /1m tokens SaÃ­da: $ 1.20 /1m tokens Thinking variant of the 235B/22-expert MoE model with a 64K-token context. Delivers enhanced chain-of-thought reasoning, ultrafast inference, full tool calling and JSON functions for complex problem solving. Pensamento profundo FunÃ§Ãµes JSON @qwen/qwen3-32b Entrada: $ 0.29 /1m tokens SaÃ­da: $ 0.59 /1m tokens 32B-parameter LLM with a 131K-token context window, offering advanced chain-of-thought reasoning, seamless tool calling, native JSON outputs, and robust multilingual fluency. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON zai-org Nome do modelo PreÃ§os DescriÃ§Ã£o @zai-org/glm-4.5 Entrada: $ 0.55 /1m tokens SaÃ­da: $ 2.19 /1m tokens High-capacity Mixture-of-Experts model (335B/32 experts) with a 131K-token context window. Combines deep reasoning, fast tool calling and native JSON function outputs for large-scale language understanding and structured data tasks. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @zai-org/glm-4.5-air Entrada: $ 0.22 /1m tokens SaÃ­da: $ 0.88 /1m tokens Cost-efficient MoE variant (110B/32 experts) of GLM-4.5 with a 131K-token context. Delivers high-speed inference, full tool integration and JSON function support for budget-sensitive deployments. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON"
  },
  "docs/en/pricing.html": {
    "href": "docs/en/pricing.html",
    "title": "Pricing | AIVAX",
    "keywords": "Pricing The AIVAX payment model is pre-paid, meaning you use our services with the balance you add to your account. We do not send invoices at the beginning of the month for your usage. This way, it is predictable to know how much you will spend using our inference and agent creation services. AIVAX charges a small fee (variable by payment method) when adding credits to cover taxes, payment provider fees, and our service fee. The pricing of models and inference is provided directly by the inference providers and their models, such as Google and OpenAI. There are no fees or additions on top of these prices. You pay the same value you would pay to these providers directly. We use different services to help you create agentive assistants. Some tools and services have a cost, and these costs are passed on to your account without any additional fees. Inference is charged in US dollars (USD), so there may be currency fluctuations when converting from your local currency to US dollars. Bring-your-own-key (BYOK) You can bring your own OpenAI-compatible API key to use directly on AIVAX. Since we do not know which model you will be using, we do not charge anything on top of the inference you use on your models. Additionally, when using your own model with AIVAX, the rate limits are increased to 3,600 requests per minute, which is equivalent to 60 requests per second. Note that you are still charged for services you use with your own models, such as RAG, internet search, image generation, etc. If your account has a negative balance, you will not be able to use any services, including inference for your own API keys, until you add balance again."
  },
  "docs/en/protocol-functions.html": {
    "href": "docs/en/protocol-functions.html",
    "title": "Server-side Functions | AIVAX",
    "keywords": "Server-side Functions The AIVAX protocol functions, or server-side functions, are an implementation where the model tool calls occur on the server side. Similar to MCP, but with native support for authentication and optimized to work externally. The protocol functions allow actions to be taken on the AIVAX server side, removing the need to implement the function on the client side and integrating with existing applications and services. These functions expect a callback through a URL, and when the model decides to call the function, the callback is accessed with the parameters informed by the assistant itself. The assistant does not know which URL it is calling, as it remains invisible to both the assistant and the user. Choosing the Function Name The function name should be simple and deterministic to what this function does. Avoid names that are difficult to guess or do not refer to the role of the function, as the assistant may become confused and not call the function when appropriate. As an example, let's consider a function to query a user in an external database. The following names are good examples to consider for the call: search_user query_user search_user Bad names include: search (too broad) query-user-in-database-data (name too large) pesquisa-usuario (name not in English) search user (name with improper characters) Having the function name, we can think about the function description. Choosing the Function Description The function description should explain conceptually two situations: what it does and when it should be called by the assistant. This description should include the scenarios that the assistant should consider calling it and when it should not be called, providing a few examples of calls (one-shot) and/or making the function rules explicit. Defining Protocol Functions These functions are defined in the AI-gateway, which allows the creation of intelligent agents that perform actions without human intervention. They follow a simple syntax, expecting the function name, a description of what it does, and the invocation parameters. Protocol functions are defined in the AI gateway following the JSON: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctions\": [ { \"name\": \"list-clients\", \"description\": \"Use this tool to list and search for the user's clients.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use this tool to get details and orders from a client through their ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"format\": \"uuid\" } }, \"required\": [\"user_id\"] } } ] } } In the snippet above, you are providing two functions for your AI model: list-clients and view-client, which will decide which one to execute during its reasoning. You can also provide a JSON content format for which the model will call your API providing the informed content. You can also define the list of supported functions through an endpoint. Every time the model receives a message, it will consult the provided endpoint to get a list of functions it can execute. Define the function listing endpoints in your AI gateway: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctionSources\": [ \"https://my-external-api.com/api/scp/listings\" ] } } The function provision endpoint must respond following the format: GET https://my-external-api.com/api/scp/listings { \"functions\": [ { \"name\": \"list-clients\", \"description\": \"Use this tool to list and search for the user's clients.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view-client\", \"description\": \"Use this tool to get details and orders from a client through their ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"format\": \"uuid\" } }, \"required\": [\"user_id\"] } } ] } These functions are stored in cache for 10 minutes before a new request is made to the provided endpoint. Handling Function Calls The functions are invoked at the endpoint provided in callbackUrl through an HTTP POST request, with the body: { \"function\": { \"name\": \"view-client\", \"content\": { \"user_id\": \"3e5a2823-98fa-49a1-831a-0c4c5d33450e\" } }, \"context\": { \"externalUserId\": \"...\", \"moment\": \"2025-05-18T03:36:27\" } } The response to this action must always respond with an HTTP OK status (2xx or 3xx), even for errors that the assistant may have made. A non-OK response will indicate to the assistant that it was not possible to call the function and it will not continue with what it was planning to do. Response Format Successful responses must be textual and will be attached as a response to the function in the way it is responded by the endpoint. There is no JSON format or structure for this response, but it is advisable to provide a simple, human-readable response, so that the assistant can read the result of the action. Errors can be common, such as not finding a client by ID or some field not being in the desired format. In these cases, respond with an OK status and in the response body include a human description of the error and how the assistant can work around it. It is guaranteed that the request will strictly follow the JSON schema of the content provided by the function definition. Functions that do not expect arguments should not specify a content format for that function. Important The more functions you define, the more tokens you will consume in the reasoning process. The function definition, as well as its format, consumes tokens from the reasoning process. Authentication The authentication of requests is done through the X-Aivax-Nonce header sent in all protocol function requests, even the listing ones. See the authentication manual to understand how to authenticate reverse AIVAX requests. User Authentication Function calls send a field $.context.externalUserId containing the user tag created in a chat session. This tag can be used to authenticate the user who called this function. Security Considerations For the AI model, only the function name, description, and format are visible. It is not capable of seeing the endpoint to which that function points. Additionally, it does not have access to the user tag that is authenticated in a chat client."
  },
  "docs/entities/ai-gateways/ai-gateway.html": {
    "href": "docs/entities/ai-gateways/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway Os gateways de AI Ã© um serviÃ§o que a AIVAX fornece para criar um tÃºnel de inferÃªncia entre um modelo de LLM e uma base de conhecimento. Nele Ã© possÃ­vel: Criar um modelo com instruÃ§Ãµes personalizadas Usar um modelo provido por vocÃª atravÃ©s de um endpoint OpenAI compatÃ­vel, ou usar um modelo disponibilizado pela AIVAX Personalizar parÃ¢metros de inferÃªncia, como temperatura, top_p, prefill Usar uma coleÃ§Ã£o de conhecimento como fundaÃ§Ã£o de respostas para IA Dentre outros recursos. Com o AI Gateway, vocÃª cria um modelo pronto para uso, parametrizado e fundamentado nas instruÃ§Ãµes que vocÃª definir. Modelos VocÃª pode trazer um modelo de IA compatÃ­vel com a interface OpenAI para o gateway de IA. Se vocÃª trazer seu modelo de IA, iremos cobrar apenas pela pesquisa de documentos anexada na IA. VocÃª tambÃ©m pode usar um dos modelos abaixo que jÃ¡ estÃ£o prontos para comeÃ§ar com o AIVAX. Ao usar um modelo, vocÃª perceberÃ¡ que alguns sÃ£o mais inteligentes que outros para determinadas tarefas. Alguns modelos sÃ£o melhores com certas estratÃ©gias de obtenÃ§Ã£o de dados do que outros. Realize testes para encontrar o melhor modelo. VocÃª pode ver os modelos disponÃ­veis na pÃ¡gina de modelos. Usar um gateway de IA A AIVAX provÃª um endpoint compatÃ­vel com a interface OpenAI atravÃ©s de um AI-gateway, o que facilita a integraÃ§Ã£o do modelo criado pela AIVAX com aplicaÃ§Ãµes e SDKs existentes. Vale ressaltar que somente algumas propriedades sÃ£o suportadas. Em um gateway de IA, vocÃª jÃ¡ configura os parÃ¢metros do modelo, como System Prompt, temperatura e nome do modelo. Ao usar esse endpoint, alguns valores do gateway podem ser sobrescritos pela requisiÃ§Ã£o. RequisiÃ§Ã£o POST /api/v1/chat-completions { \"model\": \"0197cb0f-893a-7b7d-be0a-71ada1208aaf\", \"messages\": [ { \"role\": \"user\", \"content\": \"Quem Ã© vocÃª?\" } ], \"stream\": false } Resposta para nÃ£o-streaming { \"id\": \"0197dbdc-6456-7d54-b7ec-04cb9c80f460\", \"object\": \"chat.completion\", \"created\": 1751740343, \"model\": \"@google/gemini-2.5-flash\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"completion_text\": \"Oi! ðŸ˜Š Eu sou a Zia, a assistente inteligente do ZÃ© do Ingresso. TÃ´ aqui pra te ajudar com tudo sobre ingressos, eventos, e tudo que rola na nossa querida SÃ£o JosÃ© do Rio Preto. Se precisar de alguma coisa, tipo saber sobre nomeaÃ§Ã£o de ja o bagulho, sÃ³ chamar! Vamos juntos aproveitar tudo o que tiver rolando! O que vocÃª precisa? ðŸ¥³ \", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 1118, \"completion_tokens\": 88, \"total_tokens\": 1206 }, \"service_tier\": \"default\" } Resposta streaming data: {\"id\":\"0197dbde-c40b-720d-a13e-f689c303c571\",\"object\":\"chat.completion.chunk\",\"created\":1751740498,\"model\":\"@google/gemini-2.5-flash\",\"system_fingerprint\":\"fp_y8jidd\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} ... data: {\"id\":\"0197dbde-c88c-7764-8017-c1fee0d79096\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@google/gemini-2.5-flash\",\"system_fingerprint\":\"fp_2he7ot\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"\"}}],\"usage\":null,\"sentinel_usage\":null} data: {\"id\":\"0197dbde-c890-7329-9e65-faecbe158efa\",\"object\":\"chat.completion.chunk\",\"created\":1751740500,\"model\":\"@aivax\\/sentinel-mini\",\"system_fingerprint\":\"fp_q6qh7x\",\"choices\":[{\"index\":0,\"finish_reason\":\"STOP\",\"delta\":{}}],\"usage\":{\"prompt_tokens\":1097,\"completion_tokens\":92,\"total_tokens\":1189},\"sentinel_usage\":null} Uso com SDKs Por prover endpoints compatÃ­veis com a interface OpenAI, a AIVAX Ã© totalmente compatÃ­vel com SDKs existentes, facilitando a integraÃ§Ã£o plug-and-play. Veja o exemplo abaixo: from openai import OpenAI client = OpenAI( base_url=\"https://inference.aivax.net/api/v1\", api_key=\"oky_gr5u...oqbfd3d9y\" ) response = client.chat.completions.create( model=\"my-gateway:50c3\", # you can also provide your ai-gateway full ID here messages=[ {\"role\": \"user\", \"content\": \"Explain why AI-gateways are useful.\"} ] ) print(response.choices[0].message.content) No momento, a AIVAX sÃ³ suporta o formato chat/completions. No futuro, pretendemos criar suporte para a API Responses."
  },
  "docs/entities/ai-gateways/pipelines.html": {
    "href": "docs/entities/ai-gateways/pipelines.html",
    "title": "Pipelines de IA | AIVAX",
    "keywords": "Pipelines de IA A AIVAX fornece vÃ¡rios pipelines para usar em seu gateway de IA. VocÃª pode usar vÃ¡rios pipelines para executarem no contexto do seu gateway. RAG AtravÃ©s de coleÃ§Ãµes, vocÃª pode vincular uma coleÃ§Ã£o de documentos para seu gateway de IA. VocÃª pode definir os parÃ¢metros de incorporaÃ§Ã£o, como quantidade de documentos, pontuaÃ§Ã£o mÃ­nima e estratÃ©gia de incorporaÃ§Ã£o. Cada estratÃ©gia de incorporaÃ§Ã£o Ã© mais refinada que a outra. Algumas criam resultados melhores que as demais, mas Ã© importante realizar testes prÃ¡ticos com vÃ¡rias estratÃ©gias para entender qual se ajusta melhor no modelo, conversa e tom do usuÃ¡rio. Talvez seja necessÃ¡rio realizar ajustes no prompt do sistema para informar melhor como a IA deverÃ¡ considerar os documentos anexados na conversa. Os documentos sÃ£o anexados como uma mensagem do usuÃ¡rio, limitados aos parÃ¢metros que vocÃª define na estratÃ©gia de obtenÃ§Ã£o. EstratÃ©gias com reescrita normalmente geram os melhores resultados Ã  um baixo custo de latÃªncia e custo. O modelo de reescrita usado sempre o com menor custo, escolhido normalmente por um pool interno que decide o modelo que estÃ¡ com menor latÃªncia no momento. EstratÃ©gias sem custo de reescrita: Plain: a estratÃ©gia padrÃ£o. Ã‰ a menos otimizada e nÃ£o possui custo de reescrita: a Ãºltima mensagem do usuÃ¡rio Ã© usada como termo de busca para pesquisar na coleÃ§Ã£o anexada do gateway. Concatenate: Concatena em linhas as Ãºltimas N mensagens do usuÃ¡rio, e entÃ£o o resultado da concatenaÃ§Ã£o Ã© usada como termo de busca. EstratÃ©gias com custo de reescrita (os tokens de inferÃªncia sÃ£o cobrados): UserRewrite: reescreve as Ãºltimas N mensagens do usuÃ¡rio usando um modelo menor, criando uma pergunta contextualizada no que o usuÃ¡rio quer dizer. FullRewrite: reescreve as Ãºltimas N*2 mensagens do chat usando um modelo menor. Similar ao UserRewrite, mas considera tambÃ©m as mensagens da assistente na formulaÃ§Ã£o da nova pergunta. Geralmente cria as melhores perguntas, com um custo um pouco maior. Ã‰ a estratÃ©gia mais estÃ¡vel e consistente. Funciona com qualquer modelo. EstratÃ©gias de funÃ§Ã£o: QueryFunction: fornece uma funÃ§Ã£o de pesquisa na coleÃ§Ã£o integrada para o modelo de IA. VocÃª deverÃ¡ ajustar nas instruÃ§Ãµes do sistema os cenÃ¡rios ideais para o modelo chamar essa funÃ§Ã£o quando necessÃ¡rio. Pode nÃ£o funcionar tÃ£o bem em modelos menores. Ao definir uma coleÃ§Ã£o de RAG no pipeline de seu gateway, a primeira mensagem do contexto da conversa serÃ¡ o resultado da incorporaÃ§Ã£o do RAG como uma mensagem do usuÃ¡rio (exceto para quando usado como ferramentas onde o resultado da incorporaÃ§Ã£o Ã© anexado como uma resposta de ferramenta). Definir muitos documentos de resposta do RAG aumenta o consumo de tokens de entrada e pode aumentar o preÃ§o final da inferÃªncia. Fixando instruÃ§Ãµes O pipeline de instruÃ§Ãµes permite prefixar instruÃ§Ãµes em diversos lugares do modelo, guiando e restrigindo o formato de resposta do modelo. As formas atuais de definir instruÃ§Ãµes sÃ£o: InstruÃ§Ãµes do sistema: insire um texto fixo no prompt de sistema do contexto. Template de prompt do usuÃ¡rio: reformata a pergunta do usuÃ¡rio para seguir um formato especÃ­fico de pergunta. InicializaÃ§Ã£o de assistente (prefill): inicializa a mensagem da assistente com tokens iniciais de geraÃ§Ã£o. Esses parÃ¢metros podem ser muito Ãºteis para prompt engineering, no entanto, podem nÃ£o ser compatÃ­vel com todos os modelos. AtenÃ§Ã£o: prefixando instruÃ§Ãµes, templates e inicializaÃ§Ãµes podem remover a capacidade de raciocÃ­nio, interpretaÃ§Ã£o multi-modalidades e chamadas de ferramentas do modelo. ParametrizaÃ§Ã£o O pipeline de parametrizaÃ§Ã£o configura os hiper-parÃ¢metros iniciais da inferÃªncia, como temperatura, nucleus sampling, presence penalty e demais hiperparÃ¢metros de inferÃªncia. Truncating O pipeline de truncating permite definir o tamanho de uma conversa em tokens antes dela ser recortada. Quando esse pipeline estÃ¡ ativado, antes de toda inferÃªncia, Ã© calculado se num_of_chars / 4 Ã© maior que o mÃ¡ximo de tokens de entrada da conversa. Se o contexto for maior, o pipeline comeÃ§a a remover mensagens do comeÃ§o da conversa atÃ© que as mensagens caibam no contexto especificado. Ao menos uma mensagem do usuÃ¡rio (comumente a Ãºltima mensagem) Ã© mantida na conversa. Todas as demais mensagens sÃ£o removidas, exceto as instruÃ§Ãµes do sistema. Alternativamente, vocÃª pode definir que ao atingir o limite um erro Ã© disparado na API ao invÃ©s de recortar o contexto. Tool message truncating O pipeline de contagem de mensagens de ferramentas Ã© similar ao de truncating: ele remove a resposta de ferramentas mais antigas e preserva somente as mais novas. Isso pode ser Ãºtil para quando respostas de ferramentas anteriores nÃ£o sejam mais Ãºteis em mensagens mais recentes e ocupam espaÃ§o no contexto, mas pode ser prejudicial ao usar com modelos agÃªnticos que chamam ferramentas em cadeia. Esse pipeline Ã© configurado em quantidade de mensagens de ferramentas Ã  serem preservadas ao invÃ©s de tokens. Quando uma mensagem de ferramenta Ã© considerada antiga, ela nÃ£o Ã© removida, mas tem seu conteÃºdo removido. Ferramentas do lado do servidor Esse pipeline permite a execuÃ§Ã£o de ferramentas do lado do servidor da AIVAX, similar ao protocolo MCP. Leia mais sobre esse pipeline aqui. Ferramentas embutidas VocÃª pode adicionar ferramentas providas pela AIVAX em seu gateway, como pesquisa na internet, geraÃ§Ã£o de imagens e acesso de links. Consulte todas as ferramentas disponÃ­veis aqui. Workers Workers definem o comportamento do seu gateway remotamente, usado para controlar quando certos eventos devem ser abortados ou continuados. Leia mais sobre esse pipeline aqui."
  },
  "docs/entities/ai-gateways/workers.html": {
    "href": "docs/entities/ai-gateways/workers.html",
    "title": "Workers de IA | AIVAX",
    "keywords": "Workers de IA Os workers do gateway de IA sÃ£o recursos do Gateways de IA que permitem controlar o comportamento de seus recursos remotamente atravÃ©s de eventos. Com um controlador externo configurado, eventos sÃ£o enviados para ele, e a resposta do seu controlador define se aquela aÃ§Ã£o deve continuar, ser abortada ou configurada. Quando um evento Ã© acionado no lado da AIVAX, uma requisiÃ§Ã£o POST Ã© disparada ao worker configurado com informaÃ§Ãµes do evento disparado. Com base em sua resposta, a aÃ§Ã£o pode ser anulada ou configurada. NÃ£o hÃ¡ nenhum cache - a requisiÃ§Ã£o Ã© feita em todos os eventos que ocorrem no seu gateway de IA. O tempo de processamento da resposta acrescenta uma latÃªncia entre toda aÃ§Ã£o do gateway, no entanto, adiciona uma camada de controle e moderaÃ§Ã£o que vocÃª pode controlar Ã  qualquer momento. Criando um worker de IA Quando um evento Ã© acionado, uma requisiÃ§Ã£o POST Ã© disparada ao seu worker seguindo o formato abaixo: { \"gatewayId\": \"0197dda5-985f-7d76-96e5-0d0451c539f6\", \"moment\": \"2025-08-09T00:21:40\", \"event\": { \"name\": \"message.received\", \"data\": { \"message\": { \"role\": \"user\", \"content\": \"Bom dia!\" }, \"origin\": [ \"SessionsApi\" ], \"externalUserId\": \"mini-app-session@lot1xc9k03g2my3j4w2y1\" } } } O exemplo acima ilustra uma mensagem do evento message.received com os seus argumentos do evento. ApÃ³s o envio da requisiÃ§Ã£o, a AIVAX aguarda a resposta do seu worker, e com ela: Resposta OK (2xx): continua e prossegue com a execuÃ§Ã£o normal do evento. Outras respostas: aborta e interrompe a execuÃ§Ã£o do evento. Lista de eventos Atualmente, os eventos que podem ser enviados para seu worker sÃ£o: message.received - enviado quando uma mensagem Ã© recebida pelo gateway. Esse evento Ã© acionado com a Ãºltima mensagem recebida no contexto, o que pode ser do usuÃ¡rio ou nÃ£o. { \"name\": \"message.received\", \"data\": { \"message\": {}, // chat/completions message entity \"origin\": [ \"SessionsApi\" // message origin ], \"externalUserId\": \"mini-app-session@lot1xc9k03g2my3j4w2y1\" } } Exemplo O exemplo abaixo ilustra um Cloudflare Worker que autentica uma conversa no Telegram com base no nome de usuÃ¡rio da converÃ§a: export default { async fetch(request, env, ctx) { // O ID do gateway que estamos esperando lidar const CHECKING_GATEWAY_ID = \"0197dda5-985f-7c76-96e5-0d0451c596e5\"; const ALLOWED_USERNAMES = [ \"myusername\" ]; if (request.method == \"POST\") { const requestData = await request.json(); const { event, gatewayId } = requestData; // Verifica se Ã© um evento de mensagem recebida, se Ã© o gateway que estamos // gerenciando no worker e se essa mensagem vem de um chat do Telegram if (gatewayId === CHECKING_GATEWAY_ID && event.name == \"message.received\" && event.data.externalUserId?.startsWith(\"zp_telegram:\")) { // obtÃ©m o username do telegram, que estÃ¡ entre o ':' e o '@' do externalUserId const telegramUsername = event.data.externalUserId.split(':')[0].split('@')[0]; // verifica se o usuÃ¡rio Ã© permitido na integraÃ§Ã£o if (!ALLOWED_USERNAMES.includes(telegramUsername)) { // o usuÃ¡rio nÃ£o existe na lista de usernames permitidos, logo, retorna uma resposta nÃ£o-ok // indicando que a mensagem nÃ£o deve ser enviada return new Response(\"User is not authed\", { status: 400 }); } } } // continua com a execuÃ§Ã£o return new Response(); } };"
  },
  "docs/entities/chat-clients.html": {
    "href": "docs/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients Um cliente de chat provÃª uma interface de usuÃ¡rio atravÃ©s de um AI Gateway que permite o usuÃ¡rio conversar com sua assistente. Um chat client Ã© integrado Ã  inferÃªncia do AI gateway e dÃ¡ suporte para pensamento profundo, pesquisa e conversa por texto. Recursos multi-modais, como envio de imagens e Ã¡udio estÃ£o em desenvolvimento. VocÃª pode personalizar a interface do seu chat client com CSS e JavaScript personalizado, alÃ©m de poder escolher a linguagem dos recursos do chat. Criar uma sessÃ£o de chat Uma sessÃ£o de chat Ã© onde vocÃª cria uma conversa entre seu chat client e o usuÃ¡rio. VocÃª pode chamar esse endpoint informando contexto adicional para conversa, como o nome do usuÃ¡rio, onde ele estÃ¡, etc. Uma sessÃ£o de chat expira apÃ³s algum tempo por seguranÃ§a do token de acesso gerado. Quando vocÃª chama esse endpoint informando uma tag vocÃª pode chamar o mesmo endpoint vÃ¡rias vezes e obter a sessÃ£o de chat que estÃ¡ ativa para a tag informada, ou criar um chat novo se nÃ£o existir uma sessÃ£o em andamento. Quando uma sessÃ£o Ã© encontrada no cliente de chat atravÃ©s da tag informada, a sessÃ£o Ã© renovada pelo perÃ­odo informado e o contexto Ã© atualizado. Uma sessÃ£o de chat tambÃ©m restaura todas as mensagens da conversa da mesma sessÃ£o apÃ³s desconexÃ£o. O usuÃ¡rio pode limpar a conversa ao clicar no botÃ£o de limpar conversa no canto superior direito do cliente de chat. Essa sessÃ£o usa os limites definidos pelo cliente de chat, como mÃ¡ximo de mensagens e tokens na conversa. Se uma sessÃ£o estiver prÃ³xima de expirar, ela Ã© renovada por mais 20 minutos na prÃ³xima mensagem do usuÃ¡rio. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Tempo em segundos para o chat expirar. O mÃ­nimo Ã© 10 minutos. O mÃ¡ximo Ã© 30 dias. \"expires\": 3600, // Opcional. Contexto adicional para a IA sobre o chat. \"extraContext\": \"# Contexto adicional\\r\\n\\r\\nVocÃª estÃ¡ falando com Eduardo.\", // Opcional. Fornece um endpoint para a sessÃ£o obter contexto adicional. Esse endpoint Ã© chamado em toda mensagem enviada pelo usuÃ¡rio, atualizada em tempo real sem qualquer cache. \"contextLocation\": \"https://example.com/context.txt\", // Opcional (recomendado). Um id externo para identificar a sessÃ£o posteriormente e reaproveitÃ¡-la sempre que chamar o mesmo endpoint. Pode ser o ID do usuÃ¡rio do seu banco de dados ou uma string que facilite a identificaÃ§Ã£o desse chat posteriormente. \"tag\": \"my-user-tag\" } Resposta { \"message\": null, \"data\": { // ID da sessÃ£o de chat criada \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Chave pÃºblica de acesso do chat \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // A URL pÃºblica para conversar com o chat \"talkUrl\": \"https://console.aivax.net/chat/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } } SessÃµes de integraÃ§Ãµes A AIVAX fornece duas integraÃ§Ãµes para clientes de chat: Telegram e WhatsApp (atravÃ©s do Z-Api). Cada conversa em um aplicativo Ã© uma sessÃ£o individual, identificada pelo ID da conversa ou nÃºmero de telefone do usuÃ¡rio. Essas sessÃµes obedecem as regras do chat client original. AlÃ©m disso, sessÃµes de chat nessas integraÃ§Ãµes possuem dois comandos especiais: /reset: limpa o contexto atual da sessÃ£o. /usage: quando debug estÃ¡ ativo no chat client, exibe o uso atual do chat em tokens."
  },
  "docs/entities/collections.html": {
    "href": "docs/entities/collections.html",
    "title": "ColeÃ§Ãµes | AIVAX",
    "keywords": "ColeÃ§Ãµes Uma coleÃ§Ã£o Ã© uma biblioteca de conhecimento: ela abriga vÃ¡rios documentos de conhecimento. Use coleÃ§Ãµes para agrupar documentos por finalidade, como documentar um produto, uma empresa, um serviÃ§o ou fluxo. ColeÃ§Ãµes nÃ£o produzem custo. NÃ£o hÃ¡ limite de coleÃ§Ãµes por conta. Criar uma coleÃ§Ã£o Para criar uma coleÃ§Ã£o vazia, informe apenas o nome dela: RequisiÃ§Ã£o POST /api/v1/collections { // O nome da coleÃ§Ã£o nÃ£o pode ser vazio. \"collectionName\": \"Minha primeira coleÃ§Ã£o\" } Resposta { \"message\": null, \"data\": { // ID Ãºnico da coleÃ§Ã£o criada. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } Listar coleÃ§Ãµes Lista as coleÃ§Ãµes disponÃ­veis na sua conta. RequisiÃ§Ã£o GET /api/v1/collections Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"Minha coleÃ§Ã£o\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Outra coleÃ§Ã£o\" } ] } } Ver uma coleÃ§Ã£o ObtÃ©m detalhes de uma coleÃ§Ã£o, como seu progresso de indexaÃ§Ã£o e informaÃ§Ãµes como data de criaÃ§Ã£o. RequisiÃ§Ã£o GET /api/v1/collections/{collection-id}/ Resposta { \"message\": null, \"data\": { \"name\": \"Minha coleÃ§Ã£o\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // traz a quantidade de documentos aguardando indexaÃ§Ã£o \"queuedDocuments\": 0, // quantidade de documentos pronto para consulta \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Excluir uma coleÃ§Ã£o Exclui uma coleÃ§Ã£o e todos os documentos nela. Essa aÃ§Ã£o Ã© irreversÃ­vel. RequisiÃ§Ã£o DELETE /api/v1/collections/{collection-id}/ Resposta { \"message\": \"Collection deleted successfully.\", \"data\": null } Limpar uma coleÃ§Ã£o Diferente da exclusÃ£o de coleÃ§Ã£o, essa operaÃ§Ã£o remove todos os documentos da coleÃ§Ã£o, incluindo os indexados e os em fila. RequisiÃ§Ã£o DELETE /api/v1/collections/{collection-id}/reset-only Resposta { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/entities/documents.html": {
    "href": "docs/entities/documents.html",
    "title": "Documentos | AIVAX",
    "keywords": "Documentos Um documento representa um pedaÃ§o de um conhecimento. Ã‰ um trecho limitado, autosuficiente e que faÃ§a sentido de forma isolada. Um documento Ã© o componente que Ã© indexado pelo modelo interno para ser recuperado posteriormente atravÃ©s de um termo de busca semÃ¢ntico. Considere um manual sobre um carro: ele nÃ£o Ã© um documento mas sim vÃ¡rios documentos. Cada um destes documentos fala, de forma isolada, sobre um determinado assunto sobre esse carro, de forma que esse documento nÃ£o dependa de um contexto ou informaÃ§Ã£o externa para fazer sentido. Cada documento deste manual irÃ¡ falar de um assunto: um irÃ¡ falar sobre como ligar o carro, outro de como desligÃ¡-lo, outro de como sua pintura Ã© feita e outro de como trocar o Ã³leo periodicamente. NÃ£o Ã© uma boa ideia reservar um documento para falar de vÃ¡rias coisas ao mesmo tempo, pois isso irÃ¡ reduzir a objetividade e escopo da inferÃªncia e reduzir a qualidade de obtenÃ§Ã£o. Exemplos de criaÃ§Ã£o de documentos: âŒ NÃ£o faÃ§a NÃ£o crie documentos muito curtos (com 10 ou menos palavras). NÃ£o crie documentos muito grandes (com 700) ou mais palavras. NÃ£o fale sobre mais de uma coisa em um documento. NÃ£o misture linguas diferentes em documentos. NÃ£o seja implÃ­cito em documentos. NÃ¢o escreva documentos usando linguagem tÃ©cnica, como cÃ³digos ou estruturas como JSON. âœ… FaÃ§a Seja explÃ­cito sobre o objetivo do seu documento. Foque documentos em assuntos individuais, que resumam o que deve ser feito ou explicado. Sempre repita termos que sÃ£o palavras-chave para a busca do documento. Exemplo: prefira usar \"A cor do Honda Civic 2015 Ã© amarela\" ao invÃ©s de \"a cor do carro Ã© amarelo\". Restrinja o conteÃºdo do documento para falar de apenas um tÃ³pico ou assunto. Use uma linguagem humana, simples e fÃ¡cil de entender. Uso da API Como todos os documentos sÃ£o entidades que pertencem Ã  uma coleÃ§Ã£o, sempre tenha em mÃ£os a coleÃ§Ã£o de onde o documento estÃ¡/serÃ¡ localizado. Enviar documentos em lote Para enviar uma lista em massa de documentos para uma coleÃ§Ã£o, estruture-os seguindo o formato JSONL. A estrutura do arquivo de indexaÃ§Ã£o Ã©: {\"docid\":\"Carros/HondaCivic2015.rmd:1\",\"text\":\"O Honda Civic 2015 estÃ¡ disponÃ­vel em [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:2\",\"text\":\"O motor do Honda Civic 2015 Ã© [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:3\",\"text\":\"A cor do Honda Civic 2015 Ã© Amarela [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} ... A estrutura Ã© compsta pelas propriedades: Propriedade Tipo DescriÃ§Ã£o docid string Especifica o nome do documento. Ãštil para depuraÃ§Ã£o e identificaÃ§Ã£o. text string O conteÃºdo \"cru\" do documento que serÃ¡ indexado. __ref string Opcional. Especifica um ID de referÃªncia do documento. __tags string[] Opcional. Especifica um array de tags do documento. Ãštil para gestÃ£o de documentos. A referÃªncia de um documento Ã© um ID que pode ser especificado em vÃ¡rios documentos que precisam estar vinculados em uma busca quando um dos mesmos for correspondido em uma busca de similaridade. Por exemplo, se uma busca encontrar um documento que possui um ID de referÃªncia, todos os outros documentos da mesma coleÃ§Ã£o que compartilham o mesmo ID de referÃªncia do documento correspondido tambÃ©m serÃ£o incluÃ­dos na resposta da busca. O uso de referÃªncias pode ser Ãºtil para quando um documento depende de outro ou mais documentos para fazer sentido. NÃ£o hÃ¡ exigÃªncia de formato para o ID de referÃªncia: qualquer formato Ã© aceito. VocÃª pode enviar atÃ© 1.000 linhas de documentos por requisiÃ§Ã£o. Se precisar enviar mais documentos, separe o envio em mais requisiÃ§Ãµes. Se vocÃª enviar um documento com mais de 1.000 linhas, as linhas seguintes serÃ£o ignoradas. Vale notar que documentos muito longos, que excede a quantidade de tokens permitida no modelo de embedding interno, terÃ£o seu conteÃºdo truncado e a qualidade de indexaÃ§Ã£o poderÃ¡ ser gravemente afetada. Para evitar esse problema, envie documentos que contenham entre 20 e 700 palavras. Warning AtenÃ§Ã£o: esse endpoint gera custo. O custo Ã© calculado em cima dos tokens do conteÃºdo de cada documento. O conteÃºdo de cada documento Ã© tokenizado de acordo com o modelo usado na indexaÃ§Ã£o dos documentos. RequisiÃ§Ã£o O envio deve ser feito usando multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Resposta { \"message\": null, \"data\": [ { \"name\": \"Institucional/Empresa.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institucional/Empresa.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institucional/Empresa.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Produtos/Agendamentos.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Criar ou modificar documento Esse endpoint cria ou modifica um documento a partir do seu nome. Quando um documento Ã© modificado, seus vetores de indexaÃ§Ã£o sÃ£o resetados, isto Ã©, o documento entrarÃ¡ em fila novamente para ser indexado pelo motor de indexaÃ§Ã£o. Essa indexaÃ§Ã£o nÃ£o Ã© isenta de custo. O custo Ã© relativo Ã  quantidade de tokens do conteÃºdo enviado. O custo somente Ã© gerado quando o documento Ã© de fato alterado. Chamar essa rota com o mesmo conteÃºdo do documento nÃ£o gera modificaÃ§Ã£o, portanto, nÃ£o gera custo. Warning AtenÃ§Ã£o: esse endpoint gera custo. O custo Ã© calculado em cima dos tokens do conteÃºdo do arquivo. O conteÃºdo do arquivo Ã© tokenizado de acordo com o modelo usado na indexaÃ§Ã£o dos documentos. RequisiÃ§Ã£o PUT /api/v1/collections/{collection-id}/documents { // o nome do documento que serÃ¡ modificado \"name\": \"document-name\", // o conteÃºdo do documento que serÃ¡ criado ou sobreposto caso o nome jÃ¡ exista \"contents\": \"ConteÃºdo do meu documento\", // parÃ¢metros explicados anteriormente \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Resposta { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // o estado da operaÃ§Ã£o indica se o documento foi modificado \"Modified\" ou criado \"Created\". Sempre virÃ¡ apenas um valor no array. \"state\": [\"Modified\"] } } Listar documentos Esse endpoint lista todos os documentos disponÃ­veis em uma coleÃ§Ã£o. VocÃª pode passar um parÃ¢metro da query adicional filter para filtrar documentos por nome, tag ou conteÃºdo. Esse filtro suporta expressÃµes que auxiliam a filtrar o que vocÃª estÃ¡ procurando: -t \"tag\" - filtra documentos que possuem essa tag. -r \"reference\" - filtra documentos que possuem esse ID de referÃªncia. -c \"content\" - filtra documentos que possuem esse trecho em seu conteÃºdo. -n \"name\" - filtra documentos que possuem esse trecho em seu nome. RequisiÃ§Ã£o GET /api/v1/collections/{collection-id}/documents Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Ajuda/Clientes.rmd:1\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"Um cliente Ã© um cadastro na sua platafor...\", \"indexState\": [\"Indexed\"] }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Ajuda/Clientes.rmd:2\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"No cadastro do cliente, Ã© possÃ­vel modif...\", \"indexState\": [\"Indexed\"] }, ... ] } } Ver documento VÃª detalhes sobre um documento especÃ­fico. RequisiÃ§Ã£o GET /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institucional/Empresa.rmd:1\", // representa a situaÃ§Ã£o de indexaÃ§Ã£o do documento. // valores vÃ¡lidos: Queud, Indexed, Cancelled \"state\": [\"Indexed\"], // conteÃºdo do documento indexado \"contents\": \"...\", // id da referÃªncia do documento \"reference\": \"institucional-empresa\" } } Excluir documento Permanentemente exclui um documento atravÃ©s do seu ID. RequisiÃ§Ã£o DELETE /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/entities/functions.html": {
    "href": "docs/entities/functions.html",
    "title": "FunÃ§Ãµes | AIVAX",
    "keywords": "FunÃ§Ãµes FunÃ§Ãµes Ã© uma forma de forÃ§ar seu modelo ao processamento de informaÃ§Ãµes usando JSON como intermÃ©dio de comunicaÃ§Ã£o. Com as funÃ§Ãµes, vocÃª consegue fazer qualquer modelo responder no formato JSON que vocÃª quiser. Pode ser Ãºtil para categorizar comentÃ¡rios, aplicar moderaÃ§Ã£o em avaliaÃ§Ãµes ou processar informaÃ§Ãµes com auxÃ­lio da IA. No momento, sÃ³ Ã© possÃ­vel usar funÃ§Ãµes com modelos providos pela AIVAX. Chamar uma funÃ§Ã£o Para chamar uma funÃ§Ã£o de IA, vocÃª precisarÃ¡ informar o que a IA deverÃ¡ responder e fornecer um JSON Schema que ela deverÃ¡ seguir. Modelos menos inteligentes tendem a falhar a geraÃ§Ã£o de JSON, gerando um documento invÃ¡lido ou problemÃ¡tico. Para isso, ajuste seu modelo, a instruÃ§Ã£o e o parÃ¢metro de tentativas se for necessÃ¡rio. VocÃª Ã© cobrado por cada tentativa que a IA tentar gerar. Modelos um pouco mais inteligentes tendem a gerar resultados corretos na primeira tentativa. Ã‰ garantido que um JSON vÃ¡lido serÃ¡ gerado e que esse JSON seguirÃ¡ o mesmo esquema fornecido na requisiÃ§Ã£o. Considere usar um cache do lado da sua aplicaÃ§Ã£o para dados que nÃ£o precisam ser constantementes atualizados, como dados meteorolÃ³gicos, estatÃ­sticas diÃ¡rias, etc. A AIVAX nÃ£o realiza nenhum cache pelo nosso lado. RequisiÃ§Ã£o POST /api/v1/functions/json { // ObrigatÃ³rio. Especifique o nome do modelo integrado que serÃ¡ usado para realizar a aÃ§Ã£o. \"modelName\": \"@metaai/llama-3.1-8b\", // ObrigatÃ³rio. Explique o que seu modelo deverÃ¡ fazer com a entrada e como ele deve trazer a resposta. \"instructions\": \"Classifique o comentÃ¡rio do usuÃ¡rio, indicando se Ã© positivo ou negativo, e se possui alguma informaÃ§Ã£o relevante (nÃºmero entre 0 (pouco relevante) e 10 (muito relevante))\", // ObrigatÃ³rio. O JSON Schema que o modelo deverÃ¡ seguir para gerar a resposta. VocÃª pode fornecer exemplos de geraÃ§Ã£o no campo de instruÃ§Ãµes. \"responseSchema\": { \"type\": \"object\", \"properties\": { \"feedbackType\": { \"type\": \"string\", \"enum\": [\"neutral\", \"positive\", \"negative\"] }, \"informationScore\": { \"type\": \"integer\", \"minimum\": 0, \"maximum\": 10 } }, \"required\": [\"feedbackType\", \"informationScore\"] }, // Opcional. Define uma entrada JSON para o modelo. Pode ser qualquer tipo de valor JSON. \"inputData\": { \"userComment\": \"Pessimo mercado. Tem guarda dentro te vigiando pra vc nao roubar e os acougueiros te ignoram e atendem mocinhas bonitinhas na tua frente. Mas graÃ§as a Deus tem outros mercados chegando e o fim dessa palhaÃ§ada vai chegar\" }, // Opcional. Define quantas tentativas o modelo deve tentar antes da API retornar um erro. Deve ser um nÃºmero entre 1 e 30. \"maxAttempts\": 10, // Opcional. Define o tempo limite em segundos para obter um JSON vÃ¡lido antes da API retornar um erro. Deve ser um nÃºmero entre 1 e 3600 (uma hora). \"timeout\": 300, // Opcional. Define a temperatura de geraÃ§Ã£o do JSON. Valores maiores tendem a serem mais criativos, enquanto menores mais determinÃ­sticos. NÃºmero de 0 Ã  2. \"temperature\": 0.4, // Opcional. Fornece contexto adicional para a geraÃ§Ã£o atravÃ©s de mensagens no formato chat/completions. VocÃª pode fornecer conteÃºdo multi-modalidades tambÃ©m para modelos compatÃ­veis. \"context\": [ { \"role\": \"user\", \"content\": \"Additional context\" } ], // Opcional. Fornece funÃ§Ãµes embutidas da AIVAX para a geraÃ§Ã£o da ferramenta. \"tools\": [ \"WebSearch\", \"Code\", \"OpenUrl\", \"ImageGeneration\", \"XPostsSearch\" ], // Opcional. Define parÃ¢metros de geraÃ§Ã£o de ferramentas. \"toolsOptions\": { \"webSearchMode\": \"Full\" | \"Summarized\", \"webSearchMaxResults\": 10, \"imageGenerationMaxResults\": 2, \"imageGenerationQuality\": \"Low\" | \"Medium\" | \"High\" | \"Highest\", \"imageGenerationAllowMatureContent\": false } } Resposta { \"result\": { \"requiresAttention\": true, \"shortSummary\": \"Customer threatens cancellation and bad publicity if not contacted today.\" }, \"attempt\": 0, \"elapsedMilliseconds\": 1235, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0000123, \"unitPrice\": 1e-7, \"quantity\": 123, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0000116, \"unitPrice\": 4e-7, \"quantity\": 29, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" } ], \"runnedFunctions\": [] } } Diretrizes do JSON Schema O formato de resposta deve ser fornecido por um JSON Schema. Por trÃ¡s dos panos, a AIVAX guia o modelo para gerar uma resposta com o esquema JSON fornecido. Quando o modelo gera algo invÃ¡lido, indicamos Ã  ele tentar novamente e corrigir os erros atÃ© que a saÃ­da esteja conforme a especificaÃ§Ã£o fornecida. As diretrizes suportadas do JSON Schema da AIVAX sÃ£o: string: minLength maxLength pattern format Pode ser date-time, email, time, duration, uri, url, ipv4, ipv6, uuid ou guid. enum number e integer: minimum maximum exclusiveMinimum exclusiveMaximum multipleOf array items uniqueItems minItems maxItems object properties required bool e boolean null AlÃ©m disso, Ã© possÃ­vel informar um ou mais valores no type do objeto, exemplo: { \"type\": [\"string\", \"number\"] } Nota: number e integer sÃ£o sinÃ´nimos e integer nÃ£o garante que o nÃºmero serÃ¡ um inteiro. FunÃ§Ãµes em ferramentas Ã‰ possÃ­vel usar ferramentas embutidas as funÃ§Ãµes JSON. Isso irÃ¡ permitir que o modelo chame funÃ§Ãµes para obter contexto necessÃ¡rio para gerar o JSON final. Exemplos Confira exemplos de funÃ§Ãµes de IA para vÃ¡rias tarefas cotidianas: Classificar comentÃ¡rios bons ou ruins POST /api/v1/functions/json { \"modelName\": \"@google/gemini-2.0-flash\", \"instructions\": \"Classifique o comentÃ¡rio do usuÃ¡rio fornecendo uma nota para seu comentÃ¡rio.\", \"inputData\": { \"inputText\": \"A comida Ã© boa, mas o ambiente Ã© muito barulhento e um pouco sujo tambÃ©m.\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"commentSummary\": { \"type\": \"string\", \"description\": \"Resumo do que o usuÃ¡rio quis dizer.\" }, \"score\": { \"type\": \"integer\", \"min\": 1, \"max\": 5, \"description\": \"A nota extraÃ­da da avaliaÃ§Ã£o, sendo 1 muito ruim e 5 muito bom.\" } }, \"required\": [ \"commentSummary\", \"score\" ] } } { \"result\": { \"commentSummary\": \"The food is good, but the environment is noisy and a bit dirty.\", \"score\": 3 }, \"attempt\": 0, \"elapsedMilliseconds\": 788, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0000083, \"unitPrice\": 1e-7, \"quantity\": 83, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0000128, \"unitPrice\": 4e-7, \"quantity\": 32, \"description\": \"JSON function rendering (@google/gemini-2.0-flash)\" } ], \"runnedFunctions\": [] } } Avaliar uma expressÃ£o matemÃ¡tica POST /api/v1/functions/json { \"modelName\": \"@qwen/qwen3-32b\", \"instructions\": \"Evaluate the given math problem and provide the result step-by-step.\", \"inputData\": { \"inputText\": \"what is two plus two minus pi ?\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"steps\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"stepDescription\": { \"type\": \"string\", \"description\": \"Current math step description.\" }, \"carry\": { \"type\": \"number\", \"description\": \"The current result.\" } }, \"required\": [ \"stepDescription\", \"carry\" ] }, \"minItems\": 1 }, \"finalResult\": { \"type\": \"number\", \"description\": \"The math operation final result.\" } }, \"required\": [ \"steps\", \"finalResult\" ] }, \"tools\": [ \"Code\" ] } { \"result\": { \"steps\": [ { \"stepDescription\": \"Add 2 and 2\", \"carry\": 4 }, { \"stepDescription\": \"Subtract pi (Ï€) from the result\", \"carry\": 0.858407346410207 } ], \"finalResult\": 0.858407346410207 }, \"attempt\": 0, \"elapsedMilliseconds\": 5775, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"inference.functions.json.in\", \"amount\": 0.00031001, \"unitPrice\": 2.9e-7, \"quantity\": 1069, \"description\": \"JSON function rendering (@qwen/qwen3-32b)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.00054162, \"unitPrice\": 5.9e-7, \"quantity\": 918, \"description\": \"JSON function rendering (@qwen/qwen3-32b)\" } ], \"runnedFunctions\": [ { \"functionName\": \"evaluate_code\", \"success\": true, \"context\": { \"arguments\": \"console.log(2 + 2 - Math.PI);\", \"result\": { \"evaluatedCode\": \"console.log(2 + 2 - Math.PI);\", \"result\": \"0.8584073464102069\\nScript evaluation result: undefined\\n\" } } } ] } } Trazer Ãºltimas notÃ­cias e clima de uma determinada cidade POST /api/v1/functions/json { \"modelName\": \"@openai/gpt-4o-mini\", \"instructions\": \"Search for the 5 latest news and weather data for the given city.\", \"inputData\": { \"city\": \"Tokyo\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"latestNews\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"title\": { \"type\": \"string\" }, \"details\": { \"type\": \"string\" }, \"link\": { \"type\": \"string\", \"format\": \"uri\" } }, \"required\": [ \"title\", \"details\", \"link\" ], \"additionalProperties\": false } }, \"weather\": { \"type\": \"object\", \"properties\": { \"currentTemperature\": { \"type\": \"number\" }, \"currentWeather\": { \"type\": \"string\", \"enum\": [ \"sunny\", \"cloudy\", \"rain\", \"thunderstorm\" ] }, \"forecast\": { \"type\": \"string\", \"enum\": [ \"sunny\", \"cloudy\", \"rain\", \"thunderstorm\" ] } }, \"required\": [ \"currentTemperature\", \"currentWeather\", \"forecast\" ], \"additionalProperties\": false }, \"assistantSummary\": { \"type\": \"string\" } }, \"required\": [ \"latestNews\", \"weather\", \"assistantSummary\" ], \"additionalProperties\": false }, \"tools\": [ \"XPostsSearch\", \"WebSearch\" ] } { \"result\": { \"latestNews\": [ { \"title\": \"Indian Ambassador to Japan Expands Cooperation\", \"details\": \"Indian Ambassador to Japan Sibi George expressed eagerness to expand cooperation between India and Japan in business, technology, and security.\", \"link\": \"https://japannews.yomiuri.co.jp/\" }, { \"title\": \"Emperor Emeritus Akihito Discharged from Hospital\", \"details\": \"Japanâ€™s Emperor Emeritus Akihito was discharged from the University hospital.\", \"link\": \"https://www.japantimes.co.jp/\" }, { \"title\": \"Tokyo Stocks Climb Following Wall Street Gains\", \"details\": \"Tokyo stocks climbed in the morning following Wall Street gains.\", \"link\": \"https://www.independent.co.uk/topic/tokyo\" }, { \"title\": \"Tightening of Business Manager Visa Requirements\", \"details\": \"Tokyo is tightening requirements for popular business manager visas.\", \"link\": \"https://www.japantimes.co.jp/latest-news/\" }, { \"title\": \"ANA Plans Affordable Flying Taxi Service\", \"details\": \"ANA plans to launch an affordable flying taxi service in Japan by 2027.\", \"link\": \"https://www.japantimes.co.jp/\" } ], \"weather\": { \"currentTemperature\": 31, \"currentWeather\": \"cloudy\", \"forecast\": \"rain\" }, \"assistantSummary\": \"The latest news from Tokyo includes diplomatic and economic updates, while the current weather is partly cloudy with a temperature of 31Â°C.\" }, \"attempt\": 0, \"elapsedMilliseconds\": 19772, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"serviceusage.web_search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"Web Search request\" }, { \"sku\": \"serviceusage.web_search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"Web Search request\" }, { \"sku\": \"inference.functions.json.in\", \"amount\": 0.000453, \"unitPrice\": 1.5e-7, \"quantity\": 3020, \"description\": \"JSON function rendering (@openai/gpt-4o-mini)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0002406, \"unitPrice\": 6e-7, \"quantity\": 401, \"description\": \"JSON function rendering (@openai/gpt-4o-mini)\" } ], \"runnedFunctions\": [ { \"functionName\": \"web_search\", \"success\": true, \"context\": { \"arguments\": { \"search_term\": \"Tokyo weather\" }, \"result\": { \"results\": [ { \"title\": \"Search results\", \"url\": \"\" } ] } } }, { \"functionName\": \"web_search\", \"success\": true, \"context\": { \"arguments\": { \"search_term\": \"latest news in Tokyo\" }, \"result\": { \"results\": [ { \"title\": \"Search results\", \"url\": \"\" } ] } } } ] } } Trazer artistas em alta por gÃªnero musical POST /api/v1/functions/json { \"modelName\": \"@openai/gpt-4.1-mini\", \"instructions\": \"A funÃ§Ã£o deve pesquisar, usando os Ãºltimos posts do X, as plataformas de streaming musical (como Spotify, Apple Music etc.) e identificar os 10 artistas mais tocados no gÃªnero informado pelo usuÃ¡rio. Em seguida, deve formatar um objeto contendo uma lista ordenada de 10 artistas, incluindo posiÃ§Ã£o (1â€“10), nome e nÃºmero estimado de streams.\", \"inputData\": { \"genre\": \"dubstep\" }, \"responseSchema\": { \"type\": \"object\", \"properties\": { \"artists\": { \"type\": \"array\", \"description\": \"Lista dos 10 artistas mais tocados no gÃªnero especificado\", \"items\": { \"type\": \"object\", \"properties\": { \"rank\": { \"type\": \"integer\", \"description\": \"PosiÃ§Ã£o no Top 10\", \"minimum\": 1, \"maximum\": 10 }, \"name\": { \"type\": \"string\", \"description\": \"Nome do artista\" }, \"monthlyStreams\": { \"type\": \"string\", \"description\": \"NÃºmero aproximado de streams mensais, formatado, ex: \\\"150M\\\"\" }, \"source\": { \"type\": \"string\", \"description\": \"Plataforma ou fonte de dados\" } }, \"required\": [ \"rank\", \"name\", \"monthlyStreams\", \"source\" ], \"additionalProperties\": false } } }, \"required\": [ \"artists\" ], \"additionalProperties\": false }, \"maxAttempts\": 4, \"temperature\": 1, \"tools\": [ \"XPostsSearch\", \"WebSearch\" ], \"toolsOptions\": {} } { \"result\": { \"artists\": [ { \"rank\": 1, \"name\": \"Skrillex\", \"monthlyStreams\": \"31M\", \"source\": \"Spotify\" }, { \"rank\": 2, \"name\": \"Virtual Riot\", \"monthlyStreams\": \"12M\", \"source\": \"Spotify\" }, { \"rank\": 3, \"name\": \"Excision\", \"monthlyStreams\": \"11M\", \"source\": \"Spotify\" }, { \"rank\": 4, \"name\": \"Zeds Dead\", \"monthlyStreams\": \"10M\", \"source\": \"Spotify\" }, { \"rank\": 5, \"name\": \"Flux Pavilion\", \"monthlyStreams\": \"9M\", \"source\": \"Spotify\" }, { \"rank\": 6, \"name\": \"Illenium\", \"monthlyStreams\": \"8.5M\", \"source\": \"Spotify\" }, { \"rank\": 7, \"name\": \"Rusko\", \"monthlyStreams\": \"7M\", \"source\": \"Spotify\" }, { \"rank\": 8, \"name\": \"Bassnectar\", \"monthlyStreams\": \"6M\", \"source\": \"Spotify\" }, { \"rank\": 9, \"name\": \"Seven Lions\", \"monthlyStreams\": \"5.5M\", \"source\": \"Spotify\" }, { \"rank\": 10, \"name\": \"Getter\", \"monthlyStreams\": \"5M\", \"source\": \"Spotify\" } ] }, \"attempt\": 0, \"elapsedMilliseconds\": 11243, \"warnings\": [], \"context\": { \"generatedUsage\": [ { \"sku\": \"serviceusage.x_api.search\", \"amount\": 0.005, \"unitPrice\": 0.005, \"quantity\": 1, \"description\": \"X Posts search\" }, { \"sku\": \"inference.functions.json.in\", \"amount\": 0.0016448, \"unitPrice\": 4e-7, \"quantity\": 4112, \"description\": \"JSON function rendering (@openai/gpt-4.1-mini)\" }, { \"sku\": \"inference.functions.json.out\", \"amount\": 0.0006272, \"unitPrice\": 0.0000016, \"quantity\": 392, \"description\": \"JSON function rendering (@openai/gpt-4.1-mini)\" } ], \"runnedFunctions\": [ { \"functionName\": \"x_posts_search\", \"success\": true, \"context\": { \"arguments\": { \"search_query\": \"dubstep artists Spotify OR Apple Music OR streaming\" }, \"result\": [ { \"url\": \"https://x.com/nathanielblow/status/1867148757466304607\", \"text\": \"NOW LIVE ON APPLE MUSIC. SPOTIFY. YOUTUBE MUSIC.\\n\\nEnjoy. https://t.co/apUxnXc7IE\", \"authorUserName\": \"nathanielblow\", \"createdAt\": \"2024-12-12T10:05:28\" }, { \"url\": \"https://x.com/yobrxxzy/status/1754800896041505222\", \"text\": \"MOST STREAMED ARTISTS ON THESE STREAMING PLATFORMS ðŸ‡³ðŸ‡¬\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nYouTube â€” BURNA BOY\\nPandora â€” WIZKID\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID https://t.co/Nm2jO5R5P6\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2024-02-06T09:35:10\" }, { \"url\": \"https://x.com/yobrxxzy/status/1922763266549301642\", \"text\": \"MOST STREAMED ARTISTS ON THESE DSP:\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nPandora â€” WIZKID\\nYouTube â€” BURNA BOY\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nDeezer â€” WIZKID\\nAnghami â€” REMA\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID\\n\\nðŸ https://t.co/3aUCGbYiEO\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2025-05-14T21:17:40\" }, { \"url\": \"https://x.com/yobrxxzy/status/1856816512524587343\", \"text\": \"MOST STREAMED ARTISTS ON THESE DSP:\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nYouTube â€” BURNA BOY\\nPandora â€” WIZKID\\nTidal â€” WIZKID\\nLine Music â€” WIZKID\\nAudiomack â€” ASAKE\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nDeezer â€” WIZKID\\nAnghami â€” REMA\\nSoundCloud â€” BURNA BOY\\nShazam â€” WIZKID\\n\\nðŸ https://t.co/yL2tmJJpHM\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2024-11-13T21:48:49\" }, { \"url\": \"https://x.com/hourjinnie/status/1858756269902881208\", \"text\": \"PLAYLISTS! Letâ€™s get to streaming and utilize all our tools! More pl coming tomorrow!!!\\n\\nSPOTIFY\\n\\nhttps://t.co/L0HRMpYEQG\\n\\nhttps://t.co/HmhdB859e5\\n\\nhttps://t.co/FA4eDmvvkS\\n\\nhttps://t.co/ZOedbxiKmO\\n\\nhttps://t.co/DyPs0qpOMQ\\n\\nDeezer\\n\\nhttps://t.co/36PY8aXkV7\\n\\nApple Music \\n\\nhttps://t.co/wk5MXtzDYC\\n\\nhttps://t.co/OgovuVtoDq\\n\\nPandora \\n\\nhttps://t.co/idkimHTTjp\\n\\nhttps://t.co/WQ83YKeUOK\", \"authorUserName\": \"hourjinnie\", \"createdAt\": \"2024-11-19T06:16:43\" }, { \"url\": \"https://x.com/yobrxxzy/status/1653723618944090112\", \"text\": \"MOST STREAMED ARTISTS ON THESE STREAMING PLATFORMS:\\n\\nApple Music â€” WIZKID\\nSpotify â€” WIZKID\\nYouTube â€” BURNA BOY\\nPandora â€” WIZKID\\nTidal â€” WIZKID\\nAudiomack â€” BURNA BOY\\nDeezer â€” WIZKID\\nBoomplay â€” BURNA BOY\\nSoundCloud â€” WIZKID\\nShazam â€” WIZKID\\n\\nWhere is @davido ? https://t.co/JOcwJZ2HUU\", \"authorUserName\": \"yobrxxzy\", \"createdAt\": \"2023-05-03T11:30:10\" }, { \"url\": \"https://x.com/EyelanderMusic/status/1882525103482872115\", \"text\": \"YO IF YOU THINK DUBSTEP IS GETTING â€œSTALEâ€ \\n\\nGET THE HELL OFF OF SPOTIFY AND ON TO SOUNDCLOUD \\n\\nTHERE IS SOOO MUCH INSANE UNDERGROUND TALENT THAT RELEASE MOSTLY JUST TO SOUNDCLOUD. \\n\\n(Plus thatâ€™s usually where all showcases, remixes and flips are for copyright purposes)\", \"authorUserName\": \"EyelanderMusic\", \"createdAt\": \"2025-01-23T20:25:34\" }, { \"url\": \"https://x.com/thisiscyclops/status/1815084003050791422\", \"text\": \"i like that more dubstep artists are doing albums this year. like i know itâ€™s not the best release move in 2024 but that kinda lets you know that theyâ€™re doing it out of love for the music rather than like the Best Spotify Strategy ðŸ‘\", \"authorUserName\": \"thisiscyclops\", \"createdAt\": \"2024-07-21T17:58:43\" }, { \"url\": \"https://x.com/mewaolix/status/1886435921895235989\", \"text\": \"Stays, we can easily reach #3 if we push a little ðŸ¥¹\\n\\n(DSP counts Spotify, Apple Music, Amazon Music, Deezer, YT Music, Anghami, Gaana, Joox, Melon, JioSaavn, Boomplay, etc) https://t.co/P2hFoSmbtT\", \"authorUserName\": \"mewaolix\", \"createdAt\": \"2025-02-03T15:25:46\" }, { \"url\": \"https://x.com/bhadext/status/1951588819561546145\", \"text\": \"Listen to my songs on Apple music, ðŸ™\\nBhadext \\n\\nHere: https://t.co/3u79BQvHAZ https://t.co/lmMjN7m2oR\", \"authorUserName\": \"bhadext\", \"createdAt\": \"2025-08-02T10:20:08\" }, { \"url\": \"https://x.com/runthismusic/status/1853150149788209234\", \"text\": \"What if 'Sticky' by @tylerthecreator was Dubstep? \\nOUT NOW ON SC + FREE DL ðŸ’šðŸ’¿\\n@skybreakedm https://t.co/OjaU8um8GC\", \"authorUserName\": \"runthismusic\", \"createdAt\": \"2024-11-03T19:00:00\" }, { \"url\": \"https://x.com/BTStreamingID/status/1953425910612320348\", \"text\": \"Party start ðŸŽ‰\\n\\nPlaylist ðŸŽ¶\\n\\nSpotify : \\n1.Premium: https://t.co/qJwpckiSlS\\n2. Free: https://t.co/NqIV40psvp\\nApple music: https://t.co/dblVEdcd1q\\nYouTube: https://t.co/gmCR3oyfze \\nYouTube Music: https://t.co/xhxF2rt90o\\n\\n#BTS #ë°©íƒ„ì†Œë…„ë‹¨ #PTD_ON_STAGE_LIVE\\n#BangtanPlaylistID\", \"authorUserName\": \"BTStreamingID\", \"createdAt\": \"2025-08-07T12:00:04\" }, { \"url\": \"https://x.com/NewEDMToday/status/1728069779066392828\", \"text\": \"@FlatlandFunk_ @FrankieSinn2k @gabybaumusic @Gladez @grislymusik @HELOSPHEREmusic @HUMANSION_music @kausedubs @Kuhlosul_ @LazrusOfficial @SHEISLUTHIEN @LVCiDdubz @MagMag_dubstep @MVSLOTUNES @raddixofficial @RazrDub @ScarexxDub @SmilesOnlyMusic @soulvalient @SpeedShift6 @stvnkfvcemusic @itstoxicmusic @TremorrOfficial @TyphonOfficial @Verosdubz @voyagerdubz @wilco_beats @zovahofficial all been released on: @Emengy \\n\\ngenre of music: #edm #dubstep\\n\\nwith 30 tracks\\n\\n54 artists\\n\\nhttps://t.co/H9KmeiXJB4\", \"authorUserName\": \"NewEDMToday\", \"createdAt\": \"2023-11-24T15:15:15\" }, { \"url\": \"https://x.com/offsetemusic/status/1729861632308744501\", \"text\": \"i mostly use my local files to listen to music, but i'll share my spotify wrapped anyway\\n\\ni'm happy to see @canotodubz here as no. 1, he's one of the most unique dubstep artists i've discovered this year https://t.co/OUBhgtikn9\", \"authorUserName\": \"offsetemusic\", \"createdAt\": \"2023-11-29T13:55:27\" }, { \"url\": \"https://x.com/paulpoint_/status/1882728999962468414\", \"text\": \"Good Morning ðŸŒž \\n\\nEspecially, Electronic Music Artists\\n\\nUse any of these?\\nâ€”Laptop \\nâ€”â€”MIDI Keyboard\\nâ€”â€”â€”Software\\n\\nHow about these?\\nâ€”Spotify\\nâ€”â€”Soundcloud\\nâ€”â€”â€”Apple Music\\n\\nMaking sounds like these?\\nâ€”House\\nâ€”â€”Techno\\nâ€”â€”â€”Trance\\nâ€”â€”â€”â€”DnB? ...Dubstep??\\n\\nIf you answered yes to anything above and you are staying consistent\\n\\nIf you are incessantly networking and improving your use of the above, particularly for live\\n\\nThen you may well be taking home your slice of a $25 Bn dollar industry\\n\\nExcited? Well are you? Keep making sounds on your computer ðŸ¤\\n\\nCan't wait to hear your next track!\\n\\nP.\", \"authorUserName\": \"paulpoint_\", \"createdAt\": \"2025-01-24T09:55:47\" }, { \"url\": \"https://x.com/Gunfingers_eu/status/1701277617280663828\", \"text\": \"Join Kaps on his bass music journey! From listener to passionate player, he's mastered the dubstep genre, sharing stages with many confirmed artists.\\nExpect a wealth of experience and skills for your enjoyment!\\n\\nSC : https://t.co/s5sqmIzrK6\\n\\nSpotify : https://t.co/o8bmaRXTNU https://t.co/6DVahvcr5D\", \"authorUserName\": \"Gunfingers_eu\", \"createdAt\": \"2023-09-11T16:52:46\" }, { \"url\": \"https://x.com/ballsackious/status/1714854867645149399\", \"text\": \"This new dirt monkey album is another\\nbeautiful example of dubstep artists \\nbranching out to other genres and sounds and creating something gorgeous and unique\\n\\nIt's so common now. So much good music is coming out I'm so pleased \\n\\nhttps://t.co/BJBXGa1Llz\", \"authorUserName\": \"ballsackious\", \"createdAt\": \"2023-10-19T04:03:55\" }, { \"url\": \"https://x.com/karmiclink/status/1951358837828509849\", \"text\": \"Our full karmic discography is LIVE in all major digital streaming platforms (Deezer, Spotify, Youtube, Apple Music, Amazon Music, TikTok, Tidal, Facebook &amp; Instagram for your stories &amp; reels)! â˜¯ #karmiclink #cult #dark #metal #project #rock #athens â˜¯ @TuneCore https://t.co/WWlVrUTVOc\", \"authorUserName\": \"karmiclink\", \"createdAt\": \"2025-08-01T19:06:16\" }, { \"url\": \"https://x.com/OneRougeWave/status/1953444705204588608\", \"text\": \"Oh yeah.. almost forgotðŸ˜….. \\nStream my music ðŸŽ¶â€¦\\n\\nhttps://t.co/mUNJL5AAGf\\n\\nhttps://t.co/aFqbVnuzZV\\n\\n#wavyboss https://t.co/c8mnh9mw3d\", \"authorUserName\": \"OneRougeWave\", \"createdAt\": \"2025-08-07T13:14:45\" } ] } } ] } }"
  },
  "docs/entities/search.html": {
    "href": "docs/entities/search.html",
    "title": "Pesquisa | AIVAX",
    "keywords": "Pesquisa A API de pesquisa, atravÃ©s da query key obtida das coleÃ§Ãµes, realiza uma busca semÃ¢ntica na mesma, realizando uma comparaÃ§Ã£o inteligente para cada documento indexado em uma coleÃ§Ã£o. ApÃ³s criar uma coleÃ§Ã£o, vocÃª obterÃ¡ seu ID. Utilize o ID da sua coleÃ§Ã£o para realizar a busca nos documentos indexados da mesma. Use os endpoints dessa API para embutir a pesquisa semÃ¢ntica de documentos no seu modelo de IA ou chatbot. Pesquisando documentos Esse endpoint espera uma requisiÃ§Ã£o GET com os parÃ¢metros: term: obrigatÃ³rio. Especifica o termo de pesquisa que serÃ¡ pesquisado nos documentos. top: Especifica o mÃ¡ximo de documentos que deverÃ£o ser retornados na busca. min: Especifica o score mÃ­nimo para obtenÃ§Ã£o dos documentos. Warning AtenÃ§Ã£o: esse endpoint gera custo. O custo Ã© calculado em cima dos tokens do termo de busca. O termo de busca Ã© tokenizado de acordo com o modelo usado na indexaÃ§Ã£o dos documentos. RequisiÃ§Ã£o GET /api/v1/collections/{collection-id}/query term=Qual a cor do honda CIVIC? Resposta { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } Para o resultado da busca, quanto maior o score, mais semelhante Ã© o documento para o termo da busca. O AIVAX utiliza modelos de embedding que permitem a orientaÃ§Ã£o da tarefa. Para a busca, o termo Ã© vetorizado com uma orientaÃ§Ã£o DOCUMENT_QUERY. Para indexaÃ§Ã£o dos documentos, a orientaÃ§Ã£o Ã© DOCUMENT_RETRIEVAL, o que fornece uma busca mais otimizada e nÃ£o para averiguar a similaridade entre documentos."
  },
  "docs/getting-started.html": {
    "href": "docs/getting-started.html",
    "title": "Bem-vindo | AIVAX",
    "keywords": "Bem-vindo Boas vindas ao AIVAX. Nosso serviÃ§o torna mais fÃ¡cil o desenvolvimento de modelos de IA inteligentes que usam uma base de conhecimento providenciada por vocÃª para conversar com o usuÃ¡rio, responder perguntas, fornecer informaÃ§Ãµes em tempo real e mais. Para comeÃ§ar, todos os endpoints devem ser feitos na URL de produÃ§Ã£o da AIVAX: https://inference.aivax.net/ Conceitos e definiÃ§Ãµes Entenda os conceitos usados pela API abaixo: Conta: representa uma conta do usuÃ¡rio, que possui um token de autenticaÃ§Ã£o. ColeÃ§Ã£o: representa uma coleÃ§Ã£o de documentos de conhecimento. Um usuÃ¡rio pode ter vÃ¡rias coleÃ§Ãµes de documentos. Documento: representa um fato, um Ãºnico conhecimento e um item de uma coleÃ§Ã£o. Uma coleÃ§Ã£o pode ter vÃ¡rios documentos. AI Gateway: representa um gateway de IA que se beneficia ou nÃ£o de uma coleÃ§Ã£o de conhecimento, como um middleware de conhecimento plug-and-play para um modelo. Modelo embutido: representa um modelo de IA que o AIVAX provÃª para o usuÃ¡rio. Chat client: representa uma interface de usuÃ¡rio que disponibiliza o AI gateway atravÃ©s de um chat interagÃ­vel online. SessÃ£o de chat: abriga uma conversa e contexto de um cliente de chat. Lidando com erros Todos os erros da API retornam uma resposta HTTP com um status nÃ£o OK (nunca 2xx ou 3xx), e sempre seguindo o formato JSON: { \"error\": \"Uma mensagem explicando o erro\", \"details\": {} // um objeto contendo informaÃ§Ãµes relevantes sobre o erro. Na maioria das vezes Ã© nulo }"
  },
  "docs/legal/privacy-policy.html": {
    "href": "docs/legal/privacy-policy.html",
    "title": "PolÃ­tica de Privacidade | AIVAX",
    "keywords": "PolÃ­tica de Privacidade Ãšltima atualizaÃ§Ã£o: 30 de julho de 2025 Bem-vindo Ã  AIVAX. Esta PolÃ­tica de Privacidade descreve como a AIVAX coleta, utiliza, armazena, compartilha e protege as informaÃ§Ãµes dos usuÃ¡rios de nossos serviÃ§os de inferÃªncia de IA. Nosso compromisso Ã© com a transparÃªncia e a proteÃ§Ã£o dos seus dados, em total conformidade com a Lei Geral de ProteÃ§Ã£o de Dados (LGPD) e o Marco Civil da Internet do Brasil. Ao utilizar os serviÃ§os da AIVAX, o gestor da conta AIVAX concorda com as prÃ¡ticas descritas nesta polÃ­tica. 1. DefiniÃ§Ãµes AIVAX: A empresa que fornece os serviÃ§os de inferÃªncia de IA e autora desta polÃ­tica. Gestor da Conta AIVAX: A pessoa fÃ­sica ou jurÃ­dica que cria e administra uma conta na plataforma AIVAX, sendo o principal alvo desta polÃ­tica. UsuÃ¡rio Final: IndivÃ­duos que interagem com aplicaÃ§Ãµes ou serviÃ§os criados pelo Gestor da Conta AIVAX que utilizam a API da AIVAX. Dados de InferÃªncia: Os dados (prompts, entradas, etc.) enviados para os modelos de IA atravÃ©s da API da AIVAX e as respostas (inferÃªncias) geradas por eles. Conversas: O registro das interaÃ§Ãµes de Dados de InferÃªncia, incluindo entradas e saÃ­das. 2. Coleta e Uso de Dados 2.1. Dados de InferÃªncia e Conversas A AIVAX processa os Dados de InferÃªncia que o Gestor da Conta AIVAX envia atravÃ©s de nossa API. Estes dados sÃ£o utilizados exclusivamente para fornecer o serviÃ§o de inferÃªncia solicitado. Compromisso de NÃ£o UtilizaÃ§Ã£o para Treinamento: A AIVAX nunca utiliza os Dados de InferÃªncia ou as Conversas dos seus clientes para treinar seus prÃ³prios modelos de inteligÃªncia artificial ou para qualquer outra finalidade que nÃ£o seja a estrita prestaÃ§Ã£o do serviÃ§o contratado. Seus dados de negÃ³cio e as interaÃ§Ãµes de seus usuÃ¡rios finais permanecem seus. 2.2. Armazenamento de Conversas Para garantir a confiabilidade e a qualidade dos nossos serviÃ§os, a AIVAX armazena temporariamente as Conversas em seus bancos de dados. Este armazenamento tem as seguintes finalidades: Monitoramento e HistÃ³rico: Permitir que o Gestor da Conta AIVAX visualize o histÃ³rico de uso, monitore custos e analise o trÃ¡fego de sua aplicaÃ§Ã£o. DepuraÃ§Ã£o de Problemas (Debugging): Em caso de falhas, erros ou comportamento inesperado dos modelos, o acesso a esses registros Ã© fundamental para que nossa equipe tÃ©cnica possa diagnosticar e resolver problemas de forma eficiente. As conversas sÃ£o tratadas com a mÃ¡xima seguranÃ§a e confidencialidade. O acesso a esses dados Ã© restrito a pessoal autorizado e limitado aos propÃ³sitos mencionados. 2.3. ExclusÃ£o de Dados O Gestor da Conta AIVAX tem controle total sobre os dados armazenados. As conversas sÃ£o removidas permanentemente de nossos sistemas de produÃ§Ã£o apÃ³s um perÃ­odo prÃ©-estabelecido, detalhado em nossa documentaÃ§Ã£o tÃ©cnica. AlÃ©m disso, o Gestor da Conta AIVAX pode, a qualquer momento, solicitar a exclusÃ£o de todas as suas conversas armazenadas atravÃ©s do painel de controle da sua conta AIVAX. Uma vez solicitada, a exclusÃ£o Ã© definitiva e irreversÃ­vel. 3. Provedores de Modelos de Terceiros A AIVAX atua como um orquestrador, roteando solicitaÃ§Ãµes de inferÃªncia para diversos modelos de IA fornecidos por empresas parceiras (\"Provedores de IA\"). Ã‰ crucial entender que: PolÃ­ticas de Terceiros: Cada Provedor de IA possui suas prÃ³prias polÃ­ticas de privacidade e termos de uso. Ao escolher utilizar um modelo especÃ­fico atravÃ©s da AIVAX, o Gestor da Conta AIVAX estÃ¡ sujeito tambÃ©m aos termos daquele provedor. PossÃ­vel Uso para Treinamento por Terceiros: Alguns desses Provedores de IA podem utilizar os dados de inferÃªncia enviados para seus modelos para fins de treinamento e aprimoramento de seus prÃ³prios produtos e serviÃ§os. A AIVAX se esforÃ§a para informar claramente a origem de cada modelo, mas recomenda fortemente que o Gestor da Conta AIVAX revise as polÃ­ticas de privacidade do Provedor de IA correspondente ao modelo que pretende utilizar, especialmente se dados sensÃ­veis forem processados. A AIVAX nÃ£o se responsabiliza pelas prÃ¡ticas de privacidade de terceiros. 4. Responsabilidades 4.1. Responsabilidade do Gestor da Conta AIVAX O Gestor da Conta AIVAX Ã© o controlador dos dados processados atravÃ©s de sua aplicaÃ§Ã£o e Ã© integralmente responsÃ¡vel por: Uso dos Modelos: A escolha dos modelos de IA e a adequaÃ§Ã£o de seu uso para a finalidade pretendida. ConteÃºdo das Conversas: Todo o conteÃºdo, incluindo textos, imagens ou outros dados que transitam pela API da AIVAX, Ã© de responsabilidade do Gestor da Conta. Dados Pessoais: O Gestor da Conta Ã© responsÃ¡vel por garantir que a coleta e o processamento de dados pessoais de seus usuÃ¡rios finais, realizados atravÃ©s da API da AIVAX, estejam em conformidade com a LGPD e outras leis aplicÃ¡veis. Isso inclui a obtenÃ§Ã£o de consentimento, quando necessÃ¡rio, e a gestÃ£o dos direitos dos titulares dos dados. 4.2. IsenÃ§Ã£o de Responsabilidade da AIVAX A AIVAX atua como operadora no processamento dos dados, seguindo as instruÃ§Ãµes do Gestor da Conta (materializadas nas chamadas de API). Portanto, a AIVAX nÃ£o se responsabiliza pela natureza do conteÃºdo processado, nem pela finalidade da interaÃ§Ã£o entre o usuÃ¡rio final, a aplicaÃ§Ã£o do gestor e o modelo de IA. A relaÃ§Ã£o e as obrigaÃ§Ãµes legais decorrentes do uso da aplicaÃ§Ã£o final sÃ£o exclusivamente entre o Gestor da Conta AIVAX e seus usuÃ¡rios finais. 5. Conformidade Legal 5.1. LegislaÃ§Ã£o Brasileira A AIVAX Ã© uma empresa sediada no Brasil. Nossas operaÃ§Ãµes e esta PolÃ­tica de Privacidade sÃ£o regidas e interpretadas de acordo com as leis da RepÃºblica Federativa do Brasil, em especial: Lei Geral de ProteÃ§Ã£o de Dados (LGPD - Lei nÂº 13.709/2018): Comprometemo-nos a seguir todos os seus princÃ­pios e obrigaÃ§Ãµes, garantindo os direitos dos titulares de dados, a seguranÃ§a da informaÃ§Ã£o e a transparÃªncia no tratamento. Marco Civil da Internet (Lei nÂº 12.965/2014): Respeitamos os princÃ­pios de neutralidade da rede, privacidade e liberdade de expressÃ£o, conforme estabelecido na lei. 6. SeguranÃ§a da InformaÃ§Ã£o A AIVAX emprega medidas de seguranÃ§a tÃ©cnicas e organizacionais para proteger os dados contra acesso nÃ£o autorizado, alteraÃ§Ã£o, divulgaÃ§Ã£o ou destruiÃ§Ã£o. Isso inclui criptografia de dados em trÃ¢nsito e em repouso, controle de acesso rigoroso e monitoramento contÃ­nuo de nossos sistemas. 7. AlteraÃ§Ãµes a esta PolÃ­tica A AIVAX pode atualizar esta PolÃ­tica de Privacidade periodicamente para refletir mudanÃ§as em nossas prÃ¡ticas ou na legislaÃ§Ã£o. Notificaremos os Gestores de Conta sobre alteraÃ§Ãµes significativas atravÃ©s de e-mail ou por um aviso em destaque em nossa plataforma. 8. Contato Se vocÃª tiver dÃºvidas sobre esta PolÃ­tica de Privacidade ou sobre as prÃ¡ticas da AIVAX, entre em contato conosco atravÃ©s do e-mail: **legal@aivax.net**."
  },
  "docs/legal/terms-of-service.html": {
    "href": "docs/legal/terms-of-service.html",
    "title": "Termos de Uso | AIVAX",
    "keywords": "Termos de Uso Ãšltima atualizaÃ§Ã£o: 30 de julho de 2025 Bem-vindo Ã  AIVAX. Estes Termos de Uso (\"Termos\") governam o seu acesso e uso dos nossos serviÃ§os de inferÃªncia de IA, APIs, site e qualquer software associado (coletivamente, os \"ServiÃ§os\"). Ao criar uma conta, acessar ou utilizar nossos ServiÃ§os, vocÃª (\"Gestor da Conta AIVAX\") concorda em ficar vinculado a estes Termos e Ã  nossa PolÃ­tica de Privacidade. Se vocÃª nÃ£o concordar com estes Termos, nÃ£o utilize nossos ServiÃ§os. 1. DefiniÃ§Ãµes AIVAX: A empresa que fornece os serviÃ§os. Gestor da Conta AIVAX: A pessoa fÃ­sica ou jurÃ­dica que cria e administra uma conta na plataforma AIVAX e concorda com estes Termos. ConteÃºdo de Entrada: Os dados, textos, prompts ou qualquer outra informaÃ§Ã£o que o Gestor da Conta AIVAX envia para os ServiÃ§os para processamento. ConteÃºdo Gerado: As respostas, textos, imagens ou qualquer outro dado que Ã© gerado pelos modelos de IA como resultado do processamento do ConteÃºdo de Entrada. 2. Uso dos ServiÃ§os e Responsabilidades 2.1. Uso ResponsÃ¡vel e Conduta VocÃª concorda em usar os ServiÃ§os da AIVAX de forma Ã©tica e responsÃ¡vel. Ã‰ estritamente proibido: Abusar do Sistema: NÃ£o se envolver em atividades que abusem, interfiram, interrompam ou prejudiquem os ServiÃ§os, nossos servidores ou redes. Isso inclui, mas nÃ£o se limita a, enviar um volume de requisiÃ§Ãµes excessivo que possa ser caracterizado como ataque de negaÃ§Ã£o de serviÃ§o (DoS), tentar encontrar e explorar vulnerabilidades ou tentar acessar Ã¡reas nÃ£o autorizadas do sistema. Comportamento Inadequado: NÃ£o utilizar os ServiÃ§os para assediar, ameaÃ§ar, difamar, enganar, ou violar os direitos legais e a dignidade de terceiros. A AIVAX preza por um ambiente de uso tecnolÃ³gico saudÃ¡vel e nÃ£o tolerarÃ¡ condutas que possam ser razoavelmente interpretadas como maliciosas ou abusivas. Em resumo: nÃ£o seja um babaca. 2.2. Conformidade Legal VocÃª Ã© o Ãºnico responsÃ¡vel por garantir que o seu uso dos ServiÃ§os esteja em total conformidade com todas as leis e regulamentos aplicÃ¡veis. Leis Brasileiras e Internacionais: VocÃª concorda em nÃ£o usar os ServiÃ§os para criar ou disseminar ConteÃºdo Gerado que viole qualquer lei vigente no Brasil ou em jurisdiÃ§Ãµes internacionais relevantes. Isso inclui, mas nÃ£o se limita a, leis sobre direitos autorais, propriedade intelectual, difamaÃ§Ã£o, incitaÃ§Ã£o ao Ã³dio, terrorismo e exploraÃ§Ã£o infantil. Marco Civil da Internet e LGPD: Seu uso deve respeitar os princÃ­pios estabelecidos pelo Marco Civil da Internet (Lei nÂº 12.965/2014) e pela Lei Geral de ProteÃ§Ã£o de Dados (LGPD - Lei nÂº 13.709/2018). 2.3. Consentimento para Uso de Dados Pessoais O Gestor da Conta AIVAX Ã© o controlador dos dados inseridos nos ServiÃ§os. Caso o ConteÃºdo de Entrada inclua dados pessoais de terceiros, vocÃª declara e garante que: Possui a Base Legal Apropriada: VocÃª obteve a base legal necessÃ¡ria (como o consentimento explÃ­cito e informado do titular dos dados) para coletar, processar e enviar esses dados para a AIVAX para fins de inferÃªncia. Responsabilidade Integral: VocÃª Ã© o Ãºnico e exclusivo responsÃ¡vel por cumprir todas as obrigaÃ§Ãµes da LGPD em relaÃ§Ã£o a esses dados, incluindo a resposta a solicitaÃ§Ãµes dos titulares. A AIVAX atua apenas como operadora desses dados sob suas instruÃ§Ãµes. 3. ConteÃºdo Gerado e Propriedade Intelectual 3.1. Propriedade e Responsabilidade do ConteÃºdo Gerado Sujeito a estes Termos, a AIVAX concede a vocÃª todos os direitos, tÃ­tulos e interesses sobre o ConteÃºdo Gerado. Em outras palavras: o que vocÃª cria Ã© seu. Consequentemente, vocÃª Ã© o Ãºnico e exclusivo responsÃ¡vel pelo ConteÃºdo Gerado e pelo seu uso subsequente. VocÃª assume todos os riscos associados a ele, incluindo sua legalidade, precisÃ£o, adequaÃ§Ã£o e possÃ­veis violaÃ§Ãµes de direitos de terceiros. A AIVAX nÃ£o tem qualquer responsabilidade ou obrigaÃ§Ã£o sobre o uso que vocÃª faz do ConteÃºdo Gerado. 3.2. ConteÃºdo Adulto, ExplÃ­cito e SensÃ­vel A AIVAX Ã© uma ferramenta e, como tal, pode ser utilizada para gerar uma vasta gama de conteÃºdos. A geraÃ§Ã£o de conteÃºdo explÃ­cito, adulto ou pornogrÃ¡fico Ã© permitida, desde que estritamente observadas as seguintes condiÃ§Ãµes: Responsabilidade Total: VocÃª assume total e completa responsabilidade pela criaÃ§Ã£o, armazenamento e distribuiÃ§Ã£o de tal material. Legalidade InquestionÃ¡vel: O material nÃ£o deve, em hipÃ³tese alguma, violar qualquer lei vigente sobre o tema, com tolerÃ¢ncia zero para conteÃºdo que represente ou sugira exploraÃ§Ã£o infantil, violÃªncia nÃ£o consensual ou qualquer outra forma de abuso ilegal. Consentimento: Caso o material envolva a representaÃ§Ã£o de pessoas reais, vocÃª deve ter o consentimento explÃ­cito e verificÃ¡vel dessas pessoas para criar e usar suas imagens para tal finalidade. Controle de Acesso: VocÃª Ã© responsÃ¡vel por implementar seus prÃ³prios mecanismos de controle de acesso e verificaÃ§Ã£o de idade, caso decida disponibilizar esse conteÃºdo a terceiros. A AIVAX nÃ£o endossa esse tipo de conteÃºdo e se reserva o direito de investigar e suspender contas que violem as condiÃ§Ãµes acima. 4. Provedores de Modelos de Terceiros Os ServiÃ§os da AIVAX podem rotear seu ConteÃºdo de Entrada para modelos de IA operados por terceiros. Ao usar um modelo especÃ­fico, vocÃª tambÃ©m pode estar sujeito aos termos de uso do provedor daquele modelo. Ã‰ sua responsabilidade revisar e cumprir tais termos. A AIVAX nÃ£o se responsabiliza pelas polÃ­ticas ou prÃ¡ticas de provedores terceiros. 5. SuspensÃ£o e RescisÃ£o A AIVAX reserva-se o direito de suspender ou rescindir seu acesso aos ServiÃ§os, a nosso critÃ©rio exclusivo e sem aviso prÃ©vio, por qualquer violaÃ§Ã£o destes Termos. Atividades que podem levar Ã  suspensÃ£o incluem, mas nÃ£o se limitam a, violaÃ§Ãµes de conformidade legal, abuso do sistema ou falta de pagamento. 6. LimitaÃ§Ã£o de Responsabilidade e IsenÃ§Ã£o de Garantias OS SERVIÃ‡OS SÃƒO FORNECIDOS \"COMO ESTÃƒO\" E \"CONFORME DISPONÃVEIS\", SEM GARANTIAS DE QUALQUER TIPO, SEJAM EXPRESSAS OU IMPLÃCITAS. A AIVAX NÃƒO GARANTE QUE OS SERVIÃ‡OS SERÃƒO ININTERRUPTOS, SEGUROS OU LIVRES DE ERROS. EM NENHUMA CIRCUNSTÃ‚NCIA A AIVAX SERÃ RESPONSÃVEL POR QUAISQUER DANOS INDIRETOS, INCIDENTAIS, ESPECIAIS, CONSEQUENCIAIS OU PUNITIVOS DECORRENTES DO SEU ACESSO OU USO DOS SERVIÃ‡OS. 7. ModificaÃ§Ãµes nos Termos Podemos modificar estes Termos a qualquer momento. Notificaremos vocÃª sobre as alteraÃ§Ãµes publicando os novos Termos na plataforma ou enviando uma comunicaÃ§Ã£o para o seu e-mail de contato. O uso continuado dos ServiÃ§os apÃ³s a data de vigÃªncia das alteraÃ§Ãµes constituirÃ¡ sua aceitaÃ§Ã£o dos Termos revisados. 8. DisposiÃ§Ãµes Gerais Estes Termos sÃ£o regidos pelas leis da RepÃºblica Federativa do Brasil. Fica eleito o foro da Comarca de [Cidade], [Estado], Brasil, para dirimir quaisquer controvÃ©rsias oriundas destes Termos, com renÃºncia expressa a qualquer outro, por mais privilegiado que seja. Para qualquer dÃºvida sobre estes Termos de Uso, entre em contato conosco pelo e-mail: **legal@aivax.net**."
  },
  "docs/limits.html": {
    "href": "docs/limits.html",
    "title": "Limites da API | AIVAX",
    "keywords": "Limites da API Limites de taxa (\"rate limiters\") regulam o nÃºmero de requisiÃ§Ãµes que vocÃª pode enviar em uma janela de tempo. Esses limites ajudam a AIVAX a prevenir abuso e fornecer uma API estÃ¡vel Ã  todos. Os limites da API abaixo sÃ£o os mesmos para todos os modelos embutidos da AIVAX. Esses limites sÃ£o categorizados por operaÃ§Ãµes feitas pela API. Cada conta possui um tier que define quais limites sÃ£o aplicados Ã  conta. Tiers mudam de acordo com o total investido na AIVAX e o tempo que a conta existe. Tier zero: conta nova que nunca adicionou crÃ©ditos ou que possui crÃ©ditos de teste. Tier 1: conta criada hÃ¡ pelo menos 48 horas e que jÃ¡ adicionou qualquer valor em crÃ©ditos. Tier 2: conta criada hÃ¡ pelo menos 1 mÃªs e que jÃ¡ adicionou pelo menos $ 100 em crÃ©ditos. Tier 3: conta criada hÃ¡ pelo menos 3 meses e que jÃ¡ adicionou pelo menos $ 1.000 em crÃ©ditos. A mediÃ§Ã£o Ã© pela adiÃ§Ã£o de crÃ©ditos e nÃ£o pelo seu consumo. Por exemplo, vocÃª nÃ£o precisa consumir $ 100 em crÃ©ditos para avanÃ§ar ao Tier 2. Legendas dos limites: RPM: requisiÃ§Ãµes por minuto. RPD: requisiÃ§Ãµes por dia (24 horas). TPM: tokens de entrada por minuto. Conta nova Tier 1 Tier 2 Tier 3 OperaÃ§Ã£o RPM RPD TPM Pesquisa de documentos 50 - - InserÃ§Ã£o de documentos - 100 - InferÃªncia 5 300 50.000 InferÃªncia (modelos high-end) - - - Ferramentas (compartilhado) - 100 - Ferramenta web_search - 20 - Ferramenta x_posts_search - 20 - Ferramenta generate_image - 5 - OperaÃ§Ã£o RPM RPD TPM Pesquisa de documentos 150 - - InserÃ§Ã£o de documentos - 3.000 - InferÃªncia 75 10.000 1.000.000 InferÃªncia (modelos high-end) 75 10.000 200.000 Ferramentas (compartilhado) - 1.000 - Ferramenta web_search - 300 - Ferramenta x_posts_search - 300 - Ferramenta generate_image - 30 - OperaÃ§Ã£o RPM RPD TPM Pesquisa de documentos 300 - - InserÃ§Ã£o de documentos - 10.000 - InferÃªncia 200 - 4.000.000 InferÃªncia (modelos high-end) 200 - 1.000.000 Ferramentas (compartilhado) - 10.000 - Ferramenta web_search - 1.000 - Ferramenta x_posts_search - 1.000 - Ferramenta generate_image - 300 - OperaÃ§Ã£o RPM RPD TPM Pesquisa de documentos 1.000 - - InserÃ§Ã£o de documentos - 30.000 - InferÃªncia 1.000 - 10.000.000 InferÃªncia (modelos high-end) 1.000 - 4.000.000 Ferramentas (compartilhado) - 50.000 - Ferramenta web_search - 10.000 - Ferramenta x_posts_search - 10.000 - Ferramenta generate_image - 1.000 - Pesquisa de documentos: inclui pesquisa semÃ¢ntica de documentos em uma coleÃ§Ã£o pelo endpoint de pesquisa ../collections/{id}/query. InserÃ§Ã£o de documentos: inclui criaÃ§Ã£o e modificaÃ§Ã£o de documentos em uma coleÃ§Ã£o. InferÃªncia: toda chamada de inferÃªncia ou funÃ§Ã£o, seja por chat client ou API. modelos high-end se referem Ã  modelos que necessitam de Tier 1 para poder ser usado. Ferramentas (compartilhado): toda ferramenta integrada invocada pela assistente. Esse limite Ã© compartilhado para todas as ferramentas providas pela AIVAX e nÃ£o Ã© usado para ferramentas definidas por vocÃª ou suas APIs. Ferramenta (nome da ferramenta): todo uso da ferramenta mencionada. Limites para BYOK (Bring-your-own-key) Para modelos providos por vocÃª, o limite aplicado Ã© de 1.500 requisiÃ§Ãµes por minuto. Esse limite Ã© separado do limite de inferÃªncia integrada."
  },
  "docs/models.html": {
    "href": "docs/models.html",
    "title": "Modelos | AIVAX",
    "keywords": "Modelos A AIVAX provÃª modelos de diferentes provedores para tornar o desenvolvimento ainda mais rÃ¡pido, dispensando a necessidade de ter que configurar uma conta para cada provedor para ter acessos aos seus modelos mais recentes. Veja a lista abaixo dos modelos disponÃ­veis e suas precificaÃ§Ãµes. Todos os preÃ§os consideram o total de entrada e saÃ­da de tokens, com ou sem cache. Todos os preÃ§os estÃ£o em dÃ³lares dos Estados Unidos. aivax Nome do modelo PreÃ§os DescriÃ§Ã£o @aivax/fn-1 Entrada: $ 1.10 /1m tokens SaÃ­da: $ 4.40 /1m tokens Pesquisa na internet: $ 5.00 /1.000 pesquisas Optimized agent for function usage. Based on OpenAI o4-mini (low). Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo Pesquisa na internet ExecuÃ§Ã£o de cÃ³digo FunÃ§Ãµes JSON @aivax/fn-1-mini Entrada: $ 0.15 /1m tokens SaÃ­da: $ 0.60 /1m tokens Pesquisa na internet: $ 5.00 /1.000 pesquisas Optimized agent for function usage (no reasoning). Based on GPT 4o-mini. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pesquisa na internet ExecuÃ§Ã£o de cÃ³digo FunÃ§Ãµes JSON deepseekai Nome do modelo PreÃ§os DescriÃ§Ã£o @deepseekai/r1-distill-llama-70b Entrada: $ 0.75 /1m tokens SaÃ­da: $ 0.99 /1m tokens Model with deep reasoning and thought, best for most demanding tasks. Pensamento profundo google Nome do modelo PreÃ§os DescriÃ§Ã£o @google/gemini-2.5-pro Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens One of the most powerful models today. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo @google/gemini-1.5-pro Entrada: $ 1.25 /1m tokens SaÃ­da: $ 5.00 /1m tokens Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-2.5-flash Entrada: $ 0.30 /1m tokens SaÃ­da: $ 2.50 /1m tokens Google's best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @google/gemini-2.5-flash-lite Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @google/gemini-2.0-flash Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-2.0-flash-lite Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @google/gemini-1.5-flash-8b Entrada: $ 0.04 /1m tokens SaÃ­da: $ 0.08 /1m tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Entrada: aceita imagens, vÃ­deos, Ã¡udios Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON inception Nome do modelo PreÃ§os DescriÃ§Ã£o @inception/mercury Entrada: $ 0.25 /1m tokens SaÃ­da: $ 1.00 /1m tokens Extremely fast model by generative diffusion. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON metaai Nome do modelo PreÃ§os DescriÃ§Ã£o @metaai/llama-3.3-70b Entrada: $ 0.59 /1m tokens SaÃ­da: $ 0.79 /1m tokens Previous generation model with many parameters and surprisingly fast speed. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-4-maverick-17b-128e Entrada: $ 0.20 /1m tokens SaÃ­da: $ 0.60 /1m tokens Fast model, with 17 billion activated parameters and 128 experts. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-4-scout-17b-16e Entrada: $ 0.11 /1m tokens SaÃ­da: $ 0.34 /1m tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @metaai/llama-3.1-8b Entrada: $ 0.05 /1m tokens SaÃ­da: $ 0.08 /1m tokens Cheap and fast model for less demanding tasks. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON model-router Nome do modelo PreÃ§os DescriÃ§Ã£o @model-router/gemini Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.0 Flash, Gemini 2.5 Flash Lite and Gemini 2.5 Flash. @model-router/gemini-high Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for Google Gemini. The routing is made between Gemini 2.5 Flash Lite, Gemini 2.5 Flash (low) and Gemini 2.5 Flash (high). @model-router/openai Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 5 Nano, GPT 5 Mini and o4-mini. @model-router/openai-high Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for OpenAI. The routing is made between GPT 5 Mini, o4-mini (low) and o4-mini (high). @model-router/llama Entrada: $ 0.08 /1m tokens SaÃ­da: $ 0.30 /1m tokens Model router for Meta Llama. The routing is made between Llama 4 Scout, Llama 4 Maverick and Llama 3.3 70b. moonshotai Nome do modelo PreÃ§os DescriÃ§Ã£o @moonshotai/kimi-k2 Entrada: $ 1.00 /1m tokens SaÃ­da: $ 3.00 /1m tokens Model with 1tri total parameters, 32bi activated parameters, optimized for agentic intelligence. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON openai Nome do modelo PreÃ§os DescriÃ§Ã£o @openai/chatgpt-4o Entrada: $ 5.00 /1m tokens SaÃ­da: $ 15.00 /1m tokens The GPT-4o model snapshot used by ChatGPT. Entrada: aceita imagens Chamadas de funÃ§Ã£o @openai/gpt-4o Entrada: $ 2.50 /1m tokens SaÃ­da: $ 10.00 /1m tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-5 Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens OpenAI's newest flagship model for coding, reasoning, and agentic tasks across domains. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-5-chat Entrada: $ 1.25 /1m tokens SaÃ­da: $ 10.00 /1m tokens GPT-5 snapshot currently used by OpenAI's ChatGPT. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-4.1 Entrada: $ 2.00 /1m tokens SaÃ­da: $ 8.00 /1m tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/o3 Entrada: $ 2.00 /1m tokens SaÃ­da: $ 8.00 /1m tokens A well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/o4-mini Entrada: $ 1.10 /1m tokens SaÃ­da: $ 4.40 /1m tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/o3-mini Entrada: $ 1.10 /1m tokens SaÃ­da: $ 4.40 /1m tokens o3-mini provides high intelligence at the same cost and latency targets of previous versions of o-mini series. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-5-mini Entrada: $ 0.25 /1m tokens SaÃ­da: $ 2.00 /1m tokens GPT-5 mini is a faster, more cost-efficient version of GPT-5. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-4.1-mini Entrada: $ 0.40 /1m tokens SaÃ­da: $ 1.60 /1m tokens Fast and cheap for focused tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-oss-120b Entrada: $ 0.15 /1m tokens SaÃ­da: $ 0.75 /1m tokens OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with 120 billion parameters and 128 experts. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-4o-mini Entrada: $ 0.15 /1m tokens SaÃ­da: $ 0.60 /1m tokens Smaller version of 4o, optimized for everyday tasks. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-oss-20b Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.50 /1m tokens OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with 20 billion parameters and 128 experts. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @openai/gpt-4.1-nano Entrada: $ 0.10 /1m tokens SaÃ­da: $ 0.40 /1m tokens The fastest and cheapest GPT 4.1 model. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @openai/gpt-5-nano Entrada: $ 0.05 /1m tokens SaÃ­da: $ 0.40 /1m tokens OpenAI's fastest, cheapest version of GPT-5. Entrada: aceita imagens Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON qwen Nome do modelo PreÃ§os DescriÃ§Ã£o @qwen/qwen3-480b-coder Entrada: $ 2.00 /1m tokens SaÃ­da: $ 2.00 /1m tokens Lightning-fast AI model focused for coding tasks. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @qwen/qwen3-235b-a22b Entrada: $ 0.60 /1m tokens SaÃ­da: $ 1.20 /1m tokens This non-thinking version offers powerful multilingual capabilities with significant improvements in instruction following, logical reasoning, mathematics, coding, and tool usage. Chamadas de funÃ§Ã£o FunÃ§Ãµes JSON @qwen/qwen3-235b-a22b-think Entrada: $ 0.60 /1m tokens SaÃ­da: $ 1.20 /1m tokens The thinking variant of the Qwen3 235b model, with significantly improved performance of reasoning tasks. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON @qwen/qwen3-32b Entrada: $ 0.29 /1m tokens SaÃ­da: $ 0.59 /1m tokens Latest generation of LLMs in the Qwen series, offering advancements in reasoning, instruction-following, agent capabilities, and multilingual support. Chamadas de funÃ§Ã£o Pensamento profundo FunÃ§Ãµes JSON"
  },
  "docs/pricing.html": {
    "href": "docs/pricing.html",
    "title": "PrecificaÃ§Ã£o | AIVAX",
    "keywords": "PrecificaÃ§Ã£o O modelo de pagamento da AIVAX Ã© prÃ©-pago, ou seja, vocÃª usa nossos serviÃ§os com o saldo que adiciona em sua conta. NÃ£o enviamos faturas no comeÃ§o do mÃªs pelo seu uso. Dessa forma, fica previsÃ­vel saber quanto vocÃª irÃ¡ gastar usando nossos serviÃ§os de inferÃªncia e criaÃ§Ã£o de agentes. A AIVAX cobra uma pequena taxa (variÃ¡vel por mÃ©todo de pagamento) no momento da adiÃ§Ã£o de crÃ©ditos para encobrir impostos, taxas do provedor de pagamentos e nossa taxa de serviÃ§o. A precificaÃ§Ã£o dos modelos e de inferÃªncia Ã© fornecida diretamente pelos provedores de inferÃªncia e de seus modelos, como a Google e a OpenAI. NÃ£o hÃ¡ nenhuma taxa ou adiÃ§Ã£o em cima destes preÃ§os. VocÃª paga o mesmo valor que pagaria para esses provedores diretamente. Usamos diferentes serviÃ§os para ajudar vocÃª Ã  criar assistentes agÃªnticos. Algumas ferramentas e serviÃ§os possuem custo, e estes custos sÃ£o repassados para sua conta sem nenhuma taxa adicional. A inferÃªncia Ã© cobrada em dÃ³lares americanos (USD), portanto, pode existir flutuaÃ§Ã£o de moeda ao converter da sua moeda local para o dÃ³lar americano. Bring-your-own-key (BYOK) VocÃª pode trazer sua prÃ³pria chave de API compatÃ­vel com OpenAI para usar diretamente na AIVAX. Como nÃ£o sabemos qual modelo vocÃª estarÃ¡ usando, nÃ£o cobramos nada em cima da inferÃªncia que vocÃª usar em seus modelos. AlÃ©m disso, ao usar seu prÃ³prio modelo com a AIVAX, os limites de taxa sÃ£o aumentados para 1.500 requisiÃ§Ãµes por minuto, sem limitaÃ§Ã£o ao peso de tokens, que Ã© o equivalente Ã  60 requisiÃ§Ãµes por segundo. Note que, vocÃª ainda Ã© cobrado para serviÃ§os que usar com seus prÃ³prios modelos, como RAG, pesquisa na internet, geraÃ§Ã£o de imagens, etc. Se sua conta ficar com saldo negativo, vocÃª nÃ£o conseguirÃ¡ usar nenhum serviÃ§o, incluindo inferÃªncia para suas prÃ³prias API-keys, atÃ© que adicione saldo novamente. RAG (coleÃ§Ãµes) Atualmente, o modelo padrÃ£o usado para incorporaÃ§Ã£o de coleÃ§Ãµes Ã© o Gemini Embedding, o qual Ã© precificado em $ 0,15 para 1 milhÃ£o de tokens de entrada. Outros documentos podem ser vetorizados usando outros modelos de incorporaÃ§Ã£o depreciados no sistema, mas ativos por compatibilidade: @google/gemini-embedding-001, $ 0,15 por milhÃ£o de tokens. (padrÃ£o) @google/text-embedding-004, $ 0,10 por milhÃ£o de tokens. (depreciado) @baai/bge-m3, $0,012 por milhÃ£o de tokens. (depreciado) No momento, nÃ£o cobramos taxa de computaÃ§Ã£o e/ou armazenamento de vetores. Para a cobranÃ§a ocorrer, precisamos calcular quantos tokens foram processados da entrada, e nem todos os provedores de incorporaÃ§Ã£o retornam a quantia de tokens indexados. Portanto, usamos uma aproximaÃ§Ã£o para calcular a quantia de tokens processados: tokens = ceil(utf8_bytes_count / 4) O resultado dessa aproximaÃ§Ã£o Ã© o que cobramos de vocÃª. Ferramentas Ferramentas fornecidas pela AIVAX (ferramentas embutidas) possuem precificaÃ§Ãµes e limites distintos de uma para outra. Para funÃ§Ãµes que vocÃª define para sua API, nÃ£o hÃ¡ nenhuma cobranÃ§a."
  },
  "docs/protocol-functions.html": {
    "href": "docs/protocol-functions.html",
    "title": "FunÃ§Ãµes do lado do servidor | AIVAX",
    "keywords": "FunÃ§Ãµes do lado do servidor As funÃ§Ãµes de protocolo da AIVAX, ou server-side functions, Ã© uma implementaÃ§Ã£o em que a chamada de ferramentas do modelo ocorre do lado do servidor. Similar ao MCP, mas com suporte nativo Ã  autenticaÃ§Ã£o e otimizado para funcionar externamente. As funÃ§Ãµes de protocolo permitem a tomada de aÃ§Ãµes no lado do servidor da AIVAX, removendo a necessidade de implementaÃ§Ã£o da funÃ§Ã£o no lado do cliente e integrando com aplicaÃ§Ãµes e serviÃ§os existentes. Essas funÃ§Ãµes esperam um callback atravÃ©s de uma URL, e quando o modelo decide chamar a funÃ§Ã£o, o callback Ã© acessado com os parÃ¢metros informados pela prÃ³pria assistente. A assistente nÃ£o sabe qual URL ela estÃ¡ chamando, pois a mesma permanece invisÃ­vel tanto para a assistente quanto para o usuÃ¡rio. Escolhendo o nome da funÃ§Ã£o O nome da funÃ§Ã£o deve ser simples e determinÃ­stico ao que essa funÃ§Ã£o faz. Evite nomes difÃ­ceis de advinhar ou que nÃ£o remetam ao papel da funÃ§Ã£o, pois a assistente pode se confundir e nÃ£o chamar a funÃ§Ã£o quando apropriado. Como um exemplo, vamos pensar em uma funÃ§Ã£o de consultar um usuÃ¡rio em um banco de dados externo. Os nomes a seguir sÃ£o bons exemplos para considerar para a chamada: search_user query_user Nomes ruins incluem: search (pouco abrangente) query-user-in-database-data (nome muito grande) pesquisa-usuario (nome nÃ£o em inglÃªs) search user (nome com caracteres imprÃ³prios) Tendo o nome da funÃ§Ã£o, podemos pensar na descriÃ§Ã£o da funÃ§Ã£o. Escolhendo a descriÃ§Ã£o da funÃ§Ã£o. A descriÃ§Ã£o da funÃ§Ã£o deve explicar conceitualmente duas situaÃ§Ãµes: o que ela faz e quando deve ser chamada pela assistente. Essa descriÃ§Ã£o deve incluir os cenÃ¡rios que a assistente deve considerar chamar ela e quando nÃ£o deve ser chamada, fornecendo poucos exemplos de chamadas (one-shot) e/ou tornando explÃ­citas as regras da funÃ§Ã£o. Definindo funÃ§Ãµes de protocolo Essas funÃ§Ãµes sÃ£o definidas no AI-gateway, o que permite a criaÃ§Ã£o de agentes inteligentes que realizam aÃ§Ãµes sem intervenÃ§Ã£o humana. Elas seguem uma sintaxe simples, esperam o nome da funÃ§Ã£o, uma descriÃ§Ã£o do que ela faz e os parÃ¢metros de invocaÃ§Ã£o. FunÃ§Ãµes de protocolo sÃ£o definidas no AI gateway seguindo o JSON: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctions\": [ { \"name\": \"list_clients\", \"description\": \"Use essa ferramenta para listar e procurar pelos clientes do usuÃ¡rio.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view_client\", \"description\": \"Use essa ferramenta para obter detalhes e pedidos de um cliente atravÃ©s do seu ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"format\": \"uuid\" } }, \"required\": [\"user_id\"] } } ] } } No snippet acima, vocÃª estÃ¡ fornecendo duas funÃ§Ãµes para seu modelo de IA: list_clients e view_client, o qual irÃ¡ decidir qual serÃ¡ executada durante o seu raciocÃ­nio. VocÃª pode fornecer tambÃ©m um formato de conteÃºdo JSON para qual o modelo irÃ¡ chamar sua API fornecendo o contÃ©udo informado. VocÃª tambÃ©m pode definir as lista de funÃ§Ãµes suportadas atravÃ©s de um endpoint. Toda vez que o modelo receber uma mensagem, ele irÃ¡ consultar o endpoint fornecido para obter uma lista de funÃ§Ãµes que ele possa executar. Defina os endpoints de listagem de funÃ§Ãµes no seu AI gateway: POST /api/v1/ai-gateways { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctionSources\": [ \"https://my-external-api.com/api/scp/listings\" ] } } Os endpoint de fornecimento de funÃ§Ãµes deve responder seguindo o formato: GET https://my-external-api.com/api/scp/listings { \"functions\": [ { \"name\": \"list_clients\", \"description\": \"Use essa ferramenta para listar e procurar pelos clientes do usuÃ¡rio.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": null }, { \"name\": \"view_client\", \"description\": \"Use essa ferramenta para obter detalhes e pedidos de um cliente atravÃ©s do seu ID.\", \"callbackUrl\": \"https://my-external-api.com/api/scp/users\", \"contentFormat\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"format\": \"uuid\" } }, \"required\": [\"user_id\"] } } ] } Essas funÃ§Ãµes sÃ£o armazenadas em cache por 10 minutos antes de uma nova requisiÃ§Ã£o ser feita no endpoint fornecido. Lidando com chamada de funÃ§Ãµes As funÃ§Ãµes sÃ£o invocadas no endpoint fornecido em callbackUrl atravÃ©s de uma requisiÃ§Ã£o HTTP POST, com o corpo: { \"function\": { \"name\": \"view_client\", \"content\": { \"user_id\": \"3e5a2823-98fa-49a1-831a-0c4c5d33450e\" } }, \"context\": { \"externalUserId\": \"...\", \"moment\": \"2025-05-18T03:36:27\" } } A resposta dessa aÃ§Ã£o deve responder sempre com um status HTTP OK (2xx ou 3xx), atÃ© mesmo para erros que a assistente possa ter cometido. Uma resposta nÃ£o OK irÃ¡ indicar para a assistente que nÃ£o foi possÃ­vel chamar a funÃ§Ã£o e ela nÃ£o irÃ¡ continuar com o que estava planejando fazer. Formato das respostas As respostas bem sucedidas devem ser textuais e serÃ£o anexadas como resposta da funÃ§Ã£o do jeito que for respondida pelo endpoint. NÃ£o hÃ¡ formato JSON ou estrutura para essa resposta, mas Ã© aconselhÃ¡vel que dÃª uma resposta simples, humanamente legÃ­vel, para que a assistente consiga ler o resultado da aÃ§Ã£o. Erros podem ser comuns, como nÃ£o encontrar um cliente pelo ID ou algum campo nÃ£o estiver no formato desejado. Nestes casos, responda com um status OK e no corpo da resposta inclua uma descriÃ§Ã£o humana do erro e como a assistente pode contornar ele. Ã‰ garantido que a requisiÃ§Ã£o irÃ¡ seguir estritamente o JSON Schema do conteÃºdo fornecido pela definiÃ§Ã£o da funÃ§Ã£o. FunÃ§Ãµes que nÃ£o esperam argumentos nÃ£o devem especificar um formato de conteÃºdo para essa funÃ§Ã£o. Important Quanto mais funÃ§Ãµes vocÃª definir, mais de entrada tokens vocÃª irÃ¡ consumir no processo de raciocÃ­nio. A definiÃ§Ã£o da funÃ§Ã£o, bem como o formato dela, consome tokens do processo de raciocÃ­nio. AutenticaÃ§Ã£o A autenticaÃ§Ã£o das requisiÃ§Ãµes sÃ£o feitas pelo cabeÃ§alho X-Aivax-Nonce enviado em todas as requisiÃ§Ãµes de protocolo das funÃ§Ãµes, atÃ© mesmo as de listagem. Veja o manual de autenticaÃ§Ã£o para entender como autenticar requisiÃ§Ãµes reversas do AIVAX. AutenticaÃ§Ã£o de usuÃ¡rio As chamadas de funÃ§Ã£o enviam um campo $.context.externalUserId contendo a tag de usuÃ¡rio criada em uma sessÃ£o de chat. Essa tag pode ser usada para autenticar o usuÃ¡rio que chamou essa funÃ§Ã£o. ConsideraÃ§Ãµes de seguranÃ§a Para o modelo de IA, somente Ã© visÃ­vel o nome, descriÃ§Ã£o e formato da funÃ§Ã£o. Ela nÃ£o Ã© capaz de ver o endpoint para onde essa funÃ§Ã£o aponta. AlÃ©m disso, ela nÃ£o possui acesso Ã  tag do usuÃ¡rio que estÃ¡ autenticado em um cliente de chat. FunÃ§Ãµes especialistas AlÃ©m das funÃ§Ãµes embutidas, vocÃª pode definir funÃ§Ãµes especialistas, que executam tarefas especÃ­ficas na sua conta da AIVAX. VocÃª define funÃ§Ãµes especialistas pelo esquema de URL aivax://, seguindo o exemplo abaixo: { \"name\": \"my-ai-gateway\", \"parameters\": { ... \"protocolFunctions\": [ { \"name\": \"search_disease\", \"description\": \"Use essa ferramenta para pesquisar por doenÃ§as, tratamentos e sintomas.\", \"callbackUrl\": \"aivax://query-collection?collection-id=0196f5ef-9334-742b-a988-f913bb3be5ba&top=5&min=0.4\", \"contentFormat\": { \"type\": \"object\", \"properties\": { \"query\": { \"type\": \"string\", \"description\": \"Nome da doenÃ§a, tratamento ou sintomas.\" } }, \"required\": [ \"query\" ] } } ] } } A funÃ§Ã£o acima cria uma ferramenta para IA consultar em uma coleÃ§Ã£o de documentos especÃ­fica, guiando a assistente do que ela deve pesquisar nessa coleÃ§Ã£o e o que esperar de uma resposta. Dessa forma, vocÃª pode vincular vÃ¡rias coleÃ§Ãµes de RAG para uma assistente poder buscar conteÃºdo especialista. VocÃª pode personalizar a descriÃ§Ã£o das propriedades do JSON Schema de funÃ§Ãµes especialistas mas nÃ£o sua estrutura, pois nosso backend espera um formato especÃ­fico para chamar as funÃ§Ãµes. Os parÃ¢metros de funÃ§Ãµes especialistas sÃ£o fornecidos na URL atravÃ©s de parÃ¢metros da query. Atualmente, apenas uma funÃ§Ã£o especialista existe: query-collection: executa uma pesquisa RAG em uma coleÃ§Ã£o informada. ParÃ¢metros da query: collection-id: o UUID da coleÃ§Ã£o que serÃ¡ pesquisada. top: um nÃºmero indicando quantos documentos devem ser retornados na pesquisa. min: um decimal indicando qual a pontuaÃ§Ã£o mÃ­nima de similaridade da busca. Formato JSON da funÃ§Ã£o: { \"type\": \"object\", \"properties\": { \"query\": { \"type\": \"string\", \"description\": \"ConteÃºdo da pesquisa.\" } }, \"required\": [ \"query\" ] }"
  },
  "index.html": {
    "href": "index.html",
    "title": "Bem-vindo! | AIVAX",
    "keywords": "Bem-vindo! Bem-vindo Ã  documentaÃ§Ã£o da AIVAX."
  },
  "readme.html": {
    "href": "readme.html",
    "title": "Sisk Documentation | AIVAX",
    "keywords": "Sisk Documentation This repository contains the source code of the Sisk Documentation website. Building Firstly, make sure you have docfx installed in your machine. You'll need .NET SDK to install it. Clone this repository. Build the Sisk Framework project and put the .DLL binaries and XML documentation file at the ref/ directory, on the repository root. Run docfx, then docfx serve. Warning Please, do not use the docfx version 2.78.0 or later. This version has a bug that changes the documentation navigation layout. See the tracking issue. Prefer the version 2.76.0: dotnet tool install -g docfx --version 2.76.0 Then you're ready to go and you'll have the static website files at /_site. Contributing Contributions are always welcome. Contribute with spelling corrections, fixing broken links and more. Please, only edit english documentation files. Documentation files for another languages are AI-generated from english files through. Note Please do not edit API specification files (XML). These files are generated. If you want to edit any API documentation, edit it in the repository where the code is hosted."
  }
}