{
  "docs/authentication.html": {
    "href": "docs/authentication.html",
    "title": "Autenticação | AIVAX",
    "keywords": "Autenticação Quando tiver sua conta em mãos, use sua chave de autenticação única para se autenticar na nossa API através do cabeçalho Authorization: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' Você também pode enviar o seu token de autorização pelo parâmetro da query ?api-key, exemplo: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... Não há necessidade de enviar o esquema de autenticação Bearer em ambos cabeçalhos, mas é possível por questões de compatibilidade. Obtendo a API Key a partir da Login Key Sua Login Key é a chave que você usa para acessar seu painel de usuário e controlar as diversas funções da AIVAX. A partir dessa chave é possível recuperar sua API key: Requisição GET /api/v1/auth/login { \"loginKey\": \"n38oy4nq2orry7\" } Resposta { \"message\": null, \"data\": { \"apiKey\": \"oky_gr5uepj18yhdec3z6nskw3w1kqfbaawcfrwe837c8o\", \"accountName\": \"Fernando Diniz\" } }"
  },
  "docs/en/authentication.html": {
    "href": "docs/en/authentication.html",
    "title": "Authentication | AIVAX",
    "keywords": "Authentication When you have your account in hand, use your unique authentication key to authenticate on our API through the Authorization header: curl https://inference.aivax.net/api/v1/information/models.txt \\ -H 'Authorization: Bearer oky_gr5uepj...' You can also send your authorization token through the query parameter ?api-key, example: curl https://inference.aivax.net/api/v1/information/models.txt?api-key=oky_gr5uepj... There is no need to send the authentication scheme Bearer in both headers, but it is possible for compatibility reasons. Obtaining the API Key from the Login Key Your Login Key is the key you use to access your user panel and control the various functions of AIVAX. From this key, it is possible to recover your API key: Request GET /api/v1/auth/login { \"loginKey\": \"n38oy4nq2orry7\" } Response { \"message\": null, \"data\": { \"apiKey\": \"oky_gr5uepj18yhdec3z6nskw3w1kqfbaawcfrwe837c8o\", \"accountName\": \"Fernando Diniz\" } }"
  },
  "docs/en/entities/ai-gateway.html": {
    "href": "docs/en/entities/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway The AI gateways are a service provided by the Open Indexer to create an inference tunnel between an LLM model and a knowledge base. It is possible to: Create a model with customized instructions Use a model provided by you through an OpenAI compatible endpoint, or use a model made available by the Open Indexer Customize inference parameters, such as temperature, top_p, prefill Use a knowledge collection as the foundation for AI responses Among other features. With the AI Gateway, you create a ready-to-use model, parameterized and based on the instructions you define. Models You can bring an AI model compatible with the OpenAI interface to the AI gateway. If you bring your AI model, we will only charge for the document search attached to the AI. You can also use one of the models below that are already ready to start with the Open Indexer. When using a model, you will notice that some are more intelligent than others for certain tasks. Some models are better with certain data acquisition strategies than others. Perform tests to find the best model. You can see the available models on the models page. Choosing a search strategy If you are using a knowledge collection with an AI model, you can choose a strategy that the AI will use to perform an information search. Each strategy is more refined than the other. Some create better results than others, but it is essential to perform practical tests with several strategies to understand which one fits best in the model, conversation, and user tone. It may be necessary to make adjustments to the system prompt to better inform the AI how to consider the documents attached to the conversation. The documents are attached as a user message, limited to the parameters you define in the acquisition strategy. Rewrite strategies usually generate the best results at a low latency and cost. The rewrite model used always has the lowest cost, usually chosen by an internal pool that decides which model has the lowest latency at the moment. Strategies without rewrite cost: Plain: the default strategy. It is the least optimized and has no rewrite cost: the last user message is used as a search term to search the attached collection of the gateway. Concatenate: concatenates the last N user messages into lines, and then the result of the concatenation is used as a search term. Strategies with rewrite cost (inference tokens are charged): UserRewrite: rewrites the last N user messages using a smaller model, creating a contextualized question about what the user means. FullRewrite: rewrites the last N*2 chat messages using a smaller model. Similar to UserRewrite, but also considers the assistant's messages in formulating the new question. It usually creates the best questions, with a slightly higher cost. It is the most stable and consistent strategy. It works with any model. Function strategies: QueryFunction: provides a search function in the integrated collection for the AI model. You should adjust the system instructions to define the ideal scenarios for the model to call this function when necessary. It may not work as well with smaller models. Using AI functions (tools) At the moment, it is not possible to specify function calls through our API, either through the AI-Gateway or the OpenAI compatible API. This feature is on our radar for future implementation. If this is critical for your AI model to work, you can use the document search API in your model. Creating an AI gateway If you are using a model provided by you, have the following in hand: The base address compatible with the OpenAI API The API key of the endpoint (if applicable) The name of the inference model. It is not necessary to have a collection to link to your AI gateway. You can create an AI gateway and link a knowledge base to it later. Request POST /api/v1/ai-gateways { // Name of the gateway to identify it later. \"name\": \"my-gateway-model\", \"parameters\": { // Required. Endpoint compatible with OpenAI chat/completions, or use @integrated // to use a model provided by the Open Indexer. \"baseAddress\": \"@integrated\", // Required. Specifies the name of the model that will be used in inference. \"modelName\": \"@groq/compound-beta\", // Optional. ID of the collection that will be used as the knowledge base by the AI. \"knowledgeCollectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\", // Optional. Specifies how many documents should be attached to the AI context. \"knowledgeBaseMaximumResults\": 16, // Optional. Specifies the minimum similarity score that the document search should attach to the AI context. \"knowledgeBaseMinimumScore\": 0.55, // Optional. Specifies whether document references should be attached to the AI context. \"knowledgeUseReferences\": false, // Optional. Specifies the document acquisition strategy. Read \"Choosing a search strategy\" to learn more. \"queryStrategy\": \"UserRewrite\", // Optional. Parameters of the acquisition strategy. \"queryStrategyParameters\": { // Optional. Specifies the number of messages that should be considered for the UserRewrite and FullRewrite strategies. Note: for FullRewrite, the value is always multiplied by 2 to consider assistant messages. \"rewriteContextSize\": 3, // Optional. Specifies the number of user messages that should be concatenated into the search term. \"concatenateContextSize\": 3 } // Optional. Specifies the API key \"Authorization: Bearer ...\" used in inference. Leave null if using an embedded AIVAX model. \"apiKey\": null, // Optional. Specifies the model temperature. \"temperature\": 1.25, // Optional. Specifies the model's nucleos sampling. \"topP\": null, // Optional. Specifies the presence penalty of model tokens. \"presencePenalty\": null, // Optional. Specifies a \"stop\" term for the model. \"stop\": null, // Optional. Specifies the maximum number of response tokens of the model. \"maxCompletionTokens\": 4096, // Optional. Specifies the system prompt used in the model. \"systemInstruction\": \"You are a helpful assistant.\", // Optional. Transforms the user's question into the indicated format, where \"{prompt}\" is the user's original prompt. \"userPromptTemplate\": null, // Optional. Specifies a prefill (initialization) of the assistant's message. \"assistantPrefill\": null, // Optional. Specifies whether the assistantPrefill and stop should be included in the message generated by the assistant. \"includePrefillingInMessages\": false, // Optional. Specifies special flags for the model. Leave as \"0\" to not use any flag. The allowed flags are: // NoSystemInstruct: instead of using system prompt, inserts system instructions into a user message \"flags\": \"0\", // Optional. Passes an array of functions to the AI. \"tools\": [], // Optional. Activates protocol functions for Sentinel models. Read \"Protocol Functions\" to learn more. \"protocolFunctions\": [ { \"name\": \"get-weather\", \"description\": \"Use this function to get weather data for the informed location.\", \"callbackUrl\": \"https://my-service.com/ai-service\", \"contentFormat\": { \"location\": \"city name\" } } ] } } Response { \"message\": null, \"data\": { \"aiGatewayId\": \"01965b64-a8eb-716c-892d-880159a9f12d\" } } Editing an AI gateway The request body is basically the same as the create AI gateway endpoint. Instead of using POST, use PATCH. Request PATCH /api/v1/ai-gateways/{ai-gateway-id} Response { \"message\": \"Gateway edited.\", \"data\": null } Using an AI gateway The endpoint for conversing with an AI gateway is simple: it only expects the conversation. You can receive the response at once or by streaming. Request POST /api/v1/ai-gateways/{ai-gateway-id}/inference { \"messages\": [ { \"role\": \"user\", \"content\": \"How can I turn on my Civic 2015?\" } ], \"stream\": true } Response for stream=true The streaming response is based on server-sent events. The first line is always a response with debugging information. data: {\"content\":\"\",\"isFirstChunkMetadata\":true,\"embeddedDocuments\":[],\"debugInfo\":[{\"name\":\"EmbeddingTimeMs\",\"value\":7.5045},{\"name\":\"InferenceTimeMs\",\"value\":0},{\"name\":\"ElapsedTotalMs\",\"value\":8.3489},{\"name\":\"KnowledgeQueryText\",\"value\":null}]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} ... data: [END] Response for stream=false { \"message\": null, \"data\": { \"generatedMessage\": \"[...]\", \"embeddedDocuments\": [], \"debugInfo\": [ { \"name\": \"EmbeddingTimeMs\", \"value\": 4140.8628 }, { \"name\": \"InferenceTimeMs\", \"value\": 4140.803 }, { \"name\": \"ElapsedTotalMs\", \"value\": 4141.4771 }, { \"name\": \"KnowledgeQueryText\", \"value\": null } ] } } Viewing an AI gateway The request below brings details of an AI gateway. Request GET /api/v1/ai-gateways/{ai-gateway-id} Response { \"message\": null, \"data\": { \"name\": \"my-gateway-client\", \"parameters\": { \"baseAddress\": \"@integrated\", \"knowledgeCollectionId\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"knowledgeBaseMaximumResults\": 16, \"knowledgeBaseMinimumScore\": 0.55, \"knowledgeUseReferences\": false, \"queryStrategy\": \"ToolCall\", \"queryStrategyParameters\": { \"rewriteContextSize\": 3, \"concatenateContextSize\": 3 }, \"apiKey\": null, \"modelName\": \"@google/gemini-2.0-flash-lite\", \"temperature\": 1.25, \"topP\": null, \"presencePenalty\": null, \"stop\": null, \"maxCompletionTokens\": 4096, \"systemInstruction\": \"[...]\", \"userPromptTemplate\": null, \"assistantPrefill\": null, \"includePrefillingInMessages\": false, \"flags\": \"0\", \"tools\": null } } } Deleting an AI gateway Permanently removes an AI gateway. Warning When removing an AI gateway, all associated chat clients are also removed. Collections are not removed. Request DELETE /api/v1/ai-gateways/{ai-gateway-id} Response { \"message\": \"AI gateway deleted.\", \"data\": null } OpenAI endpoint The Open Indexer provides an endpoint compatible with the OpenAI interface through an AI gateway, which facilitates the integration of the model created by the Open Indexer with existing applications. It is worth noting that only some properties are supported. In an AI gateway, you already configure the model parameters, such as System Prompt, temperature, and model name. When using this endpoint, some gateway values can be overwritten by the request. Request POST /api/v1/ai-gateways/{ai-gateway-id}/open-ai/v1/chat/completions { // The \"model\" field is required, but it does nothing in this request. It only exists to be compatible with the Open AI API. You can leave it empty or write anything in its place, as the considered model is the one defined in the AI Gateway. \"model\": \"foobar\", // The messages must follow the Open AI format. Only \"system\" and \"user\" are supported as conversation \"roles\". \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"Hello!\" } ], // Both properties are equivalent and optional and will replace the maxCompletionTokens field if sent in the request. \"max_completion_tokens\": 1024, \"max_tokens\": 1024, // Optional. Replaces the gateway parameter. \"stop\": \"\\n\", // Optional. By default, the response is not streaming. \"stream\": true } Response for non-streaming { \"id\": \"019672f3-699c-7d45-8484-7a23f4cdc079\", \"object\": \"chat.completion\", \"created\": 1745685277, \"model\": \"gemini-2.0-flash-lite\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\\n\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0 }, \"service_tier\": \"default\" } Response for streaming data: {\"id\":\"019672f4-9a58-7932-82f0-022e457a2e63\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_2i0nmn\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"Hi\"}}]} data: {\"id\":\"019672f4-9ab9-73a2-bdb8-23c4481453a8\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_ar1qol\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" there! How can I help you today?\\n\"}}]} ... data: {\"id\":\"019672f4-9ac0-7ddf-a76a-e7f8043dd082\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_3e84ge\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}]}"
  },
  "docs/en/entities/chat-clients.html": {
    "href": "docs/en/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients A chat client provides a user interface through an AI Gateway that allows the user to converse with their assistant. A chat client is integrated with the AI gateway's inference and supports deep thinking, research, and text conversation. Multi-modal features, such as sending images and audio, are under development. Note The Open Indexer never stores the content of a chat between a client and the user. You can use JavaScript for this task, but it is your responsibility to use and store it. You can customize the interface of your chat client with custom CSS and JavaScript, as well as choose the language of the chat features. Creating a Chat Client Create a new chat client. POST /api/v1/web-chat-client/ { // Specifies the public name of your chat client \"name\": \"My Assistant\", // Specifies the ID of the AI gateway to be used by the chat \"aiGatewayId\": \"01965b64-a8eb-716c-892d-880159a9f12d\", \"clientParameters\": { // Optional. Specifies the language code to be used in the chat for most elements, such as error messages, buttons, etc. // Values: pt-BR, en \"languageCode\": \"pt-BR\" | \"en\", // Optional. Specifies a JavaScript code to execute in the chat. \"customScripts\": null, // Optional. Specifies a CSS code to apply custom styles to the chat. \"customStyles\": null, // Optional. Specifies the highlight color of the chat client elements. \"primaryColor\": \"#eabe44\", // Optional. Specifies the title of the chat page. \"pageTitle\": \"Assistant\", // Optional. Specifies the title when entering the chat for the first time. \"helloLabel\": \"It's great to see you here.\", // Optional. Specifies the subtitle when entering the chat for the first time. \"helloSubLabel\": \"I'm your assistant.\", // Optional. Specifies the placeholder of the message sending field. \"textAreaPlaceholder\": \"Talk to the assistant\", // Optional. Specifies an image/logo to display in the chat for the first time. \"logoImageUrl\": null, // Optional. Enables debugging features. \"debug\": true, // Optional. Specifies whether the chat supports multi-modal media processing, specifying which buttons will be visible to send multimedia content to the model. // Document is processed internally as markdown by the Open Indexer. \"inputModes\": [\"Image\", \"Audio\", \"Document\"], // Optional. Specifies which origins should be allowed to embed the chat client in an iframe. If this field is empty, any origin will be accepted. \"allowedFrameOrigins\": [\"https://my-domain.com.br\"], // Optional. Specifies conversation suggestion buttons when starting a new chat session. You can add as many buttons as you want, but it is recommended to have up to 3 buttons. \"suggestionButtons\": [ { // Title to be displayed on the button. \"label\": \"How to buy a car?\", // Prompt to be sent to the model. \"prompt\": \"Where and how can I buy a car in your store?\" }, ... ] }, \"limitingParameters\": { // Optional. Specifies how many messages the user can send per hour in the chat. This option is tracked by the userTag of the session. \"messagesPerHour\": 30, // Optional. Specifies the limit of messages (for the user and AI) that a session can have. \"maxMessages\": 300 } } Response { \"message\": null, \"data\": { \"id\": \"01965b65-e95e-7795-848c-ff0919ef1436\" } } Editing a Chat Client The body of this request is exactly the same as creating a chat client. PUT /api/v1/web-chat-client/{chat-client-id} Response { \"message\": \"Web client updated successfully.\", \"data\": null } Listing Chat Clients Get a list of created chat clients. GET /api/v1/web-chat-client/ Response { \"message\": null, \"data\": [ { \"id\": \"01965b59-daf6-7809-94c8-2a65b7264dba\", \"name\": \"My Chat Client\" }, ... ] } Viewing a Specific Chat Client Get details of an existing chat client. GET /api/v1/web-chat-client/{chat-client-id} Response { \"message\": null, \"data\": { \"name\": \"My Chat Client\", \"aiGateway\": { \"id\": \"01965b59-49ff-7753-8327-b3b6a6a871f2\", \"name\": \"gateway-t1\", \"knowledgeCollection\": { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"name\": \"Car Information\" } }, \"limitingParameters\": { \"messagesPerHour\": 30, \"userInputMaxTokens\": 1024, \"maxMessages\": 500 }, \"clientParameters\": { \"languageCode\": \"pt-BR\", \"customScripts\": null, \"customStyles\": null, \"primaryColor\": \"#f011d2\", \"pageTitle\": \"Lyra\", \"helloLabel\": \"It's great to see you here.\", \"helloSubLabel\": \"I'm your assistant.\", \"textAreaPlaceholder\": \"Talk to the assistant\", \"logoImageUrl\": null, \"debug\": true, \"allowedFrameOrigins\": [] } } } Creating a Chat Session A chat session is where you create a conversation between your chat client and the user. You can call this endpoint providing additional context for the conversation, such as the user's name, location, etc. A chat session expires after a certain time for security reasons of the generated access token. When you call this endpoint providing a tag, you can call the same endpoint multiple times and get the active chat session for the informed tag, or create a new chat if there is no ongoing session. A chat session also restores all conversation messages from the same session after disconnection. The user can clear the conversation by clicking the clear conversation button in the top right corner of the chat client. This session uses the limits defined by the chat client, such as the maximum number of messages and tokens in the conversation. A session is automatically renewed for another 3 days when receiving a message from the user. Important It is only possible to determine the number of tokens used in a message when using a model provided by the Open Indexer. If you use an external model, the limitingParameters.userInputMaxTokens property will be ignored. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Optional. Additional context for the AI about the chat. \"extraContext\": \"# Additional context\\r\\n\\r\\nYou are talking to Eduardo.\", // Time in seconds for the chat to expire. The minimum is 10 minutes. The maximum is 30 days. \"expires\": 3600, // Optional (recommended). An external ID to identify the session later and reuse it whenever you call the same endpoint. It can be the ID of the user in your database or a string that facilitates the identification of this chat later. \"tag\": \"my-user-tag\" } Response { \"message\": null, \"data\": { // ID of the created chat session. \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Public access key of the chat. \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // Public URL to converse with the chat. \"talkUrl\": \"https://preview-s01.proj.pw/www/web-chat-clients/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } }"
  },
  "docs/en/entities/collections.html": {
    "href": "docs/en/entities/collections.html",
    "title": "Collections | AIVAX",
    "keywords": "Collections A collection is a knowledge library: it houses several knowledge documents. Use collections to group documents by purpose, such as documenting a product, a company, a service or flow. Collections do not incur costs. There is no limit to the number of collections per account. Create a collection To create an empty collection, simply provide its name: Request POST /api/v1/collections { // The collection name cannot be empty. \"collectionName\": \"My first collection\" } Response { \"message\": null, \"data\": { // Unique ID of the created collection. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } List collections Lists the collections available in your account. Request GET /api/v1/collections Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"My collection\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Another collection\" } ] } } View a collection Obtains details of a collection, such as its indexing progress and information like creation date. Request GET /api/v1/collections/{collection-id}/ Response { \"message\": null, \"data\": { \"name\": \"My collection\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // Returns the number of documents waiting for indexing \"queuedDocuments\": 0, // Number of documents ready for query \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Delete a collection Deletes a collection and all documents within it. This action is irreversible. Request DELETE /api/v1/collections/{collection-id}/ Response { \"message\": \"Collection deleted successfully.\", \"data\": null } Clear a collection Unlike collection deletion, this operation removes all documents from the collection, including indexed and queued ones. Request DELETE /api/v1/collections/{collection-id}/reset-only Response { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/en/entities/documents.html": {
    "href": "docs/en/entities/documents.html",
    "title": "Documents | AIVAX",
    "keywords": "Documents A document represents a piece of knowledge. It is a limited, self-sufficient, and meaningful piece of text on its own. A document is the component that is indexed by the internal model to be retrieved later through a semantic search term. Consider a car manual: it is not a document, but rather several documents. Each of these documents talks, in isolation, about a specific topic related to that car, in such a way that the document does not depend on external context or information to make sense. Each document in this manual will talk about a topic: one will talk about how to turn on the car, another about how to turn it off, another about how its paint is made, and another about how to change the oil periodically. It is not a good idea to reserve a document to talk about several things at the same time, as this will reduce the objectivity and scope of the inference and reduce the quality of acquisition. Examples of document creation: Do not do Do not create very short documents (with 10 or fewer words). Do not create very large documents (with 700 or more words). Do not talk about more than one thing in a document. Do not mix different languages in documents. Do not be implicit in documents. Do not write documents using technical language, such as codes or structures like JSON. Do Be explicit about the purpose of your document. Focus documents on individual topics, summarizing what should be done or explained. Always repeat terms that are keywords for the document search. Example: prefer to use \"The color of the Honda Civic 2015 is yellow\" instead of \"the color of the car is yellow\". Restrict the document content to talk about only one topic or subject. Use simple and easy-to-understand human language. API Usage As all documents are entities that belong to a collection, always have the collection where the document is/will be located at hand. Sending documents in bulk To send a large list of documents to a collection, structure them following the JSONL format. The indexing file structure is: {\"docid\":\"Cars/HondaCivic2015.rmd:1\",\"text\":\"The Honda Civic 2015 is available in [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:2\",\"text\":\"The engine of the Honda Civic 2015 is [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} {\"docid\":\"Cars/HondaCivic2015.rmd:3\",\"text\":\"The color of the Honda Civic 2015 is Yellow [...]\",\"__ref\":null,\"__tags\":[\"Cars\",\"Honda-Civic-2015\"]} ... The structure consists of the following properties: Property Type Description docid string Specifies the document name. Useful for debugging and identification. text string The \"raw\" content of the document that will be indexed. __ref string Optional. Specifies a reference ID of the document. __tags string[] Optional. Specifies an array of document tags. Useful for document management. A document reference is an ID that can be specified in several documents that need to be linked in a search when one of them is matched in a similarity search. For example, if a search finds a document that has a reference ID, all other documents in the same collection that share the same reference ID as the matched document will also be included in the search response. The use of references can be useful when a document depends on another or more documents to make sense. There is no format requirement for the reference ID: any format is accepted. You can send up to 1,000 lines of documents per request. If you need to send more documents, separate the sending into more requests. If you send a document with more than 1,000 lines, the following lines will be ignored. Note that very long documents, which exceed the allowed number of tokens in the internal embedding model, will have their content truncated and the indexing quality may be severely affected. To avoid this problem, send documents that contain between 20 and 700 words. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of each document. The content of each document is tokenized according to the model used in the indexing of the documents. Request The sending must be done using multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Response { \"message\": null, \"data\": [ { \"name\": \"Institutional/Company.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institutional/Company.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institutional/Company.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Products/Schedules.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Create or modify document This endpoint creates or modifies a document from its name. When a document is modified, its indexing vectors are reset, that is, the document will enter the queue again to be indexed by the indexing engine. This indexing is not exempt from cost. The cost is relative to the number of tokens of the sent content. The cost is only generated when the document is actually changed. Calling this route with the same content as the document does not generate modification, therefore, it does not generate cost. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the content of the file. The content of the file is tokenized according to the model used in the indexing of the documents. Request PUT /api/v1/collections/{collection-id}/documents { // the name of the document that will be modified \"name\": \"document-name\", // the content of the document that will be created or overwritten if the name already exists \"contents\": \"Content of my document\", // parameters explained earlier \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Response { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // the state of the operation indicates whether the document was modified \"Modified\" or created \"Created\". \"state\": \"Modified\" } } List documents This endpoint lists all available documents in a collection. You can pass an additional query parameter filter to filter documents by name, tag, or content. This filter supports expressions that help filter what you are looking for: -t \"tag\" - filters documents that have this tag. -r \"reference\" - filters documents that have this reference ID. -c \"content\" - filters documents that have this snippet in their content. -n \"name\" - filters documents that have this snippet in their name. Request GET /api/v1/collections/{collection-id}/documents Response { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Help/Customers.rmd:1\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"A customer is a registration on your platform...\", \"indexState\": \"Indexed\" }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Help/Customers.rmd:2\", \"reference\": null, \"tags\": [ \"Help\", \"Customers\" ], \"contentsPreview\": \"In the customer registration, it is possible to modify...\", \"indexState\": \"Indexed\" }, ... ] } } View document View details about a specific document. Request GET /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institutional/Company.rmd:1\", // represents the indexing situation of the document. // valid values: Queued, Indexed, Cancelled \"state\": \"Indexed\", // content of the indexed document \"contents\": \"...\", // document reference ID \"reference\": \"institutional-company\" } } Delete document Permanently deletes a document through its ID. Request DELETE /api/v1/collections/{collection-id}/documents/{document-id} Response { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/en/entities/functions.html": {
    "href": "docs/en/entities/functions.html",
    "title": "Functions | AIVAX",
    "keywords": "Functions Functions are a way to force your model to process information using JSON as an intermediate communication method. With functions, you can make any model respond in the JSON format you want. It can be useful for categorizing comments, applying moderation to reviews, or processing information with the help of AI. Currently, it is only possible to use functions with models provided by Open Indexer. Calling a Function To call an AI function, you will need to specify what the AI should respond with and provide a JSON schema that it should follow. Less intelligent models tend to fail at generating JSON, resulting in an invalid or problematic document. To fix this, adjust your model, instruction, and attempt parameter if necessary. You are charged for each attempt the AI makes to generate a response. Slightly more intelligent models tend to generate correct results on the first attempt. It is guaranteed that a valid JSON will be generated and that this JSON will follow the same schema provided in the request. Additionally, you can choose to activate internet search for the function call. This option can be useful for bringing in relevant data in real-time when structuring your response. When using this function, a model with internet access will be used to obtain data from the internet to structure your response. This model will also attempt to structure your response based on the provided data, and if it can formulate a valid JSON, the step of calling the structuring model is ignored and the response is immediately returned. If the online search model is unable to structure a valid JSON, the model chosen in the request will be responsible for this task and will start the attempt chain to generate a response. More intelligent models get it right on the first attempts. Through the fetch property, you can provide a list of URLs to be attached to the generation context. Open Indexer makes a GET request to access the provided content and renders it in the request content. Only 2xx or 3xx responses are accepted, and the response content must be textual. HTML responses are sanitized to include only the page text, without scripts and CSS. The maximum size that can be read from a fetch URL is 10 Mb. The maximum number of items for fetch is 10 URLs. Retries for generating JSON content do not search the internet again or call the fetch content. Requests that search the internet bring good results and dispense with crawlers, scrapers, or the need to pay for a specific API, but can be expensive and relatively slow to obtain. Consider using a cache on the side of your application for data that does not need to be constantly updated, such as weather data, daily statistics, etc. Open Indexer does not perform any caching on our side. Request POST /api/v1/functions/json { // Required. Specify the name of the integrated model to be used to perform the action. \"modelName\": \"@metaai/llama-3.1-8b\", // Required. Explain what your model should do with the input and how it should bring the response. \"instructions\": \"Classify the user's comment, indicating whether it is positive or negative, and if it has any relevant information (number between 0 (not very relevant) and 10 (very relevant))\", // Required. The JSON object that the model should generate. You can provide generation examples in the instructions field. This object must be a valid JSON in the API. // This object must be an object, an array, or a string. \"responseSchema\": { \"feedbackType\": \"{neutral|positive|negative}\", \"informationScore\": 5 }, // Optional. Specifies a list of JSON paths that the AI must generate content for and that this field cannot be null. For arrays, specify with [*]. \"requiredFields\": [ \"$.feedbackType\", \"$.informationScore\" ], // Optional. Defines a JSON input for the model. Can be any type of JSON value. \"inputData\": { \"userComment\": \"Terrible market. Has a guard inside watching you so you don't steal and the butchers ignore you and serve pretty girls in front of you. But thank God there are other markets coming and the end of this nonsense will come\" }, // Optional. Defines how many attempts the model should try before the API returns an error. Must be a number between 1 and 30. \"maxAttempts\": 10, // Optional. Defines the time limit in seconds to obtain a valid JSON before the API returns an error. Must be a number between 1 and 3600 (one hour). \"timeout\": 300, // Optional. Allows the model to perform an internet search to improve the construction of the response. \"webSearch\": { // Required. Activates or deactivates the internet search of the function. \"enabled\": true }, // Optional. Adds external resources to complement the generation of the response. \"fetch\": { // Required. Provides the list of URLs that Open Indexer will access. The maximum is 10 URLs. \"urls\": [ \"https://url1...\", \"https://url2...\", ], // Optional. Defines the behavior of the fetch for errors when trying to access the site. Errors include responses that are not 2xx or 3xx, timeouts, certificate errors, etc. // fail -> returns an error in the function response (default) // warn -> adds a warning to the function response and does not include the error in the AI generation // ignore -> ignores the error and adds the error to the AI generation \"fetchFailAction\": \"fail\" | \"warn\" | \"ignore\", // Optional. Defines the timeout in seconds for the maximum response time and reading the contents. The maximum is 120 seconds (two minutes). \"timeout\": 10, // Optional. Defines the maximum content size in character quantity that can be included in the AI generation before being truncated. \"pageMaxLength\": 2048 } } Response { \"message\": null, \"data\": { // the result contains the object defined in \"responseSchema\", with the fields filled in by the AI \"result\": { \"feedbackType\": \"negative\", \"informationScore\": 8 }, // in which attempt the AI was able to generate a valid JSON \"attempt\": 1, // the time in milliseconds to obtain a valid JSON \"elapsedMilliseconds\": 527, // warnings produced by the generation \"warnings\": [] } } Considerations about the JSON Schema Specify enumerated values with \"{value1|value2|value3}\". In this way, the model should choose one of the values presented in the generation of the JSON. All values are placeholders for the model generation. Indicate what a field is or what it should receive as a value with a hint in its own placeholder or indicate directly in the function instructions. All values can be null, unless you specify directly to the model that they cannot. The output structure of the model is the same as informed in responseSchema. The input structure is irrelevant. Examples Check out examples of AI functions for various everyday tasks: Summarize order and classify if it requires attention or not POST /api/v1/functions/json { \"modelName\": \"@metaai/llama-4-scout-17b\", \"instructions\": \"Summarize the user's comment, creating a short description, with a maximum of 10 words indicating what they want to do. Also, indicate if this comment requires attention or not.\", \"responseSchema\": { \"shortSummary\": \"...\", \"requiresAttention\": false }, \"inputData\": \"The customer Fernando de Castro has been trying to contact support since Friday and says he will cancel if he doesn't speak to someone today. He also said he is a friend of Rebecca from the commercial and is threatening to speak badly about the company on TikTok. Please, can someone attend to this guy??\" } { \"message\": null, \"data\": { \"result\": { \"shortSummary\": \"Customer wants contact with support to avoid cancellation and threatens\", \"requiresAttention\": true }, \"attempt\": 1, \"elapsedMilliseconds\": 639 } } Bring latest news and weather for a given city POST /api/v1/functions/json { \"modelName\": \"@google/gemini-2.0-flash-lite\", \"instructions\": \"Search for the 5 latest news and weather data for the informed city.\", \"responseSchema\": { \"latestNews\": [ { \"title\": \"...\", \"details\": \"...\", \"link\": \"https://...\" } ], \"weather\": { \"currentTemperature\": 0, \"currentWeather\": \"{sunny|cloudy|rain|thunderstorm}\", \"forecast\": \"{sunny|cloudy|rain|thunderstorm}\" } }, \"inputData\": { \"city\": \"São José do Rio Preto\" }, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"latestNews\": [ { \"title\": \"GCM arrests trio for drug trafficking in Rio Preto's Calçadão\", \"details\": \"The Municipal Guard (GCM) of São José do Rio Preto arrested, on Tuesday night (6), three people suspected of drug trafficking in the Calçadão [4, 9].\", \"link\": \"https://dhoje.com.br/gcm-prende-trio-por-trafico-de-drogas-no-calcadao-de-rio-preto/\" }, { \"title\": \"Job Supported makes selection for people with disabilities\", \"details\": \"A beverage distributor in Rio Preto makes a selection this Thursday, 8/5, from 9 am to 11 am [4, 9].\", \"link\": \"https://dhoje.com.br/emprego-apoiado-faz-selecao-para-pessoas-com-deficiencia/\" }, { \"title\": \"Social Fund distributes Love Kit to pregnant women in Rio Preto\", \"details\": \"The Social Fund's campaign mobilizes volunteers and the population to support mothers in Rio Preto [4].\", \"link\": \"https://dhoje.com.br/fundo-social-distribui-enxoval-do-amor-para-gestantes-em-rio-preto/\" }, { \"title\": \"PM opens 2,200 vacancies for police officers to work\", \"details\": \"The São Paulo Military Police published, on Tuesday, 6, an edict for the hiring of 2,200 military police officers from the reserve to work [4].\", \"link\": \"https://dhoje.com.br/pm-abre-22-mil-vagas-para-policiais-da-reserva-atuarem/\" }, { \"title\": \"Rio Preto may receive R$ 63.8 million from the State of São Paulo for improvement works\", \"details\": \"The mayor is seeking R$ 63.8 million to make a package of works viable [4].\", \"link\": \"https://dhoje.com.br/infraestrutura-prefeito-busca-r-638-milhoes-para-viabilizar-pacotaco-de-obras/\" } ], \"weather\": { \"currentTemperature\": 18, \"currentWeather\": \"sunny\", \"forecast\": \"sunny\" } }, \"attempt\": 1, \"elapsedMilliseconds\": 4187 } } Bring COVID-19 statistics in real-time POST /api/v1/functions/json { \"modelName\": \"@google/gemini-1.5-flash-8b\", \"instructions\": \"Bring the count of cases and deaths by COVID-19.\", \"responseSchema\": { \"deathsWorld\": 0, \"deathsBrazil\": 0, \"casesWorld\": 0, \"casesBrazil\": 0 }, \"inputData\": null, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"deathsWorld\": 7010681, \"deathsBrazil\": 711380, \"casesWorld\": 704753890, \"casesBrazil\": 38743918 }, \"attempt\": 1, \"elapsedMilliseconds\": 1620 } } Bring top artists by music genre POST /api/v1/functions/json { \"modelName\": \"@google/gemini-1.5-flash-8b\", \"instructions\": \"Search and format a list of 10 artists in the TOP 10 of music streaming by genre.\", \"responseSchema\": { \"edm\": [ \"artist name\", \"artist name\", \"...\" ], \"rap\": [ \"artist name\", \"artist name\", \"...\" ], \"pop\": [ \"artist name\", \"artist name\", \"...\" ] }, \"inputData\": null, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"edm\": [ \"David Guetta\", \"Calvin Harris\", \"The Chainsmokers\", \"Marshmello\", \"Avicii\", \"Kygo\", \"Tiesto\", \"DJ Snake\", \"Daft Punk\", \"Skrillex\" ], \"rap\": [ \"Drake\", \"Eminem\", \"Kanye West\", \"Juice WRLD\", \"Travis Scott\", \"XXXTENTACION\", \"Kendrick Lamar\", \"Future\", \"J. Cole\", \"Nicki Minaj\" ], \"pop\": [ \"Taylor Swift\", \"Drake\", \"Bad Bunny\", \"The Weeknd\", \"Ed Sheeran\", \"Ariana Grande\", \"Justin Bieber\", \"Billie Eilish\", \"Rihanna\", \"Bruno Mars\" ] }, \"attempt\": 1, \"elapsedMilliseconds\": 8370 } }"
  },
  "docs/en/entities/search.html": {
    "href": "docs/en/entities/search.html",
    "title": "Search | AIVAX",
    "keywords": "Search The search API, through the query key obtained from the collections, performs a semantic search in it, performing an intelligent comparison for each indexed document in a collection. After creating a collection, you will get its ID. Use the ID of your collection to perform the search in the indexed documents of the same. Use the endpoints of this API to embed the semantic search of documents in your AI model or chatbot. Searching documents This endpoint expects a GET request with the following parameters: term: required. Specifies the search term that will be searched in the documents. top: Specifies the maximum number of documents that should be returned in the search. min: Specifies the minimum score for obtaining the documents. Warning Warning: this endpoint generates cost. The cost is calculated based on the tokens of the search term. The search term is tokenized according to the model used in the indexing of the documents. Request GET /api/v1/collections/{collection-id}/query term=What is the color of the Honda CIVIC? Response { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Products/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Products/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Products/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } For the search result, the higher the score, the more similar the document is to the search term. The Open Indexer uses embedding models that allow task orientation. For the search, the term is vectorized with a DOCUMENT_QUERY orientation. For document indexing, the orientation is DOCUMENT_RETRIEVAL, which provides a more optimized search and not to verify the similarity between documents."
  },
  "docs/en/getting-started.html": {
    "href": "docs/en/getting-started.html",
    "title": "Welcome | AIVAX",
    "keywords": "Welcome Welcome to Open Indexer. Our service makes it easier to develop intelligent AI models that use a knowledge base provided by you to converse with the user, answer questions, provide real-time information, and more. To get started, all endpoints must be made to the AIVAX production URL: https://inference.aivax.net/ Concepts and definitions Understand the concepts used by the API below: Account: represents a user account, which has an authentication token. Collection: represents a collection of knowledge documents. A user can have multiple document collections. Document: represents a fact, a single piece of knowledge, and an item in a collection. A collection can have multiple documents. AI Gateway: represents an AI gateway that benefits from or does not use a knowledge collection, such as a plug-and-play knowledge middleware for a model. Embedded model: represents an AI model that Open Indexer provides to the user. Chat client: represents a user interface that makes the AI gateway available through an interactive online chat. Chat session: hosts a conversation and context of a chat client. Handling errors All API errors return an HTTP response with a non-OK status (never 2xx or 3xx), and always follow the JSON format: { \"error\": \"An error explanation message\", \"details\": {} // an object containing relevant error information. Most of the time it is null }"
  },
  "docs/en/limits.html": {
    "href": "docs/en/limits.html",
    "title": "API Limits | AIVAX",
    "keywords": "API Limits Rate limits (\"rate limiters\") regulate the number of requests you can send in a time window. These limits help Open Indexer prevent abuse and provide a stable API to everyone. The API limits below are the same for all embedded AIVAX models. These limits are categorized by operations performed by the API. Each account has a tier that defines which limits are applied to the account. Tiers change according to the total invested in Open Indexer and the time the account has existed. Tier zero (free account): new account that has never added credits. Tier 1: account created at least 48 hours ago and has added any credit value. Tier 2: account created at least 1 month ago and has added at least $100 in credits. Tier 3: account created at least 3 months ago and has added at least $1,000 in credits. The measurement is by credit addition and not by consumption. For example, you don't need to consume $100 in credits to advance to Tier 2. Limit legends: RPM: requests per minute. RPD: requests per day (24 hours). Free Tier 1 Tier 2 Tier 3 Operation RPM RPD Document search 50 - Document insertion - 100 Inference 5 500 Function 5 500 Function (Live) 3 30 Operation RPM RPD Document search 150 - Document insertion - 3,000 Inference 75 1,000 Function 50 1,000 Function (Live) 5 200 Operation RPM RPD Document search 300 - Document insertion - 10,000 Inference 150 - Function 50 10,000 Function (Live) 10 400 Operation RPM RPD Document search 1,000 - Document insertion - 30,000 Inference 1,000 - Function 750 - Function (Live) 200 - Document search: includes semantic search of documents in a collection by the search endpoint ../collections/{id}/query. Document insertion: includes creation and modification of documents in a collection. Inference: every inference call, either by the Open-AI compatible API, by the route /ai-gateways/{id}/inference, or by each message sent by a client chat session. Function: every function call /functions. Function (Live): every function call connected to the internet by the webSearch parameter. Does not include fetch calls."
  },
  "docs/en/models.html": {
    "href": "docs/en/models.html",
    "title": "Models | AIVAX",
    "keywords": "Models AIVAX provides models from different providers to make development even faster, eliminating the need to configure an account for each provider to access their latest models. See the list below of available models and their pricing. All prices consider the total input and output of tokens, with or without cache. All prices are in United States dollars. aivax Model Name Prices Description @aivax/sentinel Input + output: $ 1.31 /1M tokens Internet search: $ 5.60 /1,000 searches Sentinel reasoning: $ 1.31 /1M tokens Highly intelligent and optimized to perform challenging tasks and solve complex problems. Input: accepts images Function calls Reasoning Internet search Access links Code execution Persistent memory @aivax/sentinel-mini Input + output: $ 0.88 /1M tokens Internet search: $ 5.60 /1,000 searches Sentinel reasoning: $ 1.31 /1M tokens Highly capable model, with reasoning power and complex problem-solving. Input: accepts images Function calls Reasoning Internet search Access links Code execution Persistent memory @aivax/sentinel-router Input + output: $ 0.66 /1M tokens Sentinel's router, which automatically selects the best Sentinel model to perform a task according to its complexity. Input: accepts images Function calls Reasoning Internet search Access links Code execution cognitivecomputations Model Name Prices Description @cognitivecomputations/dolphin2.9 Input + output: $ 2.60 /1M tokens An experimental model based on Mistral that is completely uncensored. Use responsibly. Function calls JSON functions deepseekai Model Name Prices Description @deepseekai/r1-distill-llama-70b Input + output: $ 2.76 /1M tokens Model with deep reasoning and thought, best for more demanding tasks. Function calls Reasoning google Model Name Prices Description @google/gemini-2.5-pro Input + output: $ 12.25 /1M tokens One of the most powerful models today. Input: accepts images, videos, audio Function calls Reasoning JSON functions @google/gemini-2.5-flash-th Input + output: $ 4.65 /1M tokens Latest generation model with integrated reasoning and thought. Input: accepts images, videos, audio Function calls Reasoning JSON functions @google/gemini-2.5-flash Input + output: $ 1.31 /1M tokens Same model as Gemini 2.5 Flash, but with the reasoning module turned off. Input: accepts images, videos, audio Function calls JSON functions @google/gemini-2.5-flash-lv Input + output: $ 1.31 /1M tokens Internet search: $ 39.20 /1,000 searches Version of Gemini 2.5 Flash with internet search. Input: accepts images, videos, audio Internet search @google/gemini-2.0-flash Input + output: $ 0.88 /1M tokens Offers new generation features, with improved speed and multi-modal generation. Input: accepts images, videos, audio Function calls JSON functions @google/gemini-2.0-flash-lv Input + output: $ 0.88 /1M tokens Internet search: $ 39.20 /1,000 searches Version of Gemini 2.0 with internet search via Google. Input: accepts images, videos, audio Internet search @google/gemini-2.0-flash-lite Input + output: $ 0.66 /1M tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Input: accepts images, videos, audio Function calls JSON functions @google/gemini-1.5-flash-8b Input + output: $ 0.33 /1M tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Input: accepts images, videos, audio Function calls JSON functions metaai Model Name Prices Description @metaai/llama-3.3-70b Input + output: $ 2.40 /1M tokens Previous generation model with many parameters and surprisingly fast speed. Function calls JSON functions @metaai/llama-4-maverick-17b-128e Input + output: $ 1.40 /1M tokens Fast model, with 17 billion activated parameters and 128 experts. Input: accepts images Function calls JSON functions @metaai/llama-4-scout-17b-16e Input + output: $ 0.79 /1M tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Input: accepts images Function calls JSON functions @metaai/llama-3.1-8b Input + output: $ 0.23 /1M tokens Cheap and fast model for less demanding tasks. Function calls JSON functions openai Model Name Prices Description @openai/gpt-4o Input + output: $ 13.50 /1M tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Input: accepts images Function calls JSON functions @openai/gpt-4.1 Input + output: $ 11.00 /1M tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Input: accepts images Function calls JSON functions @openai/o4-mini Input + output: $ 6.50 /1M tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Input: accepts images Reasoning JSON functions @openai/gpt-4.1-mini Input + output: $ 3.00 /1M tokens Fast and cheap for focused tasks. Input: accepts images Function calls JSON functions @openai/gpt-4o-mini Input + output: $ 1.31 /1M tokens Smaller version of 4o, optimized for everyday tasks. Input: accepts images Function calls JSON functions @openai/gpt-4.1-nano Input + output: $ 0.88 /1M tokens The fastest and cheapest GPT 4.1 model. Input: accepts images Function calls JSON functions qwen Model Name Prices Description @qwen/qwq-32b Input + output: $ 1.23 /1M tokens Conversational model with thinking and reasoning for solving complex tasks. Function calls Reasoning"
  },
  "docs/en/protocol-functions.html": {
    "href": "docs/en/protocol-functions.html",
    "title": "Protocol Functions | AIVAX",
    "keywords": "Protocol Functions The protocol functions of AIVAX, or server-side functions, are a customized implementation of function calls created by AIVAX that allows the model to strictly follow a function context that is not based on JSON documents. This feature is available for models with Sentinel reasoning. The protocol functions allow actions to be taken on the AIVAX server side, removing the need to implement the function on the client side and integrating with existing applications and services. These functions expect a callback through a URL, and when the model decides to call the function, the callback is accessed with the parameters informed by the assistant itself. The assistant does not know which URL it is calling, as it remains invisible to both the assistant and the user. Defining Protocol Functions These functions are defined in the AI-gateway, which allows the creation of intelligent agents that perform actions without human intervention. They follow a simple syntax, expecting the function name, a description of what it does, and the invocation parameters. Choosing the Function Name The function name must be simple and deterministic to what the function does. Avoid names that are difficult to guess or do not refer to the function's role, as the assistant may become confused and not call the function when appropriate. As an example, let's consider a function to query a user in an external database. The following names are good examples to consider for the call: search-user query-user search_user Bad names include: search (too broad) query-user-in-database-data (name too long) pesquisa-usuario (name not in English) search user (name with improper characters) Having the function name, we can think about the function description. Choosing the Function Description The function description must conceptually explain two situations: what it does and when it should be called by the assistant. This description must include the scenarios that the assistant should consider calling it and when it should not be called, providing a few examples of calls (one-shot) and/or making the function rules explicit. Creating the Endpoint that the Function will Call Creating the Function Definition"
  },
  "docs/en/sentinel.html": {
    "href": "docs/en/sentinel.html",
    "title": "Sentinel Agents | AIVAX",
    "keywords": "Sentinel Agents The Sentinel chat agents are models with optimized parameters for real-time chat conversation, with instructions that enhance the model's attention to specific tasks and allow for real-time internet research. These models are constantly updated to utilize the latest technologies and resources available from generative AI, so the internal models used by the Sentinel models can be constantly updated. Currently, the Sentinel models are available in three categories: sentinel: highly intelligent and optimized to perform challenging tasks and solve complex problems. sentinel-mini: highly capable model, optimized for cost, with reasoning and problem-solving power. Whenever a Sentinel model is going to have its price changed, a notification is sent to all users who use the model and a notification is added to the notifications page. These models should be used for conversation with the end-user and are not recommended for other tasks such as using functions, structured responses, etc. Main Features The differential of the Sentinel models are the resources provided \"out-of-the-box\" by them, including: Optimized for chat: the Sentinel models have clear instructions that make the model adapt the tone of conversation to that of the user, humor indicators, and objective. Reasoning: all Sentinel models have a customized deep thinking mechanism, optimized to reason based on their provided functions, including protocol functions, to provide a detailed response to the user. Internet research: the Sentinel models can naturally search the internet to complement their response in different scenarios, such as obtaining local news, weather data, or data on a specific topic or niche. Accessing links: the Sentinel models can access files and pages provided by the user. They can access HTML pages, various text files, even Word documents and PDFs. Code execution: the Sentinel models can execute code to perform mathematical, financial calculations, or even help the user with various tasks. Persistent memory: the Sentinel models can identify relevant facts to the user that should be persisted during several conversations, even after the current conversation is cleared or renewed. Additionally, all Sentinel models have a function execution chain. For example, you can ask a Sentinel model to access the temperature of a city, convert it to JSON, and call an external URL with the response. Note Important: the Sentinel context includes a detailed description of its behavior and functions and occupies approximately ~1.300 tokens that will be appended to all messages sent by the user. Sentinel Router You can also use the sentinel-router routing model, which works as a router model between the Sentinel models. A router automatically chooses the best model to solve the user's problem based on the complexity of their problem. How does it work? A smaller model analyzes the context of the question and evaluates the degree of complexity that the user is facing, and this model responds with an indicator of which model is best to answer that question. The router decides which is the best model per message and not per conversation. A routing model can help reduce costs and maintain conversation quality, using deep thinking resources only when necessary. Sentinel Reasoning The Sentinel's deep thinking mechanism is a plug-and-play reasoning engine that provides all the necessary context for the Sentinel model to respond to the user's question. During the thinking process, Sentinel calls functions, performs calculations, and executes code, aiming to provide an elaborate and accurate response to the user, even when used with smaller and less intelligent models. The pricing of the Sentinel's reasoning and the Sentinel's tokens are separate: you will notice that when using the Sentinel model, you will see launches of routing, reasoning, and inference. This division is made to provide transparency about the use of the Sentinel."
  },
  "docs/entities/ai-gateway.html": {
    "href": "docs/entities/ai-gateway.html",
    "title": "AI Gateway | AIVAX",
    "keywords": "AI Gateway Os gateways de AI é um serviço que a Open Indexer fornece para criar um túnel de inferência entre um modelo de LLM e uma base de conhecimento. Nele é possível: Criar um modelo com instruções personalizadas Usar um modelo provido por você através de um endpoint OpenAI compatível, ou usar um modelo disponibilizado pela Open Indexer Personalizar parâmetros de inferência, como temperatura, top_p, prefill Usar uma coleção de conhecimento como fundação de respostas para IA Dentre outros recursos. Com o AI Gateway, você cria um modelo pronto para uso, parametrizado e fundamentado nas instruções que você definir. Modelos Você pode trazer um modelo de IA compatível com a interface OpenAI para o gateway de IA. Se você trazer seu modelo de IA, iremos cobrar apenas pela pesquisa de documentos anexada na IA. Você também pode usar um dos modelos abaixo que já estão prontos para começar com o Open Indexer. Ao usar um modelo, você perceberá que alguns são mais inteligentes que outros para determinadas tarefas. Alguns modelos são melhores com certas estratégias de obtenção de dados do que outros. Realize testes para encontrar o melhor modelo. Você pode ver os modelos disponíveis na página de modelos. Escolhendo uma estratégia de busca Se você for usar uma coleção de conhecimento com um modelo de IA, você poderá escolher uma estratégia que a IA usará para realizar uma busca por informação. Cada estratégia é mais refinada que a outra. Algumas criam resultados melhores que as demais, mas é importante realizar testes práticos com várias estratégias para entender qual se ajusta melhor no modelo, conversa e tom do usuário. Talvez seja necessário realizar ajustes no prompt do sistema para informar melhor como a IA deverá considerar os documentos anexados na conversa. Os documentos são anexados como uma mensagem do usuário, limitados aos parâmetros que você define na estratégia de obtenção. Estratégias com reescrita normalmente geram os melhores resultados à um baixo custo de latência e custo. O modelo de reescrita usado sempre o com menor custo, escolhido normalmente por um pool interno que decide o modelo que está com menor latência no momento. Estratégias sem custo de reescrita: Plain: a estratégia padrão. É a menos otimizada e não possui custo de reescrita: a última mensagem do usuário é usada como termo de busca para pesquisar na coleção anexada do gateway. Concatenate: Concatena em linhas as últimas N mensagens do usuário, e então o resultado da concatenação é usada como termo de busca. Estratégias com custo de reescrita (os tokens de inferência são cobrados): UserRewrite: reescreve as últimas N mensagens do usuário usando um modelo menor, criando uma pergunta contextualizada no que o usuário quer dizer. FullRewrite: reescreve as últimas N*2 mensagens do chat usando um modelo menor. Similar ao UserRewrite, mas considera também as mensagens da assistente na formulação da nova pergunta. Geralmente cria as melhores perguntas, com um custo um pouco maior. É a estratégia mais estável e consistente. Funciona com qualquer modelo. Estratégias de função: QueryFunction: fornece uma função de pesquisa na coleção integrada para o modelo de IA. Você deverá ajustar nas instruções do sistema os cenários ideais para o modelo chamar essa função quando necessário. Pode não funcionar tão bem em modelos menores. Usando funções (tools) de IA No momento, não é possível especificar chamadas de função através da nossa API, seja pelo AI-Gateway ou pela API OpenAI compatível. Esse recurso está no nosso radar para implementação futura. Se isso é crítico para seu modelo de IA funcionar, você pode usar a API de busca de documentos no seu modelo. Criando um gateway de IA Se for usar um modelo providenciado por você, tenha em mãos: O base address compatível com a API OpenAI A chave de API do endpoint (se aplicável) O nome do modelo de inferência. Não é necessário ter uma coleção para vincular no seu gateway de IA. Você pode criar um gateway de IA e vincular uma base de conhecimento nela posteriormente. Requisição POST /api/v1/ai-gateways { // Nome do gateway para identificá-lo posteriormente. \"name\": \"my-gateway-model\", \"parameters\": { // Obrigatório. Endpoint compatível com chat/completions da OpenAI, ou use @integrated // para usar um modelo provido pela Open Indexer. \"baseAddress\": \"@integrated\", // Obrigatório. Especifica o nome do modelo que será usado na inferência. \"modelName\": \"@groq/compound-beta\", // Opcional. ID da coleção que será usada como base de conhecimento pela IA. \"knowledgeCollectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\", // Opcional. Especifica quantos documentos devem ser anexados no contexto da IA. \"knowledgeBaseMaximumResults\": 16, // Opcional. Especifica a pontuação mínima de similaridade que a busca de documentos deve anexar no contexto da IA. \"knowledgeBaseMinimumScore\": 0.55, // Opcional. Especifica se referências de documentos devem ser anexadas no contexto da IA. \"knowledgeUseReferences\": false, // Opcional. Especifica a estratégia de obtenção de documentos. Leia \"Escolhendo uma estratégia de busca\" para saber mais. \"queryStrategy\": \"UserRewrite\", // Opcional. Parâmetros da estratégia de obtenção. \"queryStrategyParameters\": { // Opcional. Especifica a quantidade de mensagens que devem ser consideradas para as estratégias UserRewrite e FullRewrite. Nota: para FullRewrite, o valor sempre é multiplicado por 2 para considerar mensagens da assistente. \"rewriteContextSize\": 3, // Opcional. Especifica a quantidade de mensagens do usuário que devem ser concatenadas no termo da busca. \"concatenateContextSize\": 3 }, // Opcional. Especifica a chave de api \"Authorization: Bearer ...\" usado na inferência. Deixe nulo se usar um modelo embutido da AIVAX. \"apiKey\": null, // Opcional. Especifica a temperatura do modelo. \"temperature\": 1.25, // Opcional. Especifica o nucleos sampling do modelo. \"topP\": null, // Opcional. Especifica a penalidade de presença de tokens do modelo. \"presencePenalty\": null, // Opcional. Especifica um termo de \"stop\" para o modelo. \"stop\": null, // Opcional. Especifica o máximo de tokens de resposta do modelo. \"maxCompletionTokens\": 4096, // Opcional. Especifica o system-prompt usado no modelo. \"systemInstruction\": \"Você é uma assistente amigável.\", // Opcional. Transforma a pergunta do usuário para o formato indicado abaixo, onde \"{prompt}\" é o prompt original do usuário. \"userPromptTemplate\": null, // Opcional. Especifica um prefill (inicialização) da mensagem da assistente. \"assistantPrefill\": null, // Opcional. Especifica se o assistantPrefill e o stop devem ser incluídos na mensagem gerada pela assistente. \"includePrefillingInMessages\": false, // Opcional. Especifica flags especiais para o modelo. Deixe como \"0\" para não usar nenhuma flag. As flags permitidas são: // NoSystemInstruct: ao invés de usar system prompt, insere as instruções do system em uma mensagem de usuário \"flags\": \"0\", // Opcional. Passa um array de funções para a IA. \"tools\": [], // Opcional. Ativa funções de protocolo para modelos Sentinel. Leia \"Protocol Functions\" para saber mais. \"protocolFunctions\": [ { \"name\": \"get-weather\", \"description\": \"Use essa função para obter dados de clima para a localização informada.\", \"callbackUrl\": \"https://my-service.com/ai-service\", \"contentFormat\": { \"location\": \"nome da cidade\" } } ] } } Resposta { \"message\": null, \"data\": { \"aiGatewayId\": \"01965b64-a8eb-716c-892d-880159a9f12d\" } } Editar um gateway de IA O corpo da requisição é basicamente o mesmo do endpoint de criar um ai-gateway. Ao invés de usar POST, use PATCH. Requisição PATCH /api/v1/ai-gateways/{ai-gateway-id} Resposta { \"message\": \"Gateway editted.\", \"data\": null } Usar um gateway de IA O endpoint de conversação com um gateway de IA é simples: ele espera apenas a conversa. Você pode receber a resposta de uma vez ou por streaming. Requisição POST /api/v1/ai-gateways/{ai-gateway-id}/inference { \"messages\": [ { \"role\": \"user\", \"content\": \"Como eu posso ligar meu Civic 2015?\" } ], \"stream\": true } Resposta para stream=true A resposta de streaming é baseada em server-sent events. A primeira linha sempre é uma resposta com informações de depuração. data: {\"content\":\"\",\"isFirstChunkMetadata\":true,\"embeddedDocuments\":[],\"debugInfo\":[{\"name\":\"EmbeddingTimeMs\",\"value\":7.5045},{\"name\":\"InferenceTimeMs\",\"value\":0},{\"name\":\"ElapsedTotalMs\",\"value\":8.3489},{\"name\":\"KnowledgeQueryText\",\"value\":null}]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} data: {\"content\":\"[...]\",\"isFirstChunkMetadata\":false,\"embeddedDocuments\":[],\"debugInfo\":[]} ... data: [END] Resposta para stream=false { \"message\": null, \"data\": { \"generatedMessage\": \"[...]\", \"embeddedDocuments\": [], \"debugInfo\": [ { \"name\": \"EmbeddingTimeMs\", \"value\": 4140.8628 }, { \"name\": \"InferenceTimeMs\", \"value\": 4140.803 }, { \"name\": \"ElapsedTotalMs\", \"value\": 4141.4771 }, { \"name\": \"KnowledgeQueryText\", \"value\": null } ] } } Ver um gateway de IA A requisição abaixo traz detalhes de um AI gateway. Requisição GET /api/v1/ai-gateways/{ai-gateway-id} Resposta { \"message\": null, \"data\": { \"name\": \"my-gateway-client\", \"parameters\": { \"baseAddress\": \"@integrated\", \"knowledgeCollectionId\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"knowledgeBaseMaximumResults\": 16, \"knowledgeBaseMinimumScore\": 0.55, \"knowledgeUseReferences\": false, \"queryStrategy\": \"ToolCall\", \"queryStrategyParameters\": { \"rewriteContextSize\": 3, \"concatenateContextSize\": 3 }, \"apiKey\": null, \"modelName\": \"@google/gemini-2.0-flash-lite\", \"temperature\": 1.25, \"topP\": null, \"presencePenalty\": null, \"stop\": null, \"maxCompletionTokens\": 4096, \"systemInstruction\": \"[...]\", \"userPromptTemplate\": null, \"assistantPrefill\": null, \"includePrefillingInMessages\": false, \"flags\": \"0\", \"tools\": null } } } Excluir um gateway de IA Permanentemente remove um gateway de IA. Warning Ao remover um gateway de IA, todos os chat clients associados também são removidos. Coleções não são removidas. Requisição DELETE /api/v1/ai-gateways/{ai-gateway-id} Resposta { \"message\": \"AI gateway deleted.\", \"data\": null } Endpoint OpenAI A Open Indexer provê um endpoint compatível com a interface OpenAI através de um AI-gateway, o que facilita a integração do modelo criado pela Open Indexer com aplicações existentes. Vale ressaltar que somente algumas propriedades são suportadas. Em um gateway de IA, você já configura os parâmetros do modelo, como System Prompt, temperatura e nome do modelo. Ao usar esse endpoint, alguns valores do gateway podem ser sobrescritos pela requisição. Requisição POST /api/v1/ai-gateways/{ai-gateway-id}/open-ai/v1/chat/completions { // O campo \"model\" é obrigatório, mas não faz nada nessa requisição. Ele só existe para ser compatível com a API Open AI. Você pode deixar ele vazio ou escrever qualquer coisa no lugar, pois o modelo considerado é o definido no AI Gateway. \"model\": \"foobar\", // As mensagens devem seguir o formato da Open AI. Somente \"system\" e \"user\" são suportados como \"roles\" da conversa. \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"Hello!\" } ], // Ambas propriedades são equivalentes e opcionais e vão substituir o campo maxCompletionTokens se forem enviadas na requisição. \"max_completion_tokens\": 1024, \"max_tokens\": 1024, // Opcional. Substitui o parâmetro do gateway. \"stop\": \"\\n\", // Opcional. Por padrão a resposta não é por streaming. \"stream\": true } Resposta para não-streaming { \"id\": \"019672f3-699c-7d45-8484-7a23f4cdc079\", \"object\": \"chat.completion\", \"created\": 1745685277, \"model\": \"gemini-2.0-flash-lite\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\\n\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0 }, \"service_tier\": \"default\" } Resposta para streaming data: {\"id\":\"019672f4-9a58-7932-82f0-022e457a2e63\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_2i0nmn\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\",\"content\":\"Hi\"}}]} data: {\"id\":\"019672f4-9ab9-73a2-bdb8-23c4481453a8\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_ar1qol\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" there! How can I help you today?\\n\"}}]} ... data: {\"id\":\"019672f4-9ac0-7ddf-a76a-e7f8043dd082\",\"object\":\"chat.completion.chunk\",\"created\":1745685355,\"model\":\"gemini-2.0-flash-lite\",\"system_fingerprint\":\"fp_3e84ge\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}]} ```'"
  },
  "docs/entities/chat-clients.html": {
    "href": "docs/entities/chat-clients.html",
    "title": "Chat Clients | AIVAX",
    "keywords": "Chat Clients Um cliente de chat provê uma interface de usuário através de um AI Gateway que permite o usuário conversar com sua assistente. Um chat client é integrado à inferência do AI gateway e dá suporte para pensamento profundo, pesquisa e conversa por texto. Recursos multi-modais, como envio de imagens e áudio estão em desenvolvimento. Note A Open Indexer nunca armazena o conteúdo de um chat entre um cliente e o usuário. Você pode usar JavaScript para essa tarefa, mas sob sua responsabilidade de uso e armazenamento. Você pode personalizar a interface do seu chat client com CSS e JavaScript personalizado, além de poder escolher a linguagem dos recursos do chat. Criando um chat client Cria um novo chat client. POST /api/v1/web-chat-client/ { // Especifica o nome público do seu chat client \"name\": \"Minha assistente\", // Especifica o ID do gateway de IA que será usado pelo chat \"aiGatewayId\": \"01965b64-a8eb-716c-892d-880159a9f12d\", \"clientParameters\": { // Opcional. Especifica o código da linguagem que será usada no chat para maioria dos elementos, como mensagens de erro, botões, etc. // Valores: pt-BR, en \"languageCode\": \"pt-BR\" | \"en\", // Opcional. Especifica um código JavaScript para executar no chat. \"customScripts\": null, // Opcional. Especifica um código CSS para aplicar estilos customizados no chat. \"customStyles\": null, // Opcional. Especifica a cor de realce dos elementos do chat client. \"primaryColor\": \"#eabe44\", // Opcional. Especifica o título da página de chat. \"pageTitle\": \"Assistente\", // Opcional. Especifica o título ao entrar no chat pela primeira vez. \"helloLabel\": \"É ótimo ver você aqui.\", // Opcional. Especifica o sub-título ao entrar no chat pela primeira vez. \"helloSubLabel\": \"Sou a sua assistente.\", // Opcional. Especifica o placeholder do campo de enviar mensagem. \"textAreaPlaceholder\": \"Falar com a assistente\", // Opcional. Especifica uma imagem/logo para exibir no chat pela primeira vez. \"logoImageUrl\": null, // Opcional. Ativa recursos de depuração. \"debug\": true, // Opcional. Especifica se o chat suporta processamento de mídia multi-modal, especificando quais botões ficarão visíveis para enviar o conteúdo multimídia ao modelo. // Document é processado internamente como markdown pela Open Indexer. \"inputModes\": [\"Image\", \"Audio\", \"Document\"], // Opcional. Especifica quais origens devem ser permitidas para embutir o cliente de chat em um iframe. Se esse campo estiver vazio, qualquer origem será aceita. \"allowedFrameOrigins\": [\"https://my-domain.com.br\"], // Opcional. Especifica botões de sugestão de conversa ao iniciar uma nova sessão de chat. Você pode adicionar quantos botões quiser, mas o aconselhável é até 3 botões. \"suggestionButtons\": [ { // Título que será exibido no botão. \"label\": \"Como comprar um carro?\", // Prompt que será enviado para o modelo. \"prompt\": \"Onde e como posso comprar um carro na sua loja?\" }, ... ] }, \"limitingParameters\": { // Opcional. Especifica quantas mensagens o usuário pode enviar por hora no chat. Essa opção é rastreada pelo userTag da sessão. \"messagesPerHour\": 30, // Opcional. Especifica o limite de mensagens (para o usuário e IA) que uma sessão pode ter. \"maxMessages\": 300 } } Resposta { \"message\": null, \"data\": { \"id\": \"01965b65-e95e-7795-848c-ff0919ef1436\" } } Editando um chat client O corpo dessa requisição é exatamente igual ao de criar um chat client. PUT /api/v1/web-chat-client/{chat-client-id} Resposta { \"message\": \"Web client updated successfully.\", \"data\": null } Listando os chat clients Obtém uma lista dos chat clients criados. GET /api/v1/web-chat-client/ Resposta { \"message\": null, \"data\": [ { \"id\": \"01965b59-daf6-7809-94c8-2a65b7264dba\", \"name\": \"Meu chat client\" }, ... ] } Vendo um chat client específico Obtém detalhes de um chat client existente. GET /api/v1/web-chat-client/{chat-client-id} Resposta { \"message\": null, \"data\": { \"name\": \"Meu chat client\", \"aiGateway\": { \"id\": \"01965b59-49ff-7753-8327-b3b6a6a871f2\", \"name\": \"gateway-t1\", \"knowledgeCollection\": { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"name\": \"Informações sobre carros\" } }, \"limitingParameters\": { \"messagesPerHour\": 30, \"userInputMaxTokens\": 1024, \"maxMessages\": 500 }, \"clientParameters\": { \"languageCode\": \"pt-BR\", \"customScripts\": null, \"customStyles\": null, \"primaryColor\": \"#f011d2\", \"pageTitle\": \"Lyra\", \"helloLabel\": \"É ótimo ver você aqui.\", \"helloSubLabel\": \"Sou sua assistente.\", \"textAreaPlaceholder\": \"Falar com a assistente\", \"logoImageUrl\": null, \"debug\": true, \"allowedFrameOrigins\": [] } } } Criar uma sessão de chat Uma sessão de chat é onde você cria uma conversa entre seu chat client e o usuário. Você pode chamar esse endpoint informando contexto adicional para conversa, como o nome do usuário, onde ele está, etc. Uma sessão de chat expira após algum tempo por segurança do token de acesso gerado. Quando você chama esse endpoint informando uma tag você pode chamar o mesmo endpoint várias vezes e obter a sessão de chat que está ativa para a tag informada, ou criar um chat novo se não existir uma sessão em andamento. Uma sessão de chat também restaura todas as mensagens da conversa da mesma sessão após desconexão. O usuário pode limpar a conversa ao clicar no botão de limpar conversa no canto superior direito do cliente de chat. Essa sessão usa os limites definidos pelo cliente de chat, como máximo de mensagens e tokens na conversa. Uma sessão é automaticamente renovada por mais 3 dias ao receber uma mensagem do usuário. Important Só é possível determinar a quantidade de tokens usados em uma mensagem ao usar um modelo provido pela Open Indexer. Se você usar um modelo externo, a propriedade limitingParameters.userInputMaxTokens será ignorada. POST /api/v1/web-chat-client/{chat-client-id}/sessions { // Opcional. Contexto adicional para a IA sobre o chat. \"extraContext\": \"# Contexto adicional\\r\\n\\r\\nVocê está falando com Eduardo.\", // Tempo em segundos para o chat expirar. O mínimo é 10 minutos. O máximo é 30 dias. \"expires\": 3600, // Opcional (recomendado). Um id externo para identificar a sessão posteriormente e reaproveitá-la sempre que chamar o mesmo endpoint. Pode ser o ID do usuário do seu banco de dados ou uma string que facilite a identificação desse chat posteriormente. \"tag\": \"my-user-tag\" } Resposta { \"message\": null, \"data\": { // ID da sessão de chat criada. \"sessionId\": \"01966f0b-172d-7bbc-9393-4273b86667d2\", // Chave pública de acesso do chat. \"accessKey\": \"wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\", // A URL pública para conversar com o chat. \"talkUrl\": \"https://preview-s01.proj.pw/www/web-chat-clients/wky_gr5uepjsgrhuqcj3aaat1iagrsmozwr9gghusnnu6zjhrsyures5xoe\" } }"
  },
  "docs/entities/collections.html": {
    "href": "docs/entities/collections.html",
    "title": "Coleções | AIVAX",
    "keywords": "Coleções Uma coleção é uma biblioteca de conhecimento: ela abriga vários documentos de conhecimento. Use coleções para agrupar documentos por finalidade, como documentar um produto, uma empresa, um serviço ou fluxo. Coleções não produzem custo. Não há limite de coleções por conta. Criar uma coleção Para criar uma coleção vazia, informe apenas o nome dela: Requisição POST /api/v1/collections { // O nome da coleção não pode ser vazio. \"collectionName\": \"Minha primeira coleção\" } Resposta { \"message\": null, \"data\": { // ID único da coleção criada. \"collectionId\": \"01965b62-17c4-7258-9aa8-af5139799527\" } } Listar coleções Lista as coleções disponíveis na sua conta. Requisição GET /api/v1/collections Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01965b62-17c4-7258-9aa8-af5139799527\", \"createdAt\": \"2025-04-22T02:44:37\", \"name\": \"Minha coleção\" }, { \"id\": \"01965b54-7fbd-70cd-982b-604de002ac0a\", \"createdAt\": \"2025-04-22T02:29:46\", \"name\": \"Outra coleção\" } ] } } Ver uma coleção Obtém detalhes de uma coleção, como seu progresso de indexação e informações como data de criação. Requisição GET /api/v1/collections/{collection-id}/ Resposta { \"message\": null, \"data\": { \"name\": \"Minha coleção\", \"createdAt\": \"2025-04-22T02:29:46\", \"state\": { // traz a quantidade de documentos aguardando indexação \"queuedDocuments\": 0, // quantidade de documentos pronto para consulta \"indexedDocuments\": 227 }, \"tags\": [ \"tag1\", \"tag2\", \"tag3\", ... ] } } Excluir uma coleção Exclui uma coleção e todos os documentos nela. Essa ação é irreversível. Requisição DELETE /api/v1/collections/{collection-id}/ Resposta { \"message\": \"Collection deleted successfully.\", \"data\": null } Limpar uma coleção Diferente da exclusão de coleção, essa operação remove todos os documentos da coleção, incluindo os indexados e os em fila. Requisição DELETE /api/v1/collections/{collection-id}/reset-only Resposta { \"message\": \"Collection cleaned successfully.\", \"data\": null }"
  },
  "docs/entities/documents.html": {
    "href": "docs/entities/documents.html",
    "title": "Documentos | AIVAX",
    "keywords": "Documentos Um documento representa um pedaço de um conhecimento. É um trecho limitado, autosuficiente e que faça sentido de forma isolada. Um documento é o componente que é indexado pelo modelo interno para ser recuperado posteriormente através de um termo de busca semântico. Considere um manual sobre um carro: ele não é um documento mas sim vários documentos. Cada um destes documentos fala, de forma isolada, sobre um determinado assunto sobre esse carro, de forma que esse documento não dependa de um contexto ou informação externa para fazer sentido. Cada documento deste manual irá falar de um assunto: um irá falar sobre como ligar o carro, outro de como desligá-lo, outro de como sua pintura é feita e outro de como trocar o óleo periodicamente. Não é uma boa ideia reservar um documento para falar de várias coisas ao mesmo tempo, pois isso irá reduzir a objetividade e escopo da inferência e reduzir a qualidade de obtenção. Exemplos de criação de documentos: ❌ Não faça Não crie documentos muito curtos (com 10 ou menos palavras). Não crie documentos muito grandes (com 700) ou mais palavras. Não fale sobre mais de uma coisa em um documento. Não misture linguas diferentes em documentos. Não seja implícito em documentos. Nâo escreva documentos usando linguagem técnica, como códigos ou estruturas como JSON. ✅ Faça Seja explícito sobre o objetivo do seu documento. Foque documentos em assuntos individuais, que resumam o que deve ser feito ou explicado. Sempre repita termos que são palavras-chave para a busca do documento. Exemplo: prefira usar \"A cor do Honda Civic 2015 é amarela\" ao invés de \"a cor do carro é amarelo\". Restrinja o conteúdo do documento para falar de apenas um tópico ou assunto. Use uma linguagem humana, simples e fácil de entender. Uso da API Como todos os documentos são entidades que pertencem à uma coleção, sempre tenha em mãos a coleção de onde o documento está/será localizado. Enviar documentos em lote Para enviar uma lista em massa de documentos para uma coleção, estruture-os seguindo o formato JSONL. A estrutura do arquivo de indexação é: {\"docid\":\"Carros/HondaCivic2015.rmd:1\",\"text\":\"O Honda Civic 2015 está disponível em [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:2\",\"text\":\"O motor do Honda Civic 2015 é [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} {\"docid\":\"Carros/HondaCivic2015.rmd:3\",\"text\":\"A cor do Honda Civic 2015 é Amarela [...]\",\"__ref\":null,\"__tags\":[\"Carros\",\"Honda-Civic-2015\"]} ... A estrutura é compsta pelas propriedades: Propriedade Tipo Descrição docid string Especifica o nome do documento. Útil para depuração e identificação. text string O conteúdo \"cru\" do documento que será indexado. __ref string Opcional. Especifica um ID de referência do documento. __tags string[] Opcional. Especifica um array de tags do documento. Útil para gestão de documentos. A referência de um documento é um ID que pode ser especificado em vários documentos que precisam estar vinculados em uma busca quando um dos mesmos for correspondido em uma busca de similaridade. Por exemplo, se uma busca encontrar um documento que possui um ID de referência, todos os outros documentos da mesma coleção que compartilham o mesmo ID de referência do documento correspondido também serão incluídos na resposta da busca. O uso de referências pode ser útil para quando um documento depende de outro ou mais documentos para fazer sentido. Não há exigência de formato para o ID de referência: qualquer formato é aceito. Você pode enviar até 1.000 linhas de documentos por requisição. Se precisar enviar mais documentos, separe o envio em mais requisições. Se você enviar um documento com mais de 1.000 linhas, as linhas seguintes serão ignoradas. Vale notar que documentos muito longos, que excede a quantidade de tokens permitida no modelo de embedding interno, terão seu conteúdo truncado e a qualidade de indexação poderá ser gravemente afetada. Para evitar esse problema, envie documentos que contenham entre 20 e 700 palavras. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do conteúdo de cada documento. O conteúdo de cada documento é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição O envio deve ser feito usando multipart form data. POST /api/v1/collections/{collection-id}/documents documents=[documents.jsonl] Resposta { \"message\": null, \"data\": [ { \"name\": \"Institucional/Empresa.rmd:1\", \"documentId\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\" }, { \"name\": \"Institucional/Empresa.rmd:2\", \"documentId\": \"01965f93-a390-79d3-9b3d-338d407f6b64\" }, { \"name\": \"Institucional/Empresa.rmd:3\", \"documentId\": \"01965f93-a391-79ef-adcf-737d98303a78\" }, { \"name\": \"Produtos/Agendamentos.rmd:1\", \"documentId\": \"01965f93-a391-712e-9292-c4d8e010bf42\" }, ... ] } Criar ou modificar documento Esse endpoint cria ou modifica um documento a partir do seu nome. Quando um documento é modificado, seus vetores de indexação são resetados, isto é, o documento entrará em fila novamente para ser indexado pelo motor de indexação. Essa indexação não é isenta de custo. O custo é relativo à quantidade de tokens do conteúdo enviado. O custo somente é gerado quando o documento é de fato alterado. Chamar essa rota com o mesmo conteúdo do documento não gera modificação, portanto, não gera custo. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do conteúdo do arquivo. O conteúdo do arquivo é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição PUT /api/v1/collections/{collection-id}/documents { // o nome do documento que será modificado \"name\": \"document-name\", // o conteúdo do documento que será criado ou sobreposto caso o nome já exista \"contents\": \"Conteúdo do meu documento\", // parâmetros explicados anteriormente \"reference\": null, \"tags\": [\"products\", \"my-product\"] } Resposta { \"message\": null, \"data\": { \"documentId\": \"0196663c-3a15-72c7-98e6-b496f8e8bb8c\", // o estado da operação indica se o documento foi modificado \"Modified\" ou criado \"Created\". \"state\": \"Modified\" } } Listar documentos Esse endpoint lista todos os documentos disponíveis em uma coleção. Você pode passar um parâmetro da query adicional filter para filtrar documentos por nome, tag ou conteúdo. Esse filtro suporta expressões que auxiliam a filtrar o que você está procurando: -t \"tag\" - filtra documentos que possuem essa tag. -r \"reference\" - filtra documentos que possuem esse ID de referência. -c \"content\" - filtra documentos que possuem esse trecho em seu conteúdo. -n \"name\" - filtra documentos que possuem esse trecho em seu nome. Requisição GET /api/v1/collections/{collection-id}/documents Resposta { \"message\": null, \"data\": { \"pageInfo\": { \"currentPage\": 1, \"hasMoreItems\": true }, \"items\": [ { \"id\": \"01968452-69f6-7f00-a497-d14c5b906b79\", \"name\": \"Ajuda/Clientes.rmd:1\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"Um cliente é um cadastro na sua platafor...\", \"indexState\": \"Indexed\" }, { \"id\": \"01968452-6a53-7ce3-adad-fad32d508856\", \"name\": \"Ajuda/Clientes.rmd:2\", \"reference\": null, \"tags\": [ \"Ajuda\", \"Clientes\" ], \"contentsPreview\": \"No cadastro do cliente, é possível modif...\", \"indexState\": \"Indexed\" }, ... ] } } Ver documento Vê detalhes sobre um documento específico. Requisição GET /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": null, \"data\": { \"id\": \"01965f93-a36b-7fc2-9e6a-c733f4955927\", \"name\": \"Institucional/Empresa.rmd:1\", // representa a situação de indexação do documento. // valores válidos: Queud, Indexed, Cancelled \"state\": \"Indexed\", // conteúdo do documento indexado \"contents\": \"...\", // id da referência do documento \"reference\": \"institucional-empresa\" } } Excluir documento Permanentemente exclui um documento através do seu ID. Requisição DELETE /api/v1/collections/{collection-id}/documents/{document-id} Resposta { \"message\": \"Document removed.\", \"data\": null }"
  },
  "docs/entities/functions.html": {
    "href": "docs/entities/functions.html",
    "title": "Funções | AIVAX",
    "keywords": "Funções Funções é uma forma de forçar seu modelo à processamento de informações usando JSON como intermédio de comunicação. Com as funções, você consegue fazer qualquer modelo responder no formato JSON que você quiser. Pode ser útil para categorizar comentários, aplicar moderação em avaliações ou processar informações com auxílio da IA. No momento, só é possível usar funções com modelos providos pela Open Indexer. Chamar uma função Para chamar uma função de IA, você precisará informar o que a IA deverá responder e fornecer um esquema JSON que ela deverá seguir. Modelos menos inteligentes tendem a falhar a geração de JSON, gerando um documento inválido ou problemático. Para isso, ajuste seu modelo, a instrução e o parâmetro de tentativas se for necessário. Você é cobrado por cada tentativa que a IA tentar gerar. Modelos um pouco mais inteligentes tendem a gerar resultados corretos na primeira tentativa. É garantido que um JSON válido será gerado e que esse JSON seguirá o mesmo esquema fornecido na requisição. Adicionalmente, você pode optar em ativar pesquisa na internet para chamada de função. Essa opção pode ser útil para trazer dados relevantes em tempo real ao estruturar sua resposta. Ao usar essa função, um modelo com acesso na internet será usado para obter dados da internet para estruturar sua resposta. Esse modelo também tentará estruturar sua resposta a partir dos dados fornecidos, e se conseguir formular um JSON válido a etapa de chamar o modelo de estruturação é ignorada e a resposta é imediatamente retornada. Se for o caso do modelo de busca online não conseguir estruturar um JSON válido, o modelo escolhido na requisição ficará responsável por essa tarefa, e irá começar o encadeamento de tentativas de geração. Modelos mais inteligentes acertam a geração nas primeiras tentativas. Através da propriedade fetch, você pode fornecer uma lista de URLs para serem anexadas no contexto da geração. O Open Indexer faz uma requisição GET para acessar os conteúdos fornecidos e renderiza-os no conteúdo da requisição. Somente respostas 2xx ou 3xx são aceitas e o conteúdo da resposta deve ser textual. Respostas em HTML são sanitizadas para incluirem somente o texto da página, sem script e CSS. O tamanho máximo que pode ser lido de uma URL do fetch é 10 Mb. O máximo de itens para o fetch são 10 URLs. Retentativas de geração do conteúdo JSON não pesquisam na internet novamente nem chamam o conteúdo do fetch. Requisições que pesquisam na internet trazem bons resultados e dispensam crawlers, scrappers ou a necessidade de pagar por uma API específica, mas podem ser custosas e relativamente lentas para serem obtidas. Considere usar um cache do lado da sua aplicação para dados que não precisam ser constantementes atualizados, como dados meteorológicos, estatísticas diárias, etc. A Open Indexer não realiza nenhum cache pelo nosso lado. Requisição POST /api/v1/functions/json { // Obrigatório. Especifique o nome do modelo integrado que será usado para realizar a ação. \"modelName\": \"@metaai/llama-3.1-8b\", // Obrigatório. Explique o que seu modelo deverá fazer com a entrada e como ele deve trazer a resposta. \"instructions\": \"Classifique o comentário do usuário, indicando se é positivo ou negativo, e se possui alguma informação relevante (número entre 0 (pouco relevante) e 10 (muito relevante))\", // Obrigatório. O objeto JSON que o modelo deverá gerar. Você pode fornecer exemplos de geração no campo de instruções. Esse objeto deve ser um JSON válido na API. // Esse objeto deve ser um objeto, um array ou uma string. \"responseSchema\": { \"feedbackType\": \"{neutral|positive|negative}\", \"informationScore\": 5 }, // Opcional. Especifica uma lista de caminhos JSON que a IA deve gerar conteúdo sempre e que esse campo não pode ser nulo. Para arrays, especifique com [*]. \"requiredFields\": [ \"$.feedbackType\", \"$.informationScore\" ], // Opcional. Define uma entrada JSON para o modelo. Pode ser qualquer tipo de valor JSON. \"inputData\": { \"userComment\": \"Pessimo mercado. Tem guarda dentro te vigiando pra vc nao roubar e os acougueiros te ignoram e atendem mocinhas bonitinhas na tua frente. Mas graças a Deus tem outros mercados chegando e o fim dessa palhaçada vai chegar\" }, // Opcional. Define quantas tentativas o modelo deve tentar antes da API retornar um erro. Deve ser um número entre 1 e 30. \"maxAttempts\": 10, // Opcional. Define o tempo limite em segundos para obter um JSON válido antes da API retornar um erro. Deve ser um número entre 1 e 3600 (uma hora). \"timeout\": 300, // Opcional. Permite que o modelo faça uma busca na internet para aperfeiçoar a construção da resposta. \"webSearch\": { // Obrigatório. Ativa ou desativa a pesquisa na internet da função. \"enabled\": true }, // Opcional. Adiciona recursos externos para complementar a geração da resposta. \"fetch\": { // Obrigatório. Fornece a lista de URLS que o Open Indexer irá acessar. O máximo são 10 URLs. \"urls\": [ \"https://url1...\", \"https://url2...\", ], // Opcional. Define o comportamento do fetch para erros ao tentar acessar o site. Erros incluem respostas que não são 2xx ou 3xx, timeouts, erros de certificados, etc. // fail -> retorna um erro na resposta da função (padrão) // warn -> adiciona um aviso na resposta da função e não inclui o erro na geração da IA // ignore -> ignora o erro e adiciona o erro na geração da IA \"fetchFailAction\": \"fail\" | \"warn\" | \"ignore\", // Opcional. Define o timeout em segundos para o tempo maximo da resposta responder e ler os conteúdos. O máximo é 120 segundos (dois minutos). \"timeout\": 10, // Opcional. Define o tamanho máximo do conteúdo em quantidade de caracteres que podem ser incluídos na geração da IA antes de serem truncados. \"pageMaxLength\": 2048 } } Resposta { \"message\": null, \"data\": { // o resultado contém o objeto definido em \"responseSchema\", com os campos preenchidos pela IA \"result\": { \"feedbackType\": \"negative\", \"informationScore\": 8 }, // em qual tentativa a IA conseguiu um JSON válido \"attempt\": 1, // o tempo em milissegundos para obter um JSON válido \"elapsedMilliseconds\": 527, // avisos produzidos pela geração \"warnings\": [] } } Considerações sobre o esquema JSON Especifique valores enumerados com \"{valor1|valor2|valor3}\". Dessa forma, o modelo deverá escolher um dos valores apresentados na geração do JSON. Todos os valores são placeholders para a geração do modelo. Indique o que um campo é ou o que deve receber de valor com um hint em seu próprio placeholder ou indique diretamente nas instruções da função. Todos os valores podem ser nulos, à menos que você especifique diretamente para o modelo que não podem. A estrutura de saída do modelo é a mesma que informada em responseSchema. A estrutura de entrada é indiferente. Exemplos Confira exemplos de funções de IA para várias tarefas cotidianas: Resumir pedido e classificar se requer atenção ou não POST /api/v1/functions/json { \"modelName\": \"@metaai/llama-4-scout-17b\", \"instructions\": \"Resuma o comentário do usuário, criando uma descrição curta, com no máximo de 10 palavras indicando o que ele quer fazer. Indique também se esse comentário requer atenção ou não.\", \"responseSchema\": { \"shortSummary\": \"...\", \"requiresAttention\": false }, \"inputData\": \"O cliente fernando de castro está tentando entrar em contato com o suporte desde sexta-feira e diz q vai cancelar se nao falar com alguém hoje. ele tbm disse que é amigo da rebeca do comercial e está ameaçando falar mal da empresa no tiktok. por favor alguém atende esse cara??\" } { \"message\": null, \"data\": { \"result\": { \"shortSummary\": \"Cliente quer contato com suporte para evitar cancelamento e ameaça\", \"requiresAttention\": true }, \"attempt\": 1, \"elapsedMilliseconds\": 639 } } Trazer últimas notícias e clima de uma determinada cidade POST /api/v1/functions/json { \"modelName\": \"@google/gemini-2.0-flash-lite\", \"instructions\": \"Pesquise as 5 últimas notícias e dados meteorológicos para a cidade informada.\", \"responseSchema\": { \"latestNews\": [ { \"title\": \"...\", \"details\": \"...\", \"link\": \"https://...\" } ], \"weather\": { \"currentTemperature\": 0, \"currentWeather\": \"{sunny|cloudy|rain|thunderstorm}\", \"forecast\": \"{sunny|cloudy|rain|thunderstorm}\" } }, \"inputData\": { \"city\": \"São José do Rio Preto\" }, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"latestNews\": [ { \"title\": \"GCM prende trio por tráfico de drogas no Calçadão de Rio Preto\", \"details\": \"A Guarda Civil Municipal (GCM) de São José do Rio Preto prendeu, na noite desta terça-feira (6), três pessoas suspeitas de tráfico de drogas no Calçadão [4, 9].\", \"link\": \"https://dhoje.com.br/gcm-prende-trio-por-trafico-de-drogas-no-calcadao-de-rio-preto/\" }, { \"title\": \"Emprego Apoiado faz seleção para pessoas com deficiência\", \"details\": \"Distribuidora de bebidas de Rio Preto faz seleção nesta quinta-feira, 8/5, das 9h às 11h [4, 9].\", \"link\": \"https://dhoje.com.br/emprego-apoiado-faz-selecao-para-pessoas-com-deficiencia/\" }, { \"title\": \"Fundo Social distribui Enxoval do Amor para gestantes em Rio Preto\", \"details\": \"Campanha do Fundo Social mobiliza voluntárias e população para apoiar mães em Rio Preto [4].\", \"link\": \"https://dhoje.com.br/fundo-social-distribui-enxoval-do-amor-para-gestantes-em-rio-preto/\" }, { \"title\": \"PM abre 2,2 mil vagas para policiais da reserva atuarem\", \"details\": \"A Polícia Militar de São Paulo publicou, nesta terça-feira, 6, edital para contratação de 2.200 policiais militares da reserva para exercerem [4].\", \"link\": \"https://dhoje.com.br/pm-abre-22-mil-vagas-para-policiais-da-reserva-atuarem/\" }, { \"title\": \"Rio Preto pode receber R$ 63,8 milhões do Governo do Estado de São Paulo para obras de melhoria\", \"details\": \"Prefeito busca R$ 63,8 milhões para viabilizar pacotaço de obras [4].\", \"link\": \"https://dhoje.com.br/infraestrutura-prefeito-busca-r-638-milhoes-para-viabilizar-pacotaco-de-obras/\" } ], \"weather\": { \"currentTemperature\": 18, \"currentWeather\": \"sunny\", \"forecast\": \"sunny\" } }, \"attempt\": 1, \"elapsedMilliseconds\": 4187 } } Trazer estatísticas da COVID-19 em tempo real POST /api/v1/functions/json { \"modelName\": \"@google/gemini-1.5-flash-8b\", \"instructions\": \"Traga a contagem de casos e mortes por COVID-19.\", \"responseSchema\": { \"deathsWorld\": 0, \"deathsBrazil\": 0, \"casesWorld\": 0, \"casesBrazil\": 0 }, \"inputData\": null, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"deathsWorld\": 7010681, \"deathsBrazil\": 711380, \"casesWorld\": 704753890, \"casesBrazil\": 38743918 }, \"attempt\": 1, \"elapsedMilliseconds\": 1620 } } Trazer artistas em alta por gênero musical POST /api/v1/functions/json { \"modelName\": \"@google/gemini-1.5-flash-8b\", \"instructions\": \"Pesquise e formate uma lista de 10 artistas no TOP 10 do streaming musical por gênero.\", \"responseSchema\": { \"edm\": [ \"artist name\", \"artist name\", \"...\" ], \"rap\": [ \"artist name\", \"artist name\", \"...\" ], \"pop\": [ \"artist name\", \"artist name\", \"...\" ] }, \"inputData\": null, \"webSearch\": { \"enabled\": true } } { \"message\": null, \"data\": { \"result\": { \"edm\": [ \"David Guetta\", \"Calvin Harris\", \"The Chainsmokers\", \"Marshmello\", \"Avicii\", \"Kygo\", \"Tiesto\", \"DJ Snake\", \"Daft Punk\", \"Skrillex\" ], \"rap\": [ \"Drake\", \"Eminem\", \"Kanye West\", \"Juice WRLD\", \"Travis Scott\", \"XXXTENTACION\", \"Kendrick Lamar\", \"Future\", \"J. Cole\", \"Nicki Minaj\" ], \"pop\": [ \"Taylor Swift\", \"Drake\", \"Bad Bunny\", \"The Weeknd\", \"Ed Sheeran\", \"Ariana Grande\", \"Justin Bieber\", \"Billie Eilish\", \"Rihanna\", \"Bruno Mars\" ] }, \"attempt\": 1, \"elapsedMilliseconds\": 8370 } }"
  },
  "docs/entities/search.html": {
    "href": "docs/entities/search.html",
    "title": "Pesquisa | AIVAX",
    "keywords": "Pesquisa A API de pesquisa, através da query key obtida das coleções, realiza uma busca semântica na mesma, realizando uma comparação inteligente para cada documento indexado em uma coleção. Após criar uma coleção, você obterá seu ID. Utilize o ID da sua coleção para realizar a busca nos documentos indexados da mesma. Use os endpoints dessa API para embutir a pesquisa semântica de documentos no seu modelo de IA ou chatbot. Pesquisando documentos Esse endpoint espera uma requisição GET com os parâmetros: term: obrigatório. Especifica o termo de pesquisa que será pesquisado nos documentos. top: Especifica o máximo de documentos que deverão ser retornados na busca. min: Especifica o score mínimo para obtenção dos documentos. Warning Atenção: esse endpoint gera custo. O custo é calculado em cima dos tokens do termo de busca. O termo de busca é tokenizado de acordo com o modelo usado na indexação dos documentos. Requisição GET /api/v1/collections/{collection-id}/query term=Qual a cor do honda CIVIC? Resposta { \"message\": null, \"data\": [ { \"documentId\": \"01965f93-a391-71a8-968a-47ccd4949de0\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:1\", \"documentContent\": \"[...]\", \"score\": 0.7972834229469299, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-76b3-bbf5-3fb74d10d412\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:2\", \"documentContent\": \"[...]\", \"score\": 0.5693517327308655, \"referencedDocuments\": [] }, { \"documentId\": \"01965f93-a391-7026-b7aa-1cc6c63cd7d1\", \"documentName\": \"Produtos/Honda Civic 2015.rmd:5\", \"documentContent\": \"[...]\", \"score\": 0.5475733876228333, \"referencedDocuments\": [] }, ... ] } Para o resultado da busca, quanto maior o score, mais semelhante é o documento para o termo da busca. O Open Indexer utiliza modelos de embedding que permitem a orientação da tarefa. Para a busca, o termo é vetorizado com uma orientação DOCUMENT_QUERY. Para indexação dos documentos, a orientação é DOCUMENT_RETRIEVAL, o que fornece uma busca mais otimizada e não para averiguar a similaridade entre documentos."
  },
  "docs/getting-started.html": {
    "href": "docs/getting-started.html",
    "title": "Bem-vindo | AIVAX",
    "keywords": "Bem-vindo Boas vindas ao Open Indexer. Nosso serviço torna mais fácil o desenvolvimento de modelos de IA inteligentes que usam uma base de conhecimento providenciada por você para conversar com o usuário, responder perguntas, fornecer informações em tempo real e mais. Para começar, todos os endpoints devem ser feitos na URL de produção da AIVAX: https://inference.aivax.net/ Conceitos e definições Entenda os conceitos usados pela API abaixo: Conta: representa uma conta do usuário, que possui um token de autenticação. Coleção: representa uma coleção de documentos de conhecimento. Um usuário pode ter várias coleções de documentos. Documento: representa um fato, um único conhecimento e um item de uma coleção. Uma coleção pode ter vários documentos. AI Gateway: representa um gateway de IA que se beneficia ou não de uma coleção de conhecimento, como um middleware de conhecimento plug-and-play para um modelo. Modelo embutido: representa um modelo de IA que o Open Indexer provê para o usuário. Chat client: representa uma interface de usuário que disponibiliza o AI gateway através de um chat interagível online. Sessão de chat: abriga uma conversa e contexto de um cliente de chat. Lidando com erros Todos os erros da API retornam uma resposta HTTP com um status não OK (nunca 2xx ou 3xx), e sempre seguindo o formato JSON: { \"error\": \"Uma mensagem explicando o erro\", \"details\": {} // um objeto contendo informações relevantes sobre o erro. Na maioria das vezes é nulo }"
  },
  "docs/limits.html": {
    "href": "docs/limits.html",
    "title": "Limites da API | AIVAX",
    "keywords": "Limites da API Limites de taxa (\"rate limiters\") regulam o número de requisições que você pode enviar em uma janela de tempo. Esses limites ajudam a Open Indexer a prevenir abuso e fornecer uma API estável à todos. Os limites da API abaixo são os mesmos para todos os modelos embutidos da AIVAX. Esses limites são categorizados por operações feitas pela API. Cada conta possui um tier que define quais limites são aplicados à conta. Tiers mudam de acordo com o total investido na Open Indexer e o tempo que a conta existe. Tier zero (conta grátis): conta nova que nunca adicionou créditos. Tier 1: conta criada há pelo menos 48 horas e que já adicionou qualquer valor em créditos. Tier 2: conta criada há pelo menos 1 mês e que já adicionou pelo menos $ 100 em créditos. Tier 3: conta criada há pelo menos 3 meses e que já adicionou pelo menos $ 1.000 em créditos. A medição é pela adição de créditos e não pelo seu consumo. Por exemplo, você não consumir $ 100 em créditos para avançar ao Tier 2. Legendas dos limites: RPM: requisições por minuto. RPD: requisições por dia (24 horas). Grátis Tier 1 Tier 2 Tier 3 Operação RPM RPD Pesquisa de documentos 50 - Inserção de documentos - 100 Inferência 5 500 Função 5 500 Função (Live) 3 30 Operação RPM RPD Pesquisa de documentos 150 - Inserção de documentos - 3.000 Inferência 75 1.000 Função 50 1.000 Função (Live) 5 200 Operação RPM RPD Pesquisa de documentos 300 - Inserção de documentos - 10.000 Inferência 150 - Função 50 10.000 Função (Live) 10 400 Operação RPM RPD Pesquisa de documentos 1.000 - Inserção de documentos - 30.000 Inferência 1.000 - Função 750 - Função (Live) 200 - Pesquisa de documentos: inclui pesquisa semântica de documentos em uma coleção pelo endpoint de pesquisa ../collections/{id}/query. Inserção de documentos: inclui criação e modificação de documentos em uma coleção. Inferência: toda chamada de inferência, seja pela API Open-AI compatível, pela rota /ai-gateways/{id}/inference ou por cada mensagem enviada por uma sessão de cliente de chat. Função: toda chamada de função /functions. Função (Live): toda chamada de função conectada à internet pelo parâmetro webSearch. Não inclui chamadas fetch."
  },
  "docs/models.html": {
    "href": "docs/models.html",
    "title": "Modelos | AIVAX",
    "keywords": "Modelos A AIVAX provê modelos de diferentes provedores para tornar o desenvolvimento ainda mais rápido, dispensando a necessidade de ter que configurar uma conta para cada provedor para ter acessos aos seus modelos mais recentes. Veja a lista abaixo dos modelos disponíveis e suas precificações. Todos os preços consideram o total de entrada e saída de tokens, com ou sem cache. Todos os preços estão em dólares dos Estados Unidos. aivax Nome do modelo Preços Descrição @aivax/sentinel Entrada + saída: $ 1.31 /1M. tokens Pesquisa na internet: $ 5.60 /1.000 pesquisas Raciocínio Sentinel: $ 1.31 /1m tokens Highly intelligent and optimized to perform challenging tasks and solve complex problems. Entrada: aceita imagens Chamadas de função Raciocínio Pesquisa na internet Acessar links Execução de código Memória persistente @aivax/sentinel-mini Entrada + saída: $ 0.88 /1M. tokens Pesquisa na internet: $ 5.60 /1.000 pesquisas Raciocínio Sentinel: $ 1.31 /1m tokens Highly capable model, with reasoning power and complex problem-solving. Entrada: aceita imagens Chamadas de função Raciocínio Pesquisa na internet Acessar links Execução de código Memória persistente @aivax/sentinel-router Entrada + saída: $ 0.66 /1M. tokens Sentinel's router, which automatically selects the best Sentinel model to perform a task according to its complexity. Entrada: aceita imagens Chamadas de função Raciocínio Pesquisa na internet Acessar links Execução de código cognitivecomputations Nome do modelo Preços Descrição @cognitivecomputations/dolphin2.9 Entrada + saída: $ 2.60 /1M. tokens An experimental model based on Mistral that is completely uncensored. Use responsibly. Chamadas de função Funções JSON deepseekai Nome do modelo Preços Descrição @deepseekai/r1-distill-llama-70b Entrada + saída: $ 2.76 /1M. tokens Model with deep reasoning and thought, best for more demanding tasks. Chamadas de função Raciocínio google Nome do modelo Preços Descrição @google/gemini-2.5-pro Entrada + saída: $ 12.25 /1M. tokens One of the most powerful models today. Entrada: aceita imagens, vídeos, áudios Chamadas de função Raciocínio Funções JSON @google/gemini-2.5-flash-th Entrada + saída: $ 4.65 /1M. tokens Latest generation model with integrated reasoning and thought. Entrada: aceita imagens, vídeos, áudios Chamadas de função Raciocínio Funções JSON @google/gemini-2.5-flash Entrada + saída: $ 1.31 /1M. tokens Same model as Gemini 2.5 Flash, but with the reasoning module turned off. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-2.5-flash-lv Entrada + saída: $ 1.31 /1M. tokens Pesquisa na internet: $ 39.20 /1.000 pesquisas Version of Gemini 2.5 Flash with internet search. Entrada: aceita imagens, vídeos, áudios Pesquisa na internet @google/gemini-2.0-flash Entrada + saída: $ 0.88 /1M. tokens Offers new generation features, with improved speed and multi-modal generation. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-2.0-flash-lv Entrada + saída: $ 0.88 /1M. tokens Pesquisa na internet: $ 39.20 /1.000 pesquisas Version of Gemini 2.0 with internet search via Google. Entrada: aceita imagens, vídeos, áudios Pesquisa na internet @google/gemini-2.0-flash-lite Entrada + saída: $ 0.66 /1M. tokens General-purpose model, with image recognition, smart and fast. Great for an economical chat. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON @google/gemini-1.5-flash-8b Entrada + saída: $ 0.33 /1M. tokens Previous generation general-purpose model, optimized for less demanding and simple tasks. Entrada: aceita imagens, vídeos, áudios Chamadas de função Funções JSON metaai Nome do modelo Preços Descrição @metaai/llama-3.3-70b Entrada + saída: $ 2.40 /1M. tokens Previous generation model with many parameters and surprisingly fast speed. Chamadas de função Funções JSON @metaai/llama-4-maverick-17b-128e Entrada + saída: $ 1.40 /1M. tokens Fast model, with 17 billion activated parameters and 128 experts. Entrada: aceita imagens Chamadas de função Funções JSON @metaai/llama-4-scout-17b-16e Entrada + saída: $ 0.79 /1M. tokens Smaller version of the Llama 4 family with 17 billion activated parameters and 16 experts. Entrada: aceita imagens Chamadas de função Funções JSON @metaai/llama-3.1-8b Entrada + saída: $ 0.23 /1M. tokens Cheap and fast model for less demanding tasks. Chamadas de função Funções JSON openai Nome do modelo Preços Descrição @openai/gpt-4o Entrada + saída: $ 13.50 /1M. tokens Dedicated to tasks requiring reasoning for mathematical and logical problem solving. Entrada: aceita imagens Chamadas de função Funções JSON @openai/gpt-4.1 Entrada + saída: $ 11.00 /1M. tokens Versatile, highly intelligent, and top-of-the-line. One of the most capable models currently available. Entrada: aceita imagens Chamadas de função Funções JSON @openai/o4-mini Entrada + saída: $ 6.50 /1M. tokens Optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. Entrada: aceita imagens Raciocínio Funções JSON @openai/gpt-4.1-mini Entrada + saída: $ 3.00 /1M. tokens Fast and cheap for focused tasks. Entrada: aceita imagens Chamadas de função Funções JSON @openai/gpt-4o-mini Entrada + saída: $ 1.31 /1M. tokens Smaller version of 4o, optimized for everyday tasks. Entrada: aceita imagens Chamadas de função Funções JSON @openai/gpt-4.1-nano Entrada + saída: $ 0.88 /1M. tokens The fastest and cheapest GPT 4.1 model. Entrada: aceita imagens Chamadas de função Funções JSON qwen Nome do modelo Preços Descrição @qwen/qwq-32b Entrada + saída: $ 1.23 /1M. tokens Conversational model with thinking and reasoning for solving complex tasks. Chamadas de função Raciocínio"
  },
  "docs/protocol-functions.html": {
    "href": "docs/protocol-functions.html",
    "title": "Funções de protocolo | AIVAX",
    "keywords": "Funções de protocolo As funções de protocolo da AIVAX, ou server-side functions, é uma implementação customizada de chamadas de função criada pela AIVAX que permite que o modelo siga estritamente um contexto de função que não é baseada em documentos JSON. Esse recurso é disponível para modelos com raciocínio Sentinel. As funções de protocolo permitem a tomada de ações no lado do servidor da AIVAX, removendo a necessidade de implementação da função no lado do cliente e integrando com aplicações e serviços existentes. Essas funções esperam um callback através de uma URL, e quando o modelo decide chamar a função, o callback é acessado com os parâmetros informados pela própria assistente. A assistente não sabe qual URL ela está chamando, pois a mesma permanece invisível tanto para a assistente quanto para o usuário. Definindo funções de protocolo Essas funções são definidas no AI-gateway, o que permite a criação de agentes inteligentes que realizam ações sem intervenção humana. Elas seguem uma sintaxe simples, esperam o nome da função, uma descrição do que ela faz e os parâmetros de invocação. Escolhendo o nome da função O nome da função deve ser simples e determinístico ao que essa função faz. Evite nomes difíceis de advinhar ou que não remetam ao papel da função, pois a assistente pode se confundir e não chamar a função quando apropriado. Como um exemplo, vamos pensar em uma função de consultar um usuário em um banco de dados externo. Os nomes a seguir são bons exemplos para considerar para a chamada: search-user query-user search_user Nomes ruins incluem: search (pouco abrangente) query-user-in-database-data (nome muito grande) pesquisa-usuario (nome não em inglês) search user (nome com caracteres impróprios) Tendo o nome da função, podemos pensar na descrição da função. Escolhendo a descrição da função. A descrição da função deve explicar conceitualmente duas situações: o que ela faz e quando deve ser chamada pela assistente. Essa descrição deve incluir os cenários que a assistente deve considerar chamar ela e quando não deve ser chamada, fornecendo poucos exemplos de chamadas (one-shot) e/ou tornando explícitas as regras da função. Criando o endpoint que a função chamará Criando a definição da função"
  },
  "docs/sentinel.html": {
    "href": "docs/sentinel.html",
    "title": "Agentes Sentinel | AIVAX",
    "keywords": "Agentes Sentinel Os agentes de chat Sentinel são modelos com parâmetros otimizados para conversação de chat em tempo real, com instruções que aprimoram a atenção do modelo para determinadas tarefas e permite o uso de pesquisação na internet em tempo real. Esses modelos são constantemente atualizados para utilizar as últimas tecnologias e recursos disponíveis por IA generativas, portanto, os modelos internos usados pelos modelos Sentinel podem ser constantemente atualizados. Atualmente, os modelos Sentinel estão disponíveis em três categorias: sentinel: altamente inteligente e otimizado para realizar tarefas desafiadoras e resolver problemas complexos. sentinel-mini: modelo altamente capaz, otimizado para custo, com poder de raciocínio e solução de problemas complexos. Sempre que um modelo Sentinel for ter seu preço alterado, uma notificação é enviada para todos os usuários que utilizam o modelo e uma notificação é adicionada na página de notificações. Estes modelos devem ser utilizados para conversação com o usuário final e não são recomendados para demais tarefas como uso de funções, respostas estruturadas e etc. Principais recursos O diferencial dos modelos Sentinel são os recursos providos \"out-of-the-box\" por ele, dentre eles: Otimizado para chat: os modelos Sentinel possuem instruções claras que faz o modelo adaptar o tom de conversa ao mesmo do usuário, indicativos de humor e objetivo. Raciocínio: todos os modelos Sentinel possuem um mecanismo pensamento profundo personalizado, otimizado para raciocinar com base em suas funções fornecidas, inclusive funções de protocolo, para fornecer uma resposta detalhada ao usuário. Pesquisa na internet: os modelos Sentinel conseguem naturalmente pesquisar na internet para complementar sua resposta em diferentes cenários, como obter notícias locais, dados meteorológicos ou dados sobre um assunto ou nicho específico. Acessar links: os modelos Sentinel conseguem acessar arquivos e páginas fornecidas pelo usuário. Ele consegue acessar páginas HTML, vários arquivos textuais, até mesmo documentos do Word e PDF. Execução de código: os modelos Sentinel conseguem executar código para realizarem cálculos matemáticos, financeiros ou até mesmo ajudar o usuário com diversas tarefas. Memória persistente: os modelos Sentinel conseguem identificar fatos relevantes ao usuário que devem ser persistidos durante várias conversas, até mesmo após a conversa atual ser limpada ou renovada. Além disso, todos os modelos Sentinel possuem uma cadeia de execução das funções. Por exemplo, você pode pedir para um modelo Sentinel acessar a temperatura de uma cidade, converter para JSON e chamar uma URL externa com a resposta. Note Importante: o contexto do Sentinel inclui uma descrição detalhada de seu comportamento e suas funções e ocupa aproximadamente ~1.300 tokens que serão anexados em todas as mensagens enviadas pelo usuário. Sentinel Router Você também pode usar o modelo de roteamento sentinel-router que funciona como um modelo de roteador entre os modelos Sentinel. Um roteador automaticamente escolhe qual o melhor modelo para resolver o problema do usuário de acordo com a complexidade de seu problema. Como funciona? Um modelo menor analisa o contexto da pergunta e avalia o grau de complexidade que o usuário está enfrentando, e esse modelo responde com o indicador de qual modelo é melhor para responder aquela pergunta. O roteador decide qual é o melhor modelo por mensagem e não por conversa. Um modelo de roteador pode ajudar à reduzir custos e manter a qualidade da conversação, usando recursos de pensamento profundo somente quando necessário. Sentinel Reasoning O mecanismo de pensamento profundo do Sentinel é um motor de raciocínio plug-and-play que fornece todo o contexto necessário para o modelo Sentinel responder a pergunta do usuário. Durante o processo de pensamento, Sentinel chama funções, realiza cálculos e executa código, visando fornecer uma resposta elaborada e precisa para o usuário, mesmo usado com modelos menores e menos inteligentes. A precificação do reasoning do Sentinel e os tokens do Sentinel são separadas: você perceberá que ao usar o modelo Sentinel, verá lançamentos do roteamento, raciocínio e inferência. Essa divisão é feita para fornecer transparência sobre o uso do Sentinel."
  },
  "index.html": {
    "href": "index.html",
    "title": "Bem-vindo! | AIVAX",
    "keywords": "Bem-vindo! Bem-vindo à documentação da AIVAX."
  },
  "readme.html": {
    "href": "readme.html",
    "title": "Sisk Documentation | AIVAX",
    "keywords": "Sisk Documentation This repository contains the source code of the Sisk Documentation website. Building Firstly, make sure you have docfx installed in your machine. You'll need .NET SDK to install it. Clone this repository. Build the Sisk Framework project and put the .DLL binaries and XML documentation file at the ref/ directory, on the repository root. Run docfx, then docfx serve. Warning Please, do not use the docfx version 2.78.0 or later. This version has a bug that changes the documentation navigation layout. See the tracking issue. Prefer the version 2.76.0: dotnet tool install -g docfx --version 2.76.0 Then you're ready to go and you'll have the static website files at /_site. Contributing Contributions are always welcome. Contribute with spelling corrections, fixing broken links and more. Please, only edit english documentation files. Documentation files for another languages are AI-generated from english files through. Note Please do not edit API specification files (XML). These files are generated. If you want to edit any API documentation, edit it in the repository where the code is hosted."
  }
}